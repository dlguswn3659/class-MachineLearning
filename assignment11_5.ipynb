{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment11-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxd8hvqjVpYAw1dDSUx0Hk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlguswn3659/class-MachineLearning/blob/master/assignment11_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw-faMg3kxeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re,string,unicodedata\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEmx3Nerk1ud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "64625158-e648-472a-efda-1c199bf5d2e0"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# import zipfile\n",
        "# import io\n",
        "# zf = zipfile.ZipFile(io.BytesIO(uploaded['movie_review.zip']), \"r\")\n",
        "# zf.extractall()\n",
        "\n",
        "review_data = load_files(r\"movie_review\")\n",
        "X, y = review_data.data, review_data.target\n",
        "\n",
        "documents = []\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "for sen in range(0, len(X)):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    \n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    \n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    \n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    \n",
        "    # Lemmatization\n",
        "    document = document.split()\n",
        "    document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    \n",
        "    documents.append(document)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "matjDvyhlDJJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "aba0fe52-22a9-419c-d621-4e6cada57bf8"
      },
      "source": [
        "print(documents)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt9Wj3kH5q8w",
        "colab_type": "code",
        "outputId": "8abe2980-d587-4b72-d76d-1418d51453b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=40000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "# vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents).toarray()\n",
        "print(X)\n",
        "\n",
        "vector_size = 885\n",
        "\n",
        "# tfidfconverter = TfidfTransformer()\n",
        "# print(tfidfconverter)\n",
        "# X = tfidfconverter.fit_transform(X).toarray()\n",
        "# print(X)\n",
        "\n",
        "X_train_tmp, X_test_tmp, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
        "\n",
        "# print(X_train.shape)\n",
        "\n",
        "X_pos_sum = np.zeros(12638)\n",
        "X_neg_sum = np.zeros(12638)\n",
        "\n",
        "X_train = np.zeros((1401, 884))\n",
        "X_test = np.zeros((601, 884))\n",
        "\n",
        "for i in range(0, 2002):\n",
        "  if y[i] == 0:\n",
        "    X_neg_sum = X_neg_sum + X[i]\n",
        "  else:\n",
        "    X_pos_sum = X_pos_sum + X[i]\n",
        "\n",
        "pos_neg_voca = 0\n",
        "\n",
        "for i in range(0, 12638):\n",
        "  # if abs(X_neg_sum[i] - X_pos_sum[i]) >= 30:\n",
        "  if (X_neg_sum[i]*3 <= X_pos_sum[i] and X_neg_sum[i] > 2) or (X_pos_sum[i]*3 <= X_neg_sum[i] and X_pos_sum[i] > 2):\n",
        "    for j in range(0, 1401):\n",
        "      X_train[j][pos_neg_voca] = X_train_tmp[j][i]\n",
        "    for j in range(0, 601):\n",
        "      X_test[j][pos_neg_voca] = X_test_tmp[j][i]\n",
        "    pos_neg_voca  = pos_neg_voca + 1\n",
        "\n",
        "print(X_neg_sum)\n",
        "print(X_pos_sum)\n",
        "print(pos_neg_voca)\n",
        "\n",
        "print(X_train)\n",
        "print(X_test)\n",
        "\n",
        "print(X_train[0])"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[ 7. 60.  4. ...  3.  8.  5.]\n",
            "[ 2. 55.  6. ...  7.  6. 12.]\n",
            "884\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQpyXu3jGOvY",
        "colab_type": "code",
        "outputId": "4a7b2a26-7e48-4bce-e13e-c2b52e173632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        }
      },
      "source": [
        "print(X_train)\n",
        "print(X_test)\n",
        "print(y_train)\n",
        "print(y_test)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[0 0 1 ... 1 0 1]\n",
            "[1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0\n",
            " 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0\n",
            " 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0\n",
            " 0 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 0]\n",
            "(1401, 884)\n",
            "(601, 884)\n",
            "(1401,)\n",
            "(601,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSA68oWdBX2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_classifier에 대한 함수 정의하기\n",
        "# def train_classifier(X_Train, y_Train):\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD42xdkL05UT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #Codes for classification\n",
        "\n",
        "# classifier = train_classifier(X_train, y_train)\n",
        "# y_pred_test = classifier.predict(X_test)\n",
        "# y_pred_train = classifier.predict(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV9K1MVaET0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix_t = np.zeros((350, vector_size))  #350줄\n",
        "matrix_t[0][0] = 1               #y레이어의 bias를 위한 값 1\n",
        "\n",
        "for i in range(1, 350):\n",
        "  for j in range(0, vector_size):\n",
        "    matrix_t[i][j] = np.random.normal()\n",
        "\n",
        "\n",
        "# matrix_u[196][0] = 0            #줄, 칸\n",
        "##############################################\n",
        "matrix_u = np.zeros((50, 350))  #50줄\n",
        "matrix_u[0][0] = 1               #y레이어의 bias를 위한 값 1\n",
        "\n",
        "for i in range(1, 50):\n",
        "  for j in range(0, 350):\n",
        "    matrix_u[i][j] = np.random.normal()\n",
        "\n",
        "\n",
        "# matrix_u[196][0] = 0            #줄, 칸\n",
        "##############################################\n",
        "matrix_v = np.zeros((8, 50))  #8줄\n",
        "matrix_v[0][0] = 1               #y레이어의 bias를 위한 값 1\n",
        "\n",
        "for i in range(1, 8):\n",
        "  for j in range(0, 50):\n",
        "    matrix_v[i][j] = np.random.normal()\n",
        "\n",
        "\n",
        "# matrix_v[196][0] = 0            #줄, 칸\n",
        "##############################################\n",
        "matrix_w = np.zeros((1, 8))  #1줄\n",
        "\n",
        "for i in range(0, 1):\n",
        "  for j in range(0, 8):\n",
        "    matrix_w[i][j] = np.random.normal()\n",
        "\n",
        "# matrix_w[196][0] = 0            #줄, 칸\n",
        "\n",
        "# print(matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfHkfVcXRZIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import matplotlib.animation as animation\n",
        "import operator\n",
        "from scipy import stats \n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.datasets.samples_generator import make_regression \n",
        "import csv\n",
        "import math\n",
        "import statistics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLiiakUcEfil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "\n",
        "x_ = np.ones((350, 1))\n",
        "y_ = np.ones((50, 1))\n",
        "\n",
        "j_list = np.ones(shape=(1401, vector_size, 1))\n",
        "x_list = np.ones(shape=(1401, 350, 1))\n",
        "\n",
        "x__list = np.ones(shape=(1401, 350, 1))\n",
        "y__list = np.ones(shape=(1401, 50, 1))\n",
        "z__list = np.ones(shape=(1401, 8, 1))\n",
        "h__list = np.ones(shape=(1401, 1, 1))\n",
        "\n",
        "x_list = np.ones(shape=(1401, 350, 1))\n",
        "y_list = np.ones(shape=(1401, 50, 1))\n",
        "z_list = np.ones(shape=(1401, 8, 1))\n",
        "h_list = np.ones(shape=(1401, 1, 1))\n",
        "l_list = np.zeros(shape=(1401, 1, 1))\n",
        "\n",
        "x_list_tmp = np.ones(shape=(1401, 350, 1))\n",
        "y_list_tmp = np.ones(shape=(1401, 50, 1))\n",
        "z_list_tmp = np.ones(shape=(1401, 8, 1))\n",
        "h_list_tmp = np.ones(shape=(1401, 1, 1))\n",
        "\n",
        "x__list_tmp = np.ones(shape=(1401, 350, 1))\n",
        "y__list_tmp = np.ones(shape=(1401, 50, 1))\n",
        "z__list_tmp = np.ones(shape=(1401, 8, 1))\n",
        "h__list_tmp = np.ones(shape=(1401, 1, 1))\n",
        "\n",
        "\n",
        "#training set 1401개에 대해서\n",
        "for i in range(0, 1401):  \n",
        "  bias   = [1]       #bias 1\n",
        "  im_vector = np.concatenate((bias, X_train[i]), axis = None)\n",
        "  j_list[i] = im_vector.reshape((vector_size, 1))\n",
        "\n",
        "  x_ = np.dot(matrix_t, j_list[i])  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "  x__list[i] = x_\n",
        "  \n",
        "  x_vector = np.ones((350,1)) #bias\n",
        "  for j in range (1, 350):\n",
        "    x_vector[j][0] = 1 / (1 + math.exp(-x_[j][0]))\n",
        "    # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  x   = x_vector.reshape((350, 1))\n",
        "  x_list[i] = x\n",
        "\n",
        "\n",
        "\n",
        "  y_ = np.dot(matrix_u, x)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "  y__list[i] = y_\n",
        "  \n",
        "  y_vector = np.ones((50,1)) #bias\n",
        "  for j in range (1, 50):\n",
        "    y_vector[j][0] = 1 / (1 + math.exp(-y_[j][0]))\n",
        "    # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  y   = y_vector.reshape((50, 1))\n",
        "  y_list[i] = y\n",
        "\n",
        "  z_  = np.dot(matrix_v, y)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "  z__list[i] = z_\n",
        "  z_vector = np.ones((8,1)) #bias\n",
        "  for j in range (1, 8):\n",
        "    z_vector[j][0] = 1 / (1 + math.exp(-z_[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  z   = z_vector.reshape((8, 1))\n",
        "  z_list[i] = z\n",
        "  \n",
        "  # print(z)\n",
        "\n",
        "  h_  = np.dot(matrix_w, z)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "  h__list[i] = h_\n",
        "  \n",
        "  h_vector = np.ones((1,1)) #bias\n",
        "  for j in range (0, 1):\n",
        "    h_vector[j][0] = 1 / (1 + math.exp(-h_[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "  h   = h_vector.reshape((1, 1))\n",
        "  # tmp = h.ravel()\n",
        "  # max_value = max(tmp)\n",
        "  # print(max_value)\n",
        "  # max_index = tmp.index(max_value)\n",
        "  h_list[i] = h\n",
        "\n",
        "  tmp_label = 0\n",
        "  # print(h)\n",
        "\n",
        "  if h >= 0.5:\n",
        "    tmp_label = 1\n",
        "\n",
        "  l_list[i][0] = tmp_label\n",
        "  # print(l_list[i])\n",
        "  # print(h)\n",
        "\n",
        "  x_list_tmp[i] = x_list[i]\n",
        "  y_list_tmp[i] = y_list[i]\n",
        "  z_list_tmp[i] = z_list[i]\n",
        "  h_list_tmp[i] = h_list[i]\n",
        "  x__list_tmp[i] = x__list[i]\n",
        "  y__list_tmp[i] = y__list[i]\n",
        "  z__list_tmp[i] = z__list[i]\n",
        "  h__list_tmp[i] = h__list[i]\n",
        "\n",
        "# print(l_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b91lsWSoEg8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "\n",
        "x_2 = np.ones((350, 1))\n",
        "y_2 = np.ones((50, 1))\n",
        "\n",
        "j_list2 = np.ones(shape=(601, vector_size, 1))\n",
        "x_list2 = np.ones(shape=(601, 350, 1))\n",
        "\n",
        "x__list2 = np.ones(shape=(601, 350, 1))\n",
        "y__list2 = np.ones(shape=(601, 50, 1))\n",
        "z__list2 = np.ones(shape=(601, 8, 1))\n",
        "h__list2 = np.ones(shape=(601, 1, 1))\n",
        "\n",
        "x_list2 = np.ones(shape=(601, 350, 1))\n",
        "y_list2 = np.ones(shape=(601, 50, 1))\n",
        "z_list2 = np.ones(shape=(601, 8, 1))\n",
        "h_list2 = np.ones(shape=(601, 1, 1))\n",
        "l_list2 = np.zeros(shape=(601, 1, 1))\n",
        "\n",
        "x_list_tmp2 = np.ones(shape=(601, 350, 1))\n",
        "y_list_tmp2 = np.ones(shape=(601, 50, 1))\n",
        "z_list_tmp2 = np.ones(shape=(601, 8, 1))\n",
        "h_list_tmp2 = np.ones(shape=(601, 1, 1))\n",
        "\n",
        "x__list_tmp2 = np.ones(shape=(601, 350, 1))\n",
        "y__list_tmp2 = np.ones(shape=(601, 50, 1))\n",
        "z__list_tmp2 = np.ones(shape=(601, 8, 1))\n",
        "h__list_tmp2 = np.ones(shape=(601, 1, 1))\n",
        "\n",
        "\n",
        "#training set 1401개에 대해서\n",
        "for i in range(0, 601):  \n",
        "  bias2   = [1]       #bias 1\n",
        "  im_vector2 = np.concatenate((bias2, X_test[i]), axis = None)\n",
        "  j_list2[i] = im_vector2.reshape((vector_size, 1))\n",
        "\n",
        "  x_2 = np.dot(matrix_t, j_list2[i])  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "  x__list2[i] = x_2\n",
        "  \n",
        "  x_vector2 = np.ones((350,1)) #bias\n",
        "  for j in range (1, 350):\n",
        "    x_vector2[j][0] = 1 / (1 + math.exp(-x_2[j][0]))\n",
        "    # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  x2   = x_vector2.reshape((350, 1))\n",
        "  x_list2[i] = x2\n",
        "\n",
        "\n",
        "\n",
        "  y_2 = np.dot(matrix_u, x2)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "  y__list2[i] = y_2\n",
        "  \n",
        "  y_vector2 = np.ones((50,1)) #bias\n",
        "  for j in range (1, 50):\n",
        "    y_vector2[j][0] = 1 / (1 + math.exp(-y_2[j][0]))\n",
        "    # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  y2   = y_vector2.reshape((50, 1))\n",
        "  y_list2[i] = y2\n",
        "\n",
        "  z_2  = np.dot(matrix_v, y2)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "  z__list2[i] = z_2\n",
        "  z_vector2 = np.ones((8,1)) #bias\n",
        "  for j in range (1, 8):\n",
        "    z_vector2[j][0] = 1 / (1 + math.exp(-z_2[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  z2   = z_vector2.reshape((8, 1))\n",
        "  z_list2[i] = z2\n",
        "  \n",
        "  # print(z)\n",
        "\n",
        "  h_2 = np.dot(matrix_w, z2)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "  h__list2[i] = h_2\n",
        "  \n",
        "  h_vector2 = np.ones((1,1)) #bias\n",
        "  for j in range (0, 1):\n",
        "    h_vector2[j][0] = 1 / (1 + math.exp(-h_2[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "  h2   = h_vector2.reshape((1, 1))\n",
        "  # tmp = h.ravel()\n",
        "  # max_value = max(tmp)\n",
        "  # print(max_value)\n",
        "  # max_index = tmp.index(max_value)\n",
        "  h_list2[i] = h2\n",
        "\n",
        "  tmp_label = 0\n",
        "\n",
        "  if h2 >= 0.5:\n",
        "    tmp_label = 1\n",
        "\n",
        "  l_list2[i][0] = tmp_label\n",
        "  # print(l_list[i])\n",
        "  # print(h2)\n",
        "\n",
        "  x_list_tmp2[i] = x_list2[i]\n",
        "  y_list_tmp2[i] = y_list2[i]\n",
        "  z_list_tmp2[i] = z_list2[i]\n",
        "  h_list_tmp2[i] = h_list2[i]\n",
        "  x__list_tmp2[i] = x__list2[i]\n",
        "  y__list_tmp2[i] = y__list2[i]\n",
        "  z__list_tmp2[i] = z__list2[i]\n",
        "  h__list_tmp2[i] = h__list2[i]\n",
        "\n",
        "# print(l_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVgg5PyfCGAn",
        "colab_type": "code",
        "outputId": "5a4916ea-8fda-4813-f84c-b6ac31aaa114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# (1401, 1500)\n",
        "# (601, 1500)\n",
        "# (1401,)\n",
        "# (601,)\n",
        "\n",
        "iteration = 100\n",
        "l_r = 0.1\n",
        "lambda_ = 1\n",
        "\n",
        "cost_history = np.zeros(iteration)    #about train\n",
        "cost_history2 = np.zeros(iteration)   #about test\n",
        "\n",
        "label_group = np.zeros(shape = (1401, 1))\n",
        "label_group2 = np.zeros(shape = (601, 1))\n",
        "\n",
        "accuracy_history = np.zeros(iteration)\n",
        "accuracy_history2 = np.zeros(iteration)\n",
        "\n",
        "for i in range(0, 1401):\n",
        "  label_group[i][0] = l_list[i][0]\n",
        "\n",
        "for i in range(0, 601):\n",
        "  label_group2[i][0] = l_list2[i][0]\n",
        "\n",
        "for t in range(0, iteration):\n",
        "  number_of_correct = 0\n",
        "  number_of_correct2 = 0\n",
        "\n",
        "  print(t, end=' ')\n",
        "\n",
        "  parameter_sum = 0\n",
        "\n",
        "  for q in range(0, vector_size):\n",
        "    for w in range(1, 350):\n",
        "      parameter_sum = parameter_sum + matrix_t[w][q]**2\n",
        "\n",
        "  for q in range(0, 350):\n",
        "    for w in range(1, 50):\n",
        "      parameter_sum = parameter_sum + matrix_u[w][q]**2\n",
        "\n",
        "  for q in range(0, 50):\n",
        "    for w in range(1, 8):\n",
        "      parameter_sum = parameter_sum + matrix_v[w][q]**2\n",
        "\n",
        "  for q in range(0, 8):\n",
        "    for w in range(0, 1):\n",
        "      parameter_sum = parameter_sum + matrix_w[w][q]**2\n",
        "\n",
        "  result = 0.0\n",
        "  for i in range(0, 1401):\n",
        "    for k in range(0, 1):\n",
        "      result = result + ((-1)*label_group[i][k]*math.log(h_list[i][k][0]) - (1-label_group[i][k])*math.log(1-h_list[i][k][0])) + ((lambda_)/(2 * 543258)) * parameter_sum\n",
        "  result = result / 1401\n",
        "  cost_history[t] = result\n",
        "\n",
        "##gradient descent\n",
        "  result_w = 0\n",
        "  result_v = 0\n",
        "  result_u = 0\n",
        "  result_t = 0\n",
        "\n",
        "  for i in range(0, 1401):\n",
        "    small_delta_2 = np.zeros(shape = (1, 1))\n",
        "    for j in range(0, 1):\n",
        "      small_delta_2[j][0] = h_list[i][j] - label_group[i][j]\n",
        "    # print(h_list[i])\n",
        "    # print(label_group[i])\n",
        "    # small_delta_2 = h_list[i] - label_group[i] #여기 label_group 다시 봐야함.\n",
        "    small_delta_1 = np.dot(np.transpose(matrix_w), small_delta_2)\n",
        "    small_delta_0 = np.dot(np.transpose(matrix_v), small_delta_1)*(y_list_tmp[i]*(1 - y_list_tmp[i]))\n",
        "    small_delta_ = np.dot(np.transpose(matrix_u), small_delta_0)*(x_list_tmp[i]*(1 - x_list_tmp[i])) #이거..괜찮은가..?\n",
        "\n",
        "    # print(small_delta_2)\n",
        "    result_w = result_w + np.dot(small_delta_2, np.transpose(z_list[i]))\n",
        "    result_v = result_v + np.dot(small_delta_1, np.transpose(y_list[i]))\n",
        "    result_u = result_u + np.dot(small_delta_0, np.transpose(x_list[i]))\n",
        "    result_t = result_t + np.dot(small_delta_, np.transpose(j_list[i]))\n",
        "\n",
        "  matrix_w = matrix_w - (result_w / 1401 *l_r)\n",
        "  matrix_v = matrix_v - (result_v / 1401 *l_r)\n",
        "  matrix_u = matrix_u - (result_u / 1401 *l_r)\n",
        "  matrix_t = matrix_t - (result_t / 1401 *l_r)\n",
        "\n",
        "\n",
        "####################################################################\n",
        "\n",
        "  for k in range(0, 1401):\n",
        "    bias   = [1]       #bias 1\n",
        "    im_vector = np.concatenate((bias, X_train[k]), axis = None)\n",
        "    j_list[k] = im_vector.reshape((vector_size, 1))\n",
        "\n",
        "    x_ = np.dot(matrix_t, j_list[k])  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "    x__list[k] = x_\n",
        "    \n",
        "    x_vector = np.ones((350,1)) #bias\n",
        "    for j in range (1, 350):\n",
        "      x_vector[j][0] = 1 / (1 + math.exp(-x_[j][0]))\n",
        "      # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "    x   = x_vector.reshape((350, 1))\n",
        "    x_list[k] = x\n",
        "\n",
        "\n",
        "\n",
        "    y_ = np.dot(matrix_u, x)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "    y__list[k] = y_\n",
        "    \n",
        "    y_vector = np.ones((50,1)) #bias\n",
        "    for j in range (1, 50):\n",
        "      y_vector[j][0] = 1 / (1 + math.exp(-y_[j][0]))\n",
        "      # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "    y   = y_vector.reshape((50, 1))\n",
        "    y_list[k] = y\n",
        "\n",
        "    z_  = np.dot(matrix_v, y)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "    z__list[k] = z_\n",
        "    z_vector = np.ones((8,1)) #bias\n",
        "    for j in range (1, 8):\n",
        "      z_vector[j][0] = 1 / (1 + math.exp(-z_[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "    z   = z_vector.reshape((8, 1))\n",
        "    z_list[k] = z\n",
        "    \n",
        "    # print(z)\n",
        "\n",
        "    h_  = np.dot(matrix_w, z)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "    h__list[k] = h_\n",
        "    \n",
        "    h_vector = np.ones((1,1)) #bias\n",
        "    for j in range (0, 1):\n",
        "      h_vector[j][0] = 1 / (1 + math.exp(-h_[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "    h   = h_vector.reshape((1, 1))\n",
        "    # tmp = h.ravel()\n",
        "    # max_value = max(tmp)\n",
        "    # print(max_value)\n",
        "    # max_index = tmp.index(max_value)\n",
        "\n",
        "    h_list11_tmp = h_list.reshape((1401))\n",
        "    median_val = statistics.median(h_list11_tmp)\n",
        "\n",
        "    h_list[k] = h\n",
        "\n",
        "    tmp_label = 0\n",
        "\n",
        "    if h >= median_val:\n",
        "        # print(h)\n",
        "        tmp_label = 1\n",
        "\n",
        "    # if h >= 0.5:\n",
        "    #   # print(h)\n",
        "    #   tmp_label = 1\n",
        "\n",
        "    l_list[k][0] = tmp_label\n",
        "    # print(l_list[k])\n",
        "    # print(h)\n",
        "\n",
        "    x_list_tmp[k] = x_list[k]\n",
        "    y_list_tmp[k] = y_list[k]\n",
        "    z_list_tmp[k] = z_list[k]\n",
        "    h_list_tmp[k] = h_list[k]\n",
        "    x__list_tmp[k] = x__list[k]\n",
        "    y__list_tmp[k] = y__list[k]\n",
        "    z__list_tmp[k] = z__list[k]\n",
        "    h__list_tmp[k] = h__list[k]\n",
        "\n",
        "    if l_list[k][0] == y_train[k]:\n",
        "      number_of_correct = number_of_correct + 1\n",
        "\n",
        "  accuracy_history[t] = number_of_correct / 1401 * 100\n",
        "  print(accuracy_history[t])\n",
        "  # accuracy_history[t] = number_of_correct_predictions/total_number_of_predictions*100\n",
        "  # print(accuracy_history[t])\n",
        "\n",
        "############################\n",
        "  #test data!\n",
        "\n",
        "  number_of_correct_predictions2 = 0\n",
        "  \n",
        "  print(t, end=' ')\n",
        "\n",
        "  result2 = 0.0\n",
        "  for i in range(0, 601):\n",
        "    for k in range(0, 1):\n",
        "      # if h_list_tmp[i][k][0] <= 0:\n",
        "      # print(h_list_tmp[i][k][0])\n",
        "      result2 = result2 + ((-1)*label_group2[i][k]*math.log(h_list2[i][k][0]) - (1-label_group2[i][k])*math.log(1-h_list2[i][k][0])) + ((lambda_)/(2 * 164995)) * parameter_sum\n",
        "  result2 = result2 / 601\n",
        "  cost_history2[t] = result2\n",
        "\n",
        "\n",
        "  for k in range(0, 601):\n",
        "  # print(i)\n",
        "    bias2   = [1]       #bias 1\n",
        "    im_vector2 = np.concatenate((bias2, X_test[k]), axis = None)\n",
        "    j_list2[k] = im_vector2.reshape((vector_size, 1))\n",
        "\n",
        "    x_2 = np.dot(matrix_t, j_list2[k])  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "    x__list2[k] = x_2\n",
        "    \n",
        "    x_vector2 = np.ones((350,1)) #bias\n",
        "    for j in range (1, 350):\n",
        "      x_vector2[j][0] = 1 / (1 + math.exp(-x_2[j][0]))\n",
        "      # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "    x2   = x_vector2.reshape((350, 1))\n",
        "    x_list2[k] = x2\n",
        "\n",
        "\n",
        "\n",
        "    y_2 = np.dot(matrix_u, x2)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "    y__list2[k] = y_2\n",
        "    \n",
        "    y_vector2 = np.ones((50,1)) #bias\n",
        "    for j in range (1, 50):\n",
        "      y_vector2[j][0] = 1 / (1 + math.exp(-y_2[j][0]))\n",
        "      # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "    y2   = y_vector2.reshape((50, 1))\n",
        "    y_list2[k] = y2\n",
        "\n",
        "    z_2  = np.dot(matrix_v, y2)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "    z__list2[k] = z_2\n",
        "    z_vector2 = np.ones((8,1)) #bias\n",
        "    for j in range (1, 8):\n",
        "      z_vector2[j][0] = 1 / (1 + math.exp(-z_2[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "    z2   = z_vector2.reshape((8, 1))\n",
        "    z_list2[k] = z2\n",
        "    \n",
        "    # print(z)\n",
        "\n",
        "    h_2 = np.dot(matrix_w, z2)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "    h__list2[k] = h_2\n",
        "    \n",
        "    h_vector2 = np.ones((1,1)) #bias\n",
        "    for j in range (0, 1):\n",
        "      h_vector2[j][0] = 1 / (1 + math.exp(-h_2[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "    h2   = h_vector2.reshape((1, 1))\n",
        "    # tmp = h.ravel()\n",
        "    # max_value = max(tmp)\n",
        "    # print(max_value)\n",
        "    # max_index = tmp.index(max_value)\n",
        "\n",
        "    h_list22_tmp = h_list2.reshape((601))\n",
        "    median_val2 = statistics.median(h_list22_tmp)\n",
        "\n",
        "    h_list2[k] = h2\n",
        "\n",
        "    tmp_label = 0\n",
        "\n",
        "    if h2 >= median_val2:\n",
        "      # print(h2)\n",
        "      tmp_label = 1\n",
        "\n",
        "    # if h2 >= 0.5:\n",
        "    #   # print(h2)\n",
        "    #   tmp_label = 1\n",
        "\n",
        "    l_list2[k][0] = tmp_label\n",
        "    # print(l_list[k])\n",
        "    # print(h2)\n",
        "\n",
        "    x_list_tmp2[k] = x_list2[k]\n",
        "    y_list_tmp2[k] = y_list2[k]\n",
        "    z_list_tmp2[k] = z_list2[k]\n",
        "    h_list_tmp2[k] = h_list2[k]\n",
        "    x__list_tmp2[k] = x__list2[k]\n",
        "    y__list_tmp2[k] = y__list2[k]\n",
        "    z__list_tmp2[k] = z__list2[k]\n",
        "    h__list_tmp2[k] = h__list2[k]\n",
        "\n",
        "    if l_list2[k][0] == y_test[k]:\n",
        "      number_of_correct2 = number_of_correct2 + 1\n",
        "\n",
        "  accuracy_history2[t] = number_of_correct2 / 601 * 100\n",
        "  print(accuracy_history2[t])\n",
        "\n",
        "  # accuracy_history2[t] = number_of_correct_predictions2/total_number_of_predictions2*100\n",
        "  t = t + 1"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 51.605995717344754\n",
            "0 48.25291181364393\n",
            "1 51.82012847965739\n",
            "1 48.25291181364393\n",
            "2 52.24839400428265\n",
            "2 49.58402662229617\n",
            "3 52.9621698786581\n",
            "3 51.24792013311148\n",
            "4 52.46252676659529\n",
            "4 51.08153078202995\n",
            "5 52.890792291220556\n",
            "5 51.24792013311148\n",
            "6 53.24768022840828\n",
            "6 50.08319467554077\n",
            "7 53.03354746609564\n",
            "7 50.41597337770383\n",
            "8 52.67665952890792\n",
            "8 50.249584026622294\n",
            "9 52.67665952890792\n",
            "9 50.08319467554077\n",
            "10 53.24768022840828\n",
            "10 49.58402662229617\n",
            "11 53.461812990720915\n",
            "11 50.08319467554077\n",
            "12 53.461812990720915\n",
            "12 50.249584026622294\n",
            "13 53.89007851534618\n",
            "13 50.74875207986689\n",
            "14 53.96145610278372\n",
            "14 51.08153078202995\n",
            "15 54.31834403997144\n",
            "15 50.74875207986689\n",
            "16 54.31834403997144\n",
            "16 50.91514143094842\n",
            "17 54.389721627408996\n",
            "17 51.08153078202995\n",
            "18 54.246966452533904\n",
            "18 50.74875207986689\n",
            "19 54.10421127765882\n",
            "19 50.582362728785355\n",
            "20 54.246966452533904\n",
            "20 49.750415973377706\n",
            "21 54.53247680228408\n",
            "21 50.08319467554077\n",
            "22 54.389721627408996\n",
            "22 50.08319467554077\n",
            "23 54.246966452533904\n",
            "23 49.417637271214645\n",
            "24 53.81870092790864\n",
            "24 49.58402662229617\n",
            "25 53.747323340471084\n",
            "25 49.25124792013311\n",
            "26 53.81870092790864\n",
            "26 49.08485856905158\n",
            "27 53.81870092790864\n",
            "27 49.417637271214645\n",
            "28 53.81870092790864\n",
            "28 50.582362728785355\n",
            "29 53.89007851534618\n",
            "29 50.582362728785355\n",
            "30 54.10421127765882\n",
            "30 49.91680532445923\n",
            "31 53.89007851534618\n",
            "31 49.750415973377706\n",
            "32 53.604568165596\n",
            "32 49.750415973377706\n",
            "33 53.89007851534618\n",
            "33 49.750415973377706\n",
            "34 53.604568165596\n",
            "34 50.08319467554077\n",
            "35 53.461812990720915\n",
            "35 49.91680532445923\n",
            "36 53.53319057815846\n",
            "36 50.249584026622294\n",
            "37 53.461812990720915\n",
            "37 50.249584026622294\n",
            "38 53.53319057815846\n",
            "38 50.74875207986689\n",
            "39 53.17630264097074\n",
            "39 50.582362728785355\n",
            "40 53.24768022840828\n",
            "40 50.582362728785355\n",
            "41 53.461812990720915\n",
            "41 50.41597337770383\n",
            "42 53.67594575303355\n",
            "42 50.74875207986689\n",
            "43 53.53319057815846\n",
            "43 50.582362728785355\n",
            "44 53.39043540328336\n",
            "44 50.582362728785355\n",
            "45 53.53319057815846\n",
            "45 50.91514143094842\n",
            "46 53.24768022840828\n",
            "46 51.08153078202995\n",
            "47 53.24768022840828\n",
            "47 50.91514143094842\n",
            "48 53.24768022840828\n",
            "48 50.91514143094842\n",
            "49 53.17630264097074\n",
            "49 50.74875207986689\n",
            "50 53.104925053533194\n",
            "50 50.582362728785355\n",
            "51 53.03354746609564\n",
            "51 50.582362728785355\n",
            "52 52.81941470378301\n",
            "52 50.41597337770383\n",
            "53 52.81941470378301\n",
            "53 50.249584026622294\n",
            "54 52.9621698786581\n",
            "54 50.08319467554077\n",
            "55 52.67665952890792\n",
            "55 49.91680532445923\n",
            "56 52.46252676659529\n",
            "56 49.750415973377706\n",
            "57 52.3197715917202\n",
            "57 49.91680532445923\n",
            "58 52.3197715917202\n",
            "58 50.41597337770383\n",
            "59 52.39114917915775\n",
            "59 50.41597337770383\n",
            "60 52.39114917915775\n",
            "60 50.249584026622294\n",
            "61 52.39114917915775\n",
            "61 49.91680532445923\n",
            "62 52.46252676659529\n",
            "62 49.58402662229617\n",
            "63 52.533904354032835\n",
            "63 49.58402662229617\n",
            "64 52.60528194147038\n",
            "64 49.750415973377706\n",
            "65 52.46252676659529\n",
            "65 49.750415973377706\n",
            "66 52.24839400428265\n",
            "66 49.91680532445923\n",
            "67 52.10563882940756\n",
            "67 49.91680532445923\n",
            "68 52.03426124197003\n",
            "68 49.750415973377706\n",
            "69 52.10563882940756\n",
            "69 49.750415973377706\n",
            "70 51.89150606709493\n",
            "70 49.58402662229617\n",
            "71 51.74875089221984\n",
            "71 49.417637271214645\n",
            "72 51.74875089221984\n",
            "72 49.58402662229617\n",
            "73 51.67737330478231\n",
            "73 49.417637271214645\n",
            "74 51.67737330478231\n",
            "74 49.25124792013311\n",
            "75 51.53461812990721\n",
            "75 49.25124792013311\n",
            "76 51.67737330478231\n",
            "76 49.25124792013311\n",
            "77 51.74875089221984\n",
            "77 48.91846921797005\n",
            "78 51.67737330478231\n",
            "78 49.08485856905158\n",
            "79 51.67737330478231\n",
            "79 49.08485856905158\n",
            "80 51.46324054246967\n",
            "80 49.25124792013311\n",
            "81 51.32048536759457\n",
            "81 48.91846921797005\n",
            "82 51.39186295503212\n",
            "82 48.91846921797005\n",
            "83 51.46324054246967\n",
            "83 48.91846921797005\n",
            "84 51.39186295503212\n",
            "84 48.91846921797005\n",
            "85 51.39186295503212\n",
            "85 49.08485856905158\n",
            "86 51.32048536759457\n",
            "86 49.08485856905158\n",
            "87 51.24910778015703\n",
            "87 49.08485856905158\n",
            "88 51.46324054246967\n",
            "88 48.91846921797005\n",
            "89 51.46324054246967\n",
            "89 48.91846921797005\n",
            "90 51.53461812990721\n",
            "90 48.91846921797005\n",
            "91 51.46324054246967\n",
            "91 48.91846921797005\n",
            "92 51.32048536759457\n",
            "92 48.91846921797005\n",
            "93 51.32048536759457\n",
            "93 48.91846921797005\n",
            "94 51.17773019271949\n",
            "94 48.75207986688852\n",
            "95 51.17773019271949\n",
            "95 48.91846921797005\n",
            "96 51.24910778015703\n",
            "96 48.91846921797005\n",
            "97 51.32048536759457\n",
            "97 48.91846921797005\n",
            "98 51.46324054246967\n",
            "98 48.75207986688852\n",
            "99 51.24910778015703\n",
            "99 48.75207986688852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtFVXgejZDIS",
        "colab_type": "code",
        "outputId": "41b1603d-466e-4125-c6e7-834efbb813a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "#그 코드를 바탕으로 training loss plotting 하기.\n",
        "real_iter = iteration\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "cost_tmp = np.zeros(real_iter)\n",
        "\n",
        "for i in range(0, real_iter):\n",
        "  cost_tmp[i] = cost_history[i]\n",
        "\n",
        "\n",
        "# print(cost_history[1])\n",
        "\n",
        "ax.set_ylabel('J(Theta)')\n",
        "ax.set_xlabel('Iterations')\n",
        "_=ax.plot(range(real_iter),cost_tmp,'b.', color = 'blue')\n",
        "_=ax.plot(range(real_iter),cost_history2,'b.', color = 'red')"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJEhxJanbFCg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "26f607d4-877c-4eff-e5f0-e62850ddfccd"
      },
      "source": [
        "#그 코드를 바탕으로 training loss plotting 하기.\n",
        "fig,ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "# print(cost_history[1])\n",
        "\n",
        "ax.set_ylabel('accuracy')\n",
        "ax.set_xlabel('Iterations')\n",
        "_=ax.plot(range(iteration),accuracy_history,'b.', color = 'blue')\n",
        "_=ax.plot(range(iteration),accuracy_history2,'b.', color = 'red')\n",
        "# print(ac_tmp)"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs0AAAHgCAYAAABelVD0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dfbBteVkf+O/jbVuwRUBpmAxNT5sRtRhTjXK0OBGThnYoYwhEY2IyWiJGW1NJYHwZXsxMkXHKuiShok5NhqFLRGdGiQmKEjEtHbBFwkG4l3cFokO6DS3YTQLYYpR085s/9j5wbvfZZ5197l57vezPp6pr373227PW2mufh8XzrKdaawEAAFb7rKEDAACAsZM0AwBAB0kzAAB0kDQDAEAHSTMAAHSQNAMAQIcrhg7gNB7xiEe06667bugwAACYuYsXL36ktXb1/ZdPImm+7rrrcuHChaHDAABg5qrqjuOWK88AAIAOkmYAAOggaQYAgA6SZgAA6CBpBgCADpJmAADoIGkGAIAOkmYAAOggaQYAgA6SZgAA6CBpBgCADpJmAADoIGkGAIAOkmYAAOggaQYAgA6SZgAA6CBphhMcHCTnzy9uAYDddcXQAcBYHRwkN96YfPKTyZVXJq97XbK/P3RUAMAQnGmGFW67bZEw33ff4va224aOCAAYiqQZVrjhhsUZ5nPnFrc33DB0RADAUJRnwAr7+4uSjNtuWyTMSjMAYHdJmuEE+/uSZQBAeQYAAHSSNAMAQAdJMwAAdJA0M1lDDh4x9AQAdotGQCZpyMEjhp4AwO5xpplJGnLwiKEnALB7JM1M0pCDRww9AYDdozyD0Tg4OH6QyHHLhxw8YugJAOyeaq0NHUOnvb29duHChaHDoEer6oTVDwMA21RVF1tre/dfrjyDUVhVJ6x+GAAYA0kzo7CqTlj9MAAwBmqaGYVVdcLqhwGAMVDTzOysaijc1PO39V4AwPatqml2pplZWbdxcJONhpoWAWC+1DQzK+s2Dm6y0VDTIgDMl6SZWVm3cXCTjYaaFgFgvpRnMCvrNg5ustFQ0yIAzJdGQAAAWDLcBAAAzkjSDAAAHSTNAADQQdLMzjg4SM6fX9xiewDAOlw9g51g8MilbA8AWI8zzewEg0cuZXsAwHokzewEg0cuZXsAwHp6Lc+oqtuT3JPkviT3Hr3mXVX9QJIXJ7m6tfaRPuMAg0cuZXsAwHq2UdP85PsnxVX1mCRPTfJ7W/h8SLJIDCWHn2F7AMDpDVWe8aNJnptk/OMIAQDYeX0nzS3Ja6vqYlXdlCRV9Ywkd7bW3tnzZwMAwEb0XZ7xpNbanVX1yCS3VtX7kvxQFqUZJ1om2TclybXXXttvlAAAcIJezzS31u5c3t6V5FVJ/mKSL0ryzmWT4DVJ3lZV/9Uxr725tbbXWtu7+uqr+wyTLTJQAwCYot7ONFfVVUk+q7V2z/LfT03yw621Rx55zu1J9lw9YzcYqAEATFWfZ5ofleSNVfXOJG9J8prW2i09fh4jZ6AGADBVvZ1pbq19IMn1Hc+5rq/PZ3wOB2ocnmk2UAMAmIptXKeZHXRw8MDBGQZqDOO4fTHk8wFgiiTNbNxJtcsGamzXunXkfT8fAKZqqOEmzJja5fFYd1/0/XwAmCpJMxt3WLt87pza5aGtuy/6fj4ATFW1Nv5J1nt7e+3ChQtDh8Ea1LmOh5pmADi9qrrYWtt7wHJJM/RvU4noSe8jeQWAy7cqadYICD3bVHPdSe+jIQ8A+qWmGXq2qea6k95HQx4A9EvSDD3bVHPdSe+jIQ8A+qWmGbZg3RplNc0AMAyNgDAy6pABYHxWJc3KM2Ag6pABYDokzTAQdcgAMB0uOQcD2d9flGSoQwaA8ZM0w4D29yXLY6fBEoBE0gywkmZNAA6paQZYQbMmAIckzQAraNYE4JDyDOBU1q3tnUMtsGZNAA5JmoFO69b2zqkWWLMmAInyDOAU1q3tVQsMwNxImoFO69b2qgUGYG6UZwCd1q3tVQsMwNxUa23oGDrt7e21CxcuDB0G0LMpNRtOKVYATq+qLrbW9u6/3JlmYBSm1Gw4pVgB2Aw1zcAoTKnZcEqxArAZkmZgFKbUbDilWAHYDDXNnMqqekx1mpzFut+nvr9/Z3kfNc0A87SqplnSTKdV9ZjqNDmLsdUD+x4DcNSqpFl5Bp1W1WOq0+QsxlYP7HsMwGlImum0qh5TnSZnMbZ6YN9jAE5DeQanoqaZTRpbPbDvMQCH1DQDbIEEHGDaDDcB6JmmQoD5UtMMsCGaCgHmS9IMsCGaCgHmS3kGwIbs7y9KMtQ0A8yPpBlgg/b3JcsAc6Q8AwAAOkiaAQCgg6QZAAA6SJon5OAgOX9+cTvlz4ChjPH7vSqmk2Id43oAzJ1GwInYxtAEgxmYszF+v1fFdFKsY1wPgF3gTPNEbGNogsEMzNkYv9+rYjop1jGuB8AukDRPxDaGJhjMwJyN8fu9KqaTYh3jegDsgmqtDR1Dp729vXbhwoWhwxjcwUH/QxO28RnQt1Xf4zF+v88S65TWD2Bqqupia23vAcslzcCc7GrN766uN8CmrUqalWcAs7KrNb+7ut4A2yJpBmZlV2t+d3W9AbbFJeeAWdnfX5Qm7Fpt766uN8C2qGkGmDkNggCnt6qm2ZlmgBnTIAiwGWqaAWZMgyDAZkiaAWZMgyDAZvRanlFVtye5J8l9Se5tre1V1f+W5BlJPpXkriTf0Vr7/T7jmDv1isAqJzUI+u0AOL1eGwGXSfNea+0jR5Z9fmvtD5f/fnaSx7XWvvek99EIuJp6ReAs/HYAHG80w00OE+alq5KM//IdI6ZeETgLvx0A6+n76hktyWurqiV5aWvt5iSpqh9J8u1JPp7kyT3HMGuH9YqHZ4vUKwKn4bcDYD19l2c8urV2Z1U9MsmtSf5+a+0NRx5/QZIHtdZeeMxrb0pyU5Jce+21T7jjjjt6i3Pq1CUCZ+G3A+CBVpVnbG24SVX9wyR/1Fp78ZFl1yb5ldbal5/02qnXNPvDBMyZ3zhgTrY+3KSqrkryWa21e5b/fmqSH66qx7bWfmf5tGckeV9fMYyBZhtgzvzGAbuiz0bARyV5Y1W9M8lbkrymtXZLkhdV1Xuq6l1ZJNLP6TGGwWm2AebMbxywK3o709xa+0CS649Z/tf6+swx0mwDzJnfOGBX9H31jJ130mCBVdQHAlNxlt84gCnaWiPg5Zh6I+A61AcCAAxnNMNNOJn6QACA8ZE0j8xhfeC5c+oDAQDGQk3zyKgPBAAYH0nzCO3vS5aB4axqRtakDOwySTMAn7aqGVmTMrDr1DQD8GmrmpE1KQO7TtIMwKetakbWpAzsOuUZAHzaqmZkTcrArjPcBAAAlgw3AQCAM5I0AwBAB0kzAAB0kDTP3MFBcv784vY0ywE2yW8NMBeunjFjhhQAQ/JbA8yJM80zZkgBMCS/NcCcSJpnzJACYEh+a4A5UZ4xY4YUAEOa+2/NwcF81w14IMNNAGBN6rVhvgw3AYANUa8Nu0fSDABrUq8Nu0dNMwCsae712sADSZoB4ASrGv729yXLsEskzQCwgoY/4JCaZgBYQcMfcEjSDAAraPgDDinPGJAL4wO7aoy/f8fFdJaGvzGuG3D5JM0DUScH7Kox/v6dFNM6DX9jXDdgM5RnDESdHLCrxvj7t6mYxrhuwGZImgeiTg7YVWP8/dtUTGNcN2AzqrU2dAyd9vb22oULF4YOY+PUvQG7alO/f6veZ93l24gJmIaqutha23vAckkzAFO0qn543eUAR61KmpVnADBJq+qH110OcBqSZgAmaVX98LrLAU7DJecAmKRV11BedznAaahpBmA0ztLAB7BJq2qanWkGYBQ08AFjpqYZgFHQwAeMmaQZgFHQwAeMmfKMDVJzB3B2c2/g8zcCpk0j4IaouQNgFX8jYDoMN+mZmjsAVvE3AqZP0rwhau4AWMXfCJg+Nc0bMpeaOwA2z98ImD41zQAAsKSmGQAAzkjSDAAAHSTNAADQQdIMAAM6OEjOn1/c9vka4PK4egYADOQsQ08MSoFhONMMAAM5y9ATg1JgGJJmABjIWYaeGJQCw1CeAQADOcvQE4NSYBi9DjepqtuT3JPkviT3ttb2quqfJPkrST6Z5P9L8qzW2sdOeh/DTQDYRQcHkmPYtlXDTbZxpvnJrbWPHLl/a5IXtNburap/lOQFSZ63hTgAYDI0/MG4bL2mubX22tbavcu7b05yzbZjAICx0/AH49J30tySvLaqLlbVTcc8/p1J/nXPMQDA5Gj4g3HpuzzjSa21O6vqkUlurar3tdbekCRV9Q+S3JvkZ4574TLJvilJrr322p7DBIBx0fAH49JrI+AlH1T1D5P8UWvtxVX1HUm+J8mNrbU/7nqtRkAAALZhVSNgb+UZVXVVVT3k8N9JnprkPVX19Umem+Tpp0mYAQBgaH2WZzwqyauq6vBzfra1dktV/W6Sz8miXCNJ3txa+94e4wAAgMvSW9LcWvtAkuuPWf7FfX0mAAD0wRhtAJiJg4Pk/PnFLbBZxmgDwAwYhgL9cqYZAGbAMBTol6QZAGbAMBTol/KMMzg4cLF5AMbFMBTol6R5TWrGABir/X1/k6AvyjPWpGYMAGD3SJrXpGYMAGD3KM9Yk5oxAIDdI2k+AzVjAEzJqgb2IRvbNdUzNZJmAJixVQ3sQza2a6pnitQ0A8CMrWpgH7KxXVM9UyRpBoAZW9XAPmRju6Z6puhU5RlV9QtJXpbkX7fWPtVvSADApqxqYD+psX2TNdDHvWZbnw2bVK217idVfV2SZyV5YpJ/meTlrbX39xzbp+3t7bULFy5s6+MAYGdtsgZ63deMsf6a3VNVF1tre/dffqryjNbav2mtfWuSr0xye5J/U1VvqqpnVdVnbzZUAGAom6yBXvc1Y6y/hkOnrmmuqi9M8h1JvivJ25P8eBZJ9K29RAYAbN0ma6DXfc0Y66/h0GnLM16V5EuT/D9Jfqq19qEjj1047hT2JinPAIDt6bumeVufDWexqjzjtEnzk1trv9ZLZKcwRNLs4AQA2D2rkubTDjd5XFW9vbX2seWbPTzJ32qt/Z+bDHIsNBwAAHDUaWuav/swYU6S1tpHk3x3PyENT8MBAABHnTZpPldVdXinqs4lubKfkIan4QAAgKNOW55xS5Kfq6qXLu9/z3LZLJ100XUA4IH0Am2X7b19p02an5dFovx3lvdvTfITvUQ0Evv7voQAcBp6gbbL9h7GaYebfKq19pLW2jcv/3tpa+2+voMDAMZPL9B22d7DONWZ5qp6bJLzSR6X5EGHy1trf7anuACAiTjsBTo886kXqF+29zBOW57x8iQvTPKjSZ6c5FlZY5ogADBfeoG2y/YexmmHm1xsrT2hqt7dWvtzR5f1HmFMBAQAjrfJhrihmutO+txNTVQco7HGernDTf60qj4rye9U1d9LcmeSz9tkgAAA69hkQ9xQzXUnfe66MU2pQXBKsR46bYnFc5J8bpJnJ3lCkm9L8sy+ggIA6LLJhrihmutO+tx1Y5pSg+CUYj3UeaZ5OcjkW1prP5jkj7KoZwYAGNQmG+KGaq476XPXjWlKDYJTivXQaWua39xae+IW4jmWmmYA4DhnqYtd9ZpNLd/kOqhp3r5VNc2nTZpfkuTRSf5lkk8cLm+t/cImg1xF0gwAbMKm6oSnWJPL6axKmk9b0/ygJP8xyVOS/JXlf0/bXHgAAP3bVJ3wFGtyuTynunpGa00dMwAweZuqE55iTS6X57QTAV+e5AF1HK2179x4RAAAPVl3MMiq5xswsntOW9P8147cfVCSb0zy+621Z/cV2FFqmgGAdQzZZDbGBre+mxnP8tljdVnDTVprP3+/N3tFkjduKDYAgI0ZsklvjA2CQzYzjnF7nNVpGwHv77FJHrnJQAAANmHIJr0xNggO2cw4xu1xVqetab4nl9Y0fzjJ83qJCADgMgzZpDfGBsEhmxnHuD3O6lQ1zUObfE3znK9MDgAjNMaa5rnHtKnPGDoNutzhJt+Y5PWttY8v7z8syQ2ttV/ceKTHmHTSvKmrqAMAkzX3P++bWr8xbKfLHW7ywsOEOUlaax9L8sJNBTdrm7qKOgAwWXP/876p9Rvzdjpt0nzc805VD73zDot5zp1b7yrqp30+ADB6c//zvqn1G/N2Om15xk8m+ViSf7Zc9HeTfEFr7Tv6C+0zJl2ekUyvmAcA2Li5/3nf1PoNvZ0ut6b5qiT/S5Kvy+IqGrcm+ZHW2ic2HehxJp80AwBMyEmJ69BJbd8ud7jJJ5I8f+NRAQAwKic1442hUW8op6pprqpbl1fMOLz/8Kr61f7CAgBgCCc14425Ua9vp20EfMTyihlJktbaR2MiIADA7JzUjDfmRr2+nfYKGJ+qqmtba7+XJFV1XS6dEAgAwAzs7y/KLo6rWz7psbk7bdL8D5K8sap+PUkl+dokN/UWFQAAg9nfX50Qn/TYnJ2qPKO1dkuSvSTvT/KKJD+Q5D/3GNduODhIzp9f3O6KTa7zLm4/5sf3GGASTnWmuaq+K8lzklyT5B1JnpjkIMlT+gtt5nax/XST67yL24/58T0GmIzTNgI+J8lXJbmjtfbkJF+RxbATzmoX2083uc67uP2YH99jgMk4bdL8J621P0mSqvqc1tr7knxp14uq6vaqendVvaOqLiyX/fWq+q2q+lRVPeDC0TtjF9tPN7nOu7j9mB/fY4DJOG0j4AeX12n+xSS3VtVHk9xxytc+ubX2kSP335Pkm5K89PRhztAutp9ucp13cfsxP77HAJNxqjHal7yg6i8meWiSW1prn+x47u1J9u6XNB8+dluSH2ytdc7HNkYbAIBtuKwx2ke11n59nacneW1VtSQvba3dvO7nAQDA0NZOmtf0pNbanVX1yCzKOt7XWnvDaV5YVTdleS3oa6+9ts8YAQDgRKdtBDyT1tqdy9u7krwqyVev8dqbW2t7rbW9q6++uq8QAQCgU29Jc1VdVVUPOfx3kqdm0QQIAACT0ueZ5kdlMXr7nUnekuQ1rbVbquobq+qDSfaTvKaqfrXHGJgDE9OYg3W/xyc93zEBsHW91TS31j6Q5Ppjlr8qi1IN6GZiGnOw7vf4pOc7JgAG0WtNM1w2E9OYg3W/xyc93zEBMAhJM+NmYhpzsO73+KTnOyYABrH2cJMhGG6y4w4OTExj+tb9Hp/0fMcEQG9WDTeRNAMAwNKqpFl5BgAAdJA0AwBAB0kzAAB0kDQDAEAHSTMAAHSQNNOPs4z53dRoYCOGd8c29vWUvk9TihVgYnobo80OO8uY302NBjZieHdsY19P6fs0pVgBJsiZZjbvLGN+NzUa2Ijh3bGNfT2l79OUYgWYIEkzm3eWMb+bGg1sxPDu2Ma+ntL3aUqxAkyQiYD04yxjfjc1GtiI4d2xjX09pe/TlGIFGCljtAEAoIMx2gAAcEaSZgAA6CBpBgCADpJmAADoIGlmd8x9Wtqq9Zv7eh9nG9tiStt7SrECjJSJgOyGuU9LW7V+c1/v42xjW0xpe08pVoARc6aZ3TD3aWmr1m/u632cbWyLKW3vKcUKMGKSZnbD3KelrVq/ua/3cbaxLaa0vacUK8CIGW7C7pj7tLRV6zf39T7ONrbFlLb3lGIFGJiJgAAA0MFEQAAAOCNJMwAAdJA0AwBAB0kzAAB0kDQDAEAHSfMcGIULl8cxdKkxbg+jwIGBGaM9dUbhwuVxDF1qjNvDKHBgBJxpnjqjcOHyOIYuNcbtYRQ4MAKS5qkzChcuj2PoUmPcHkaBAyNgIuAcGIULl8cxdKkxbg+jwIEtMUYbAAA6GKMNAABnJGkGAIAOkmYAAOggaQYAgA6S5rnre1qWaVxskqlv4zal/TClWIFJMBFwzvqelmUaF5tk6tu4TWk/TClWYDKcaZ6zvqdlmcbFJpn6Nm5T2g9TihWYDEnznPU9Lcs0LjbJ1Ldxm9J+mFKswGQYbjJ3fU/LMo2LTTL1bdymtB+mFCswKiYCAgBABxMBAQDgjCTNAADQQdIMAAAdJM0AANBB0gwAAB0kzVOyybGw677Xro6kNdb57Gyj3WFfAzvAGO2p2ORY2HXfa1dH0hrrfHa20e6wr4Ed4UzzVGxyLOy677WrI2mNdT4722h32NfAjug1aa6q26vq3VX1jqq6sFz2BVV1a1X9zvL24X3GMBubHAu77nvt6khaY53PzjbaHfY1sCN6nQhYVbcn2WutfeTIsn+c5D+11l5UVc9P8vDW2vNOeh8TAZc2ORZ23ffa1ZG0xjqfnW20O+xrYEYGGaO9Iml+f5IbWmsfqqo/k+S21tqXnvQ+kmYAALZhqDHaLclrq+piVd20XPao1tqHlv/+cJJH9RwDAABclr6vnvGk1tqdVfXIJLdW1fuOPthaa1V17KnuZZJ9U5Jce+21PYcJAACr9XqmubV25/L2riSvSvLVSf5gWZaR5e1dK157c2ttr7W2d/XVV/cZJgAAnKi3pLmqrqqqhxz+O8lTk7wnyauTPHP5tGcm+aW+YgAAgE3oszzjUUleVVWHn/OzrbVbquqtSf5FVf3tJHck+Rs9xgCsMuQVD1xtYXeM7Uo9Y/zeOx5gEnpLmltrH0hy/THL/2OSG/v6XOAUhpziZoLc7hjb9NExfu8dDzAZJgLCLhpyipsJcrtjbNNHx/i9dzzAZEiaYRcNOcXNBLndMbbpo2P83jseYDJ6HW6yKYabQA/GWNvJ/Khp7v5sxwOMyiATATdF0gwAwDYMNREQAAAmT9IMAAAdJM0AANBB0gwAAB0kzQAA0EHSDAcHyfnzi1tWb491l8NJ5vC92cYx4biD0ehtjDZMghG2l1p31K/tx1nM4XuzjWPCcQej4kwzu80I20utO+rX9uMs5vC92cYx4biDUZE0s9uMsL3UuqN+bT/OYg7fm20cE447GBUTAcEI20utO+rX9uMs5vC92cYx4biDrTNGGwAAOhijDQAAZyRpBgCADpJmAADoIGkGAIAOkuZNmsOEpjmswxjZrjB+c5i+t26sU1o3GJiJgJsyhwlNc1iHMbJdYfzmMH1v3VintG4wAs40b8ocJjTNYR3GyHaF8ZvD9L11Y53SusEISJo3ZQ4TmuawDmNku8L4zWH63rqxTmndYAQMN9mkOUxomsM6jJHtCuM3h+l768Y6pXWDLTEREAAAOpgICAAAZyRpBgCADpJmAADoIGkGAIAOkmYAAOggaYZN6nskrZG3wFkYrw2XzRht2JS+R9IaeQuchfHasBHONMOm9D2S1shb4CyM14aNkDTDpvQ9ktbIW+AsjNeGjTAREDap75G0Rt4CZ2G8NpyaMdoAANDBGG0AADgjSTMAAHSQNAMAQAdJMwAAdJA0w5BM3QKm5KTfrFWPmZTKTJgICEMxdQuYkpN+s1Y9ZlIqM+JMMwzF1C1gSk76zVr1mEmpzIikGYZi6hYwJSf9Zq16zKRUZsRwExiSqVvAlJz0m7XqMZNSmRgTAQEAoIOJgAAAcEaSZgAA6CBpBgCADpJmAADoIGkGAIAOkmYYI2NhgV011Djus1g31jGuA6dmjDaMjbGwwK4aahz3NmId4zqwFmeaYWyMhQV21VDjuM9i3VjHuA6spfekuarOVdXbq+qXl/efUlVvq6r3VNVPV5Wz3XCUsbDArhpqHPdZrBvrGNeBtfQ+EbCqvj/JXpLPT/L0JHckubG19u+q6oeT3NFae9lJ72EiIDvHWFhgVw01jvss1o11jOvAAwwyRruqrkny00l+JMn3J3lWkje31v7b5eNfm+QFrbVvOOl9JM0AAGzDUGO0fyzJc5N8ann/I0muqKrDQL45yWN6jgEAAC5Lb0lzVT0tyV2ttYuHy9ritPbfTPKjVfWWJPckuW/F62+qqgtVdeHuu+/uK0wAAOjUZxPe1yR5elV9Q5IHJfn8qvp/W2vfluRrk6SqnprkS457cWvt5iQ3J4vyjB7jBACAE/V2prm19oLW2jWtteuyOLv8+tbat1XVI5Okqj4nyfOS/F99xQAAAJswxHWa/6eqem+SdyX5V6211w8QAwAwpL6n4530/mOb5LeNWDe5bmPbftvSWhv9f094whMaADATb3pTaw9+cGvnzi1u3/Sm7b3/qsfWXT6lWDe5bmPbfj1IcqEdk4+aCAgAbFff0/FOev+xTfLbRqybXLexbb8tkjQDANvV93S8k95/bJP8thHrJtdtbNtvi3qfCLgJhpsAwMz0PR3vpPcf2yS/bcS6yXUb2/bbsEEmAm6KpBkAgG0YaiIgAABMnqQZAAA6SJoBAKCDpBkAADpImgEAoIOkGQCAS21j9PXExm5fMXQAAACMyMFBcuONiwl+V16ZvO51m7++8qrP2MZnn5EzzQAAfMY2Rl9PcOy2pBkAgM/YxujrCY7dVp4BAMBn7O8vyiL6HH296jO28dlnZIw2AAAsGaMNAABnJGkGAIAOkmYAAOggaQYAgA6SZgAA6CBpBgCADpJmAADoIGkGAIAOkmYAAOggaQYAgA6SZgAA6CBpBgCADpJmAADoIGkGAIAOkmYAAOggaQYAgA6SZgAA6CBpBgCADpJmAADoIGkGAIAOkmYAAOggaQYAgA6SZgAA6CBpBgCADpJmAADoIGle5eAgOX9+cbvOYwAAzM4VQwcwSgcHyY03Jp/8ZHLllcnrXpfs73c/BgDALDnTfJzbblskxffdt7i97bbTPQYAwCxJmo9zww2Ls8jnzi1ub7jhdI8BADBLyjOOs7+/KLu47bZFUny0/OKkxwAAmKVqrQ0dQ6e9vb124cKFocMAAGDmqupia23v/suVZwAAQAdJMwAAdJA0AwBAB0kzAAB0kDQDAEAHSTMAAHSQNAMAQAdJMwAAdOg9aa6qc1X19qr65eX9G6vqbVX1jqp6Y1V9cd8xAADA5djGmebnJHnvkfsvSfKtrbXHJ/nZJP/zFmIAAIAz6zVprqprkvzlJD9xZHFL8vnLfz80ye/3GQMAAFyuK3p+/x9L8twkDzmy7LuS/EpV/eckf5jkiT3HAAAAl6W3M81V9bQkd7XWLt7voe9L8g2ttWuSvDzJP13x+puq6kJVXbj77rv7ChMAAAqBTZAAAAd2SURBVDr1WZ7xNUmeXlW3J/nnSZ5SVa9Jcn1r7TeXz/m5JH/+uBe31m5ure211vauvvrqHsMEAICT9ZY0t9Ze0Fq7prV2XZK/meT1SZ6R5KFV9SXLp/33ubRJEAAARqfvmuZLtNburarvTvLzVfWpJB9N8p3bjAEAANZVrbWhY+hUVXcnuWOAj35Eko8M8Llsn329O+zr3WFf7w77endsY1//N621B9QGTyJpHkpVXWit7Q0dB/2zr3eHfb077OvdYV/vjiH3tTHaAADQQdIMAAAdJM0nu3noANga+3p32Ne7w77eHfb17hhsX6tpBgCADs40AwBAB0nzClX19VX1/qr63ap6/tDxsDlV9Ziq+rWq+u2q+q2qes5y+RdU1a1V9TvL24cPHSubUVXnqurtVfXLy/tfVFW/uTy+f66qrhw6Ri5fVT2sql5ZVe+rqvdW1b7jep6q6vuWv9/vqapXVNWDHNfzUFU/WVV3VdV7jiw79jiuhf99uc/fVVVf2WdskuZjVNW5JP8syV9K8rgkf6uqHjdsVGzQvUl+oLX2uCRPTPJ3l/v3+Ule11p7bJLXLe8zD8/JpdNH/1GSH22tfXEWQ5b+9iBRsWk/nuSW1tqXJbk+i33uuJ6Zqnp0kmcn2WutfXmSc1lMHnZcz8NPJfn6+y1bdRz/pSSPXf53U5KX9BmYpPl4X53kd1trH2itfTLJP89iBDgz0Fr7UGvtbct/35PFH9ZHZ7GPf3r5tJ9O8leHiZBNqqprkvzlJD+xvF9JnpLklcun2NczUFUPTfIXkrwsSVprn2ytfSyO67m6IsmDq+qKJJ+b5ENxXM9Ca+0NSf7T/RavOo6fkeT/bgtvTvKwqvozfcUmaT7eo5P8hyP3P7hcxsxU1XVJviLJbyZ5VGvtQ8uHPpzkUQOFxWb9WJLnJvnU8v4XJvlYa+3e5X3H9zx8UZK7k7x8WYrzE1V1VRzXs9NauzPJi5P8XhbJ8seTXIzjes5WHcdbzdckzeysqvq8JD+f5H9srf3h0cfa4rIyLi0zcVX1tCR3tdYuDh0LvbsiyVcmeUlr7SuSfCL3K8VwXM/Dsp71GVn8D6X/OslVeeD/nc9MDXkcS5qPd2eSxxy5f81yGTNRVZ+dRcL8M621X1gu/oPD/1tneXvXUPGxMV+T5OlVdXsWZVZPyaLu9WHL/1s3cXzPxQeTfLC19pvL+6/MIol2XM/P1yX59621u1tr/yXJL2RxrDuu52vVcbzVfE3SfLy3JnnsshP3yiwaDF49cExsyLKm9WVJ3tta+6dHHnp1kmcu//3MJL+07djYrNbaC1pr17TWrsviOH59a+1bk/xakm9ePs2+noHW2oeT/Ieq+tLlohuT/HYc13P0e0meWFWfu/w9P9zXjuv5WnUcvzrJty+vovHEJB8/UsaxcYabrFBV35BFLeS5JD/ZWvuRgUNiQ6rqSUl+I8m785k61x/Koq75XyS5NskdSf5Ga+3+zQhMVFXdkOQHW2tPq6o/m8WZ5y9I8vYk39Za+9Mh4+PyVdXjs2j4vDLJB5I8K4uTQ47rmamq/zXJt2RxNaS3J/muLGpZHdcTV1WvSHJDkkck+YMkL0zyiznmOF7+j6b/I4vynD9O8qzW2oXeYpM0AwDAyZRnAABAB0kzAAB0kDQDAEAHSTMAAHSQNAMAQAdJM8CAquqPlrfXVdX/sOH3/qH73X/TJt8fYJdImgHG4bokayXNR6afrXJJ0txa+/NrxgTAkqQZYBxelORrq+odVfV9VXWuqv5JVb21qt5VVd+TLIa0VNVvVNWrs5iClqr6xaq6WFW/VVU3LZe9KMmDl+/3M8tlh2e1a/ne76mqd1fVtxx579uq6pVV9b6q+pnl8IBU1Yuq6reXsbx461sHYGBdZykA2I7nZzmxMEmWye/HW2tfVVWfk+TfVtVrl8/9yiRf3lr798v737mcjvXgJG+tqp9vrT2/qv5ea+3xx3zWNyV5fJLrs5i69daqesPysa9I8t8l+f0k/zbJ11TVe5N8Y5Iva621qnrYxtceYOScaQYYp6cm+faqekcWI96/MMljl4+95UjCnCTPrqp3Jnlzkscced4qT0ryitbafa21P0jy60m+6sh7f7C19qkk78iibOTjSf4kycuq6puyGFcLsFMkzQDjVEn+fmvt8cv/vqi1dnim+ROfflLVDUm+Lsl+a+36JG9P8qDL+Nw/PfLv+5Jc0Vq7N8lXJ3llkqclueUy3h9gkiTNAONwT5KHHLn/q0n+TlV9dpJU1ZdU1VXHvO6hST7aWvvjqvqyJE888th/OXz9/fxGkm9Z1k1fneQvJHnLqsCq6vOSPLS19itJvi+Lsg6AnaKmGWAc3pXkvmWZxU8l+fEsSiPetmzGuzvJXz3mdbck+d5l3fH7syjROHRzkndV1dtaa996ZPmrkuwneWeSluS5rbUPL5Pu4zwkyS9V1YOyOAP+/WdbRYDpqtba0DEAAMCoKc8AAIAOkmYAAOggaQYAgA6SZgAA6CBpBgCADpJmAADoIGkGAIAOkmYAAOjw/wNnY9HOB7zLywAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuTRLc611Ayn",
        "colab_type": "code",
        "outputId": "e870d078-9516-4e5f-a7d2-f0c1e1afa229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "#Codes for Evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "l_list_tmp = l_list.reshape((1401))\n",
        "l_list2_tmp = l_list2.reshape((601))\n",
        "\n",
        "print(confusion_matrix(y_train,l_list_tmp))\n",
        "print(classification_report(y_train,l_list_tmp))\n",
        "print(accuracy_score(y_train, l_list_tmp))\n",
        "\n",
        "print(confusion_matrix(y_test,l_list2_tmp))\n",
        "print(classification_report(y_test,l_list2_tmp))\n",
        "print(accuracy_score(y_test, l_list2_tmp))"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[359 340]\n",
            " [343 359]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.51      0.51       699\n",
            "           1       0.51      0.51      0.51       702\n",
            "\n",
            "    accuracy                           0.51      1401\n",
            "   macro avg       0.51      0.51      0.51      1401\n",
            "weighted avg       0.51      0.51      0.51      1401\n",
            "\n",
            "0.5124910778015703\n",
            "[[148 154]\n",
            " [154 145]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.49      0.49       302\n",
            "           1       0.48      0.48      0.48       299\n",
            "\n",
            "    accuracy                           0.49       601\n",
            "   macro avg       0.49      0.49      0.49       601\n",
            "weighted avg       0.49      0.49      0.49       601\n",
            "\n",
            "0.4875207986688852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDpSEICHtNlB",
        "colab_type": "code",
        "outputId": "a90fb1a0-b97b-4fea-d365-c7c8f73f2ceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class_1 = 0\n",
        "\n",
        "for i in range(0, 601):\n",
        "  if l_list2_tmp[i] == 1:\n",
        "    class_1 = class_1 + 1\n",
        "\n",
        "print(class_1)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbPP4IIn3lMu",
        "colab_type": "code",
        "outputId": "7a172f6d-367d-4712-da97-d5aa8d1066dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(y_train)\n",
        "print(l_list_tmp)"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 ... 1 0 1]\n",
            "[1. 0. 0. ... 0. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}