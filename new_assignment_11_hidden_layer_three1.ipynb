{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new_assignment_11.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOVXuJvW+yREj5kiu/JVPKa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlguswn3659/class-MachineLearning/blob/master/new_assignment_11_hidden_layer_three1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbdQEPZ_o5JX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import math\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re,string,unicodedata\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMRdc5kpo9vM",
        "colab_type": "code",
        "outputId": "f6377a9e-3c17-4441-e262-53397533ee86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# import zipfile\n",
        "# import io\n",
        "# zf = zipfile.ZipFile(io.BytesIO(uploaded['movie_review.zip']), \"r\")\n",
        "# zf.extractall()\n",
        "\n",
        "review_data = load_files(r\"movie_review\")\n",
        "X, y = review_data.data, review_data.target\n",
        "\n",
        "documents = []\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "for sen in range(0, len(X)):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    \n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    \n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    \n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    \n",
        "    # Lemmatization\n",
        "    document = document.split()\n",
        "    document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    \n",
        "    documents.append(document)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_uv_iLio-76",
        "colab_type": "code",
        "outputId": "f200e4b5-bad1-433b-a025-9a1ee616bbe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=40000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "# vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents).toarray()\n",
        "print(X)\n",
        "\n",
        "vector_size = 749\n",
        "\n",
        "###################################################\n",
        "\n",
        "# # tfidfconverter = TfidfTransformer()\n",
        "# # print(tfidfconverter)\n",
        "# # X = tfidfconverter.fit_transform(X).toarray()\n",
        "# # print(X)\n",
        "\n",
        "# X_train_tmp, X_test_tmp, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
        "\n",
        "# # print(X_train.shape)\n",
        "\n",
        "# X_pos_sum = np.zeros(12638)\n",
        "# X_neg_sum = np.zeros(12638)\n",
        "\n",
        "# X_train = np.zeros((1401, 897))\n",
        "# X_test = np.zeros((601, 897))\n",
        "\n",
        "# for i in range(0, 2002):\n",
        "#   if y[i] == 0:\n",
        "#     X_neg_sum = X_neg_sum + X[i]\n",
        "#   else:\n",
        "#     X_pos_sum = X_pos_sum + X[i]\n",
        "\n",
        "# pos_neg_voca = 0\n",
        "\n",
        "# for i in range(0, 12638):\n",
        "#   # if abs(X_neg_sum[i] - X_pos_sum[i]) >= 30:\n",
        "#   if (X_neg_sum[i]*4 <= X_pos_sum[i] and X_pos_sum[i] > 8) or (X_pos_sum[i]*4 <= X_neg_sum[i] and X_neg_sum[i] > 8):\n",
        "#     for j in range(0, 1401):\n",
        "#       X_train[j][pos_neg_voca] = X_train_tmp[j][i]\n",
        "#     for j in range(0, 601):\n",
        "#       X_test[j][pos_neg_voca] = X_test_tmp[j][i]\n",
        "#     pos_neg_voca  = pos_neg_voca + 1\n",
        "\n",
        "# # tfidfconverter = TfidfTransformer()\n",
        "# # X_train = tfidfconverter.fit_transform(X_train).toarray()\n",
        "# # X_test = tfidfconverter.fit_transform(X_test).toarray()\n",
        "\n",
        "# print(X_neg_sum)\n",
        "# print(X_pos_sum)\n",
        "# print(pos_neg_voca)\n",
        "\n",
        "#################################################\n",
        "\n",
        "list_x=X.tolist()\n",
        "negative=[]\n",
        "positive=[]\n",
        "for i in range(0,2002):\n",
        "  if y[i]==0:\n",
        "    negative.append(list_x[i])\n",
        "  else:\n",
        "    positive.append(list_x[i])\n",
        "negative=np.array(negative)\n",
        "positive=np.array(positive)\n",
        "total_sum=X.sum(axis=0)\n",
        "neg_sum=negative.sum(axis=0)\n",
        "pos_sum=positive.sum(axis=0)\n",
        "\n",
        "neg_ratio=neg_sum/total_sum\n",
        "pos_ratio=pos_sum/total_sum\n",
        "\n",
        "good=[]\n",
        "for i in range(0,len(pos_sum)):\n",
        "  if neg_ratio[i]>pos_ratio[i]*3 and neg_ratio[i] > 0.88:\n",
        "    # print(neg_ratio[i])\n",
        "    good.append(i)\n",
        "  elif pos_ratio[i]>neg_ratio[i]*3 and pos_ratio[i] > 0.88:\n",
        "    # print(pos_ratio[i])\n",
        "    good.append(i)\n",
        "print(\"good의 길이:   \" , len(good))\n",
        "new=np.ones((1,2002), dtype=float)\n",
        "for i in range(0,len(good)):\n",
        "  temp=X[:,good[i]]\n",
        "  temp=temp[np.newaxis]\n",
        "  new=np.vstack((new,temp))\n",
        "new=np.transpose(new)\n",
        "new=new[:,1:]\n",
        "# vectorizer = CountVectorizer(max_features=2, min_df=0, stop_words=stopwords.words('english'))\n",
        "# Xp = vectorizer.fit_transform(pluss).toarray()\n",
        "\n",
        "# tfidfconverter = TfidfTransformer()\n",
        "# X = tfidfconverter.fit_transform(X).toarray()\n",
        "X_train, X_test, y_train, y_test = train_test_split(new, y, test_size=0.3, shuffle=False)\n",
        "\n",
        "print(X_train)\n",
        "print(X_test)\n",
        "\n",
        "print(X_train[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "good의 길이:    748\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlTm8tUMrWdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#first hidden layer로 가기 위한 초기 fully connected u 생성\n",
        "w_tmp = np.ones(shape=(1401, 100, vector_size))\n",
        "v_tmp = np.ones(shape=(1401, 12, 197))\n",
        "u_tmp = np.ones(shape=(1401, 1, 12))\n",
        "\n",
        "matrix_u = np.zeros((100, vector_size))  #100줄\n",
        "matrix_u[0][0] = 1               #y레이어의 bias를 위한 값 1\n",
        "\n",
        "for i in range(1, 100):\n",
        "  for j in range(0, vector_size):\n",
        "    matrix_u[i][j] = np.random.normal()\n",
        "\n",
        "\n",
        "# matrix_u[196][0] = 0            #줄, 칸\n",
        "##############################################\n",
        "matrix_v = np.zeros((12, 100))  #12줄\n",
        "matrix_v[0][0] = 1               #y레이어의 bias를 위한 값 1\n",
        "\n",
        "for i in range(1, 12):\n",
        "  for j in range(0, 100):\n",
        "    matrix_v[i][j] = np.random.normal()\n",
        "\n",
        "\n",
        "# matrix_v[196][0] = 0            #줄, 칸\n",
        "##############################################\n",
        "matrix_w = np.zeros((1, 12))  #10줄\n",
        "# matrix_w[0][0] = 1               #y레이어의 bias를 위한 값 1\n",
        "\n",
        "for i in range(0, 1):\n",
        "  for j in range(0, 12):\n",
        "    matrix_w[i][j] = np.random.normal()\n",
        "\n",
        "w_tmp = matrix_w\n",
        "v_tmp = matrix_v\n",
        "u_tmp = matrix_u\n",
        "\n",
        "# matrix_w[196][0] = 0            #줄, 칸\n",
        "\n",
        "# print(matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrFN9qH2rXrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "\n",
        "y_ = np.ones((100, 1))\n",
        "\n",
        "x_list = np.ones(shape=(1401, vector_size, 1))\n",
        "\n",
        "y__list = np.ones(shape=(1401, 100, 1))\n",
        "z__list = np.ones(shape=(1401, 12, 1))\n",
        "h__list = np.ones(shape=(1401, 1, 1))\n",
        "\n",
        "y_list = np.ones(shape=(1401, 100, 1))\n",
        "z_list = np.ones(shape=(1401, 12, 1))\n",
        "h_list = np.ones(shape=(1401, 1, 1))\n",
        "l_list = np.zeros(shape=(1401, 1, 1))\n",
        "\n",
        "y_list_tmp = np.ones(shape=(1401, 100, 1))\n",
        "z_list_tmp = np.ones(shape=(1401, 12, 1))\n",
        "h_list_tmp = np.ones(shape=(1401, 1, 1))\n",
        "y__list_tmp = np.ones(shape=(1401, 100, 1))\n",
        "z__list_tmp = np.ones(shape=(1401, 12, 1))\n",
        "h__list_tmp = np.ones(shape=(1401, 1, 1))\n",
        "\n",
        "\n",
        "#training set 1401개에 대해서\n",
        "for i in range(0, 1401):\n",
        "  bias   = [1]       #bias 1\n",
        "  im_vector = np.concatenate((bias, X_train[i]), axis = None)\n",
        "\n",
        "  im_matrix   = im_vector.reshape((vector_size, 1))\n",
        "  \n",
        "  x_list[i] = im_matrix\n",
        "\n",
        "  y_ = np.dot(matrix_u, im_matrix)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "  y__list[i] = y_\n",
        "  \n",
        "  y_vector = np.ones((100,1)) #bias\n",
        "  for j in range (1, 100):\n",
        "    y_vector[j][0] = 1 / (1 + math.exp(-y_[j][0]))\n",
        "    # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  y   = y_vector.reshape((100, 1))\n",
        "  y_list[i] = y\n",
        "\n",
        "  z_  = np.dot(matrix_v, y)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "  z__list[i] = z_\n",
        "  z_vector = np.ones((12,1)) #bias\n",
        "  for j in range (1, 12):\n",
        "    z_vector[j][0] = 1 / (1 + math.exp(-z_[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  z   = z_vector.reshape((12, 1))\n",
        "  z_list[i] = z\n",
        "  \n",
        "  # print(z)\n",
        "\n",
        "  h_  = np.dot(matrix_w, z)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "  h__list[i] = h_\n",
        "  \n",
        "  h_vector = np.ones((1,1)) #bias\n",
        "  for j in range (0, 1):\n",
        "    h_vector[j][0] = 1 / (1 + math.exp(-h_[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "  h   = h_vector.reshape((1, 1))\n",
        "  \n",
        "  h_list[i] = h\n",
        "\n",
        "  tmp_label = 0\n",
        "  # print(h)\n",
        "\n",
        "  if h >= 0.5:\n",
        "    tmp_label = 1\n",
        "\n",
        "  l_list[i][0] = tmp_label\n",
        "\n",
        "  y_list_tmp[i] = y_list[i]\n",
        "  z_list_tmp[i] = z_list[i]\n",
        "  h_list_tmp[i] = h_list[i]\n",
        "  y__list_tmp[i] = y__list[i]\n",
        "  z__list_tmp[i] = z__list[i]\n",
        "  h__list_tmp[i] = h__list[i]\n",
        "\n",
        "# print(l_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB-qa4x8rZD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "\n",
        "y_2 = np.ones((100, 1))\n",
        "\n",
        "x_list2 = np.ones(shape=(601, vector_size, 1))\n",
        "\n",
        "y__list2 = np.ones(shape=(601, 100, 1))\n",
        "z__list2 = np.ones(shape=(601, 12, 1))\n",
        "h__list2 = np.ones(shape=(601, 1, 1))\n",
        "\n",
        "y_list2 = np.ones(shape=(601, 100, 1))\n",
        "z_list2 = np.ones(shape=(601, 12, 1))\n",
        "h_list2 = np.ones(shape=(601, 1, 1))\n",
        "l_list2 = np.zeros(shape=(601, 1, 1))\n",
        "\n",
        "y_list_tmp2 = np.ones(shape=(601, 100, 1))\n",
        "z_list_tmp2 = np.ones(shape=(601, 12, 1))\n",
        "h_list_tmp2 = np.ones(shape=(601, 1, 1))\n",
        "y__list_tmp2 = np.ones(shape=(601, 100, 1))\n",
        "z__list_tmp2 = np.ones(shape=(601, 12, 1))\n",
        "h__list_tmp2 = np.ones(shape=(601, 1, 1))\n",
        "\n",
        "\n",
        "#training set 1000개에 대해서\n",
        "for i in range(0, 601):\n",
        "  bias2   = [1]       #bias 1\n",
        "  im_vector2 = np.concatenate((bias2, X_test[i]), axis = None)\n",
        "\n",
        "  # print(type(list_image[: , i]))\n",
        "  # print(list_image[:, i])\n",
        "  # print(im_vector)\n",
        "  im_matrix2   = im_vector2.reshape((vector_size, 1))\n",
        "  \n",
        "  x_list2[i] = im_matrix2\n",
        "\n",
        "  y_2 = np.dot(matrix_u, im_matrix)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "  y__list2[i] = y_2\n",
        "  \n",
        "  y_vector2 = np.ones((100,1)) #bias\n",
        "  for j in range (1, 100):\n",
        "    y_vector2[j][0] = 1 / (1 + math.exp(-y_2[j][0]))\n",
        "    # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  y2   = y_vector2.reshape((100, 1))\n",
        "  y_list2[i] = y2\n",
        "\n",
        "  z_2  = np.dot(matrix_v, y2)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "  z__list2[i] = z_2\n",
        "  z_vector2 = np.ones((12,1)) #bias\n",
        "  for j in range (1, 12):\n",
        "    z_vector2[j][0] = 1 / (1 + math.exp(-z_2[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  z2   = z_vector2.reshape((12, 1))\n",
        "  z_list2[i] = z2\n",
        "  \n",
        "  # print(z)\n",
        "\n",
        "  h_2  = np.dot(matrix_w, z2)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "  h__list2[i] = h_2\n",
        "  \n",
        "  h_vector2 = np.ones((1,1)) #bias\n",
        "  for j in range (0, 1):\n",
        "    h_vector2[j][0] = 1 / (1 + math.exp(-h_2[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "  h2   = h_vector2.reshape((1, 1))\n",
        "  \n",
        "  h_list2[i] = h2\n",
        "\n",
        "  tmp_label = 0\n",
        "\n",
        "  if h2 >= 0.5:\n",
        "    tmp_label = 1\n",
        "\n",
        "  l_list2[i][0] = tmp_label\n",
        "\n",
        "  y_list_tmp2[i] = y_list2[i]\n",
        "  z_list_tmp2[i] = z_list2[i]\n",
        "  h_list_tmp2[i] = h_list2[i]\n",
        "  y__list_tmp2[i] = y__list2[i]\n",
        "  z__list_tmp2[i] = z__list2[i]\n",
        "  h__list_tmp2[i] = h__list2[i]\n",
        "\n",
        "# print(l_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j28x7DrlraXJ",
        "colab_type": "code",
        "outputId": "95b23260-a6be-410f-84f3-fec3d76d4304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5267
        }
      },
      "source": [
        "#이제 여기에다가\n",
        "\n",
        "#iteration에 따라\n",
        "#cost function 값을 배열에 저장하고\n",
        "#세타값을 업데이트 시키고(위의 세개 함수 호출)\n",
        "#업데이트 시킨값을 다시 cost함수에 넣고\n",
        "#업데이트 시키고\n",
        "#이것을 반복하는 함수를 짠다.\n",
        "\n",
        "#아직 해야하는 것 : h값에 따라 가장 큰값의 index로 L값정하는거 위에 추가해야함.\n",
        "\n",
        "iteration = 300\n",
        "l_r = 0.05\n",
        "lambda_ = 1\n",
        "\n",
        "\n",
        "#####################################\n",
        "\n",
        "cost_history = np.zeros(iteration)    #about train\n",
        "cost_history2 = np.zeros(iteration)   #about test\n",
        "\n",
        "label_group = np.zeros(shape = (1401, 1))\n",
        "label_group2 = np.zeros(shape = (601, 1))\n",
        "\n",
        "accuracy_history = np.zeros(iteration)\n",
        "accuracy_history2 = np.zeros(iteration)\n",
        "\n",
        "for i in range(0, 1401):\n",
        "  label_group[i][0] = l_list[i][0]\n",
        "\n",
        "for i in range(0, 601):\n",
        "  label_group2[i][0] = l_list2[i][0]\n",
        "\n",
        "#############################################\n",
        "\n",
        "\n",
        "for t in range(0, iteration):\n",
        "  number_of_correct = 0\n",
        "  number_of_correct2 = 0\n",
        "  \n",
        "  print(t, end=' ')\n",
        "\n",
        "  parameter_sum = 0\n",
        "\n",
        "  for q in range(0, vector_size):\n",
        "    for w in range(1, 100):\n",
        "      parameter_sum = parameter_sum + matrix_u[w][q]**2\n",
        "\n",
        "  for q in range(0, 100):\n",
        "    for w in range(1, 12):\n",
        "      parameter_sum = parameter_sum + matrix_u[w][q]**2\n",
        "\n",
        "  for q in range(0, 12):\n",
        "    for w in range(0, 1):\n",
        "      parameter_sum = parameter_sum + matrix_u[w][q]**2\n",
        "\n",
        "  result = 0.0\n",
        "  for i in range(0, 1401):\n",
        "    for k in range(0, 1):\n",
        "      # if h_list_tmp[i][k][0] <= 0:\n",
        "      # print(h_list_tmp[i][k][0])\n",
        "      result = result + ((-1)*label_group[i][k]*math.log(h_list[i][k][0]) - (1-label_group[i][k])*math.log(1-h_list[i][k][0])) + ((lambda_)/(2 * 91012)) * parameter_sum\n",
        "  result = result / 1401\n",
        "  cost_history[t] = result\n",
        "\n",
        "##gradient descent\n",
        "  result_w = 0\n",
        "  result_v = 0\n",
        "  result_u = 0\n",
        "\n",
        "  for i in range(0, 1401):\n",
        "    small_delta_2 = np.zeros(shape = (1, 1))\n",
        "    for j in range(0, 1):\n",
        "      small_delta_2[j][0] = h_list[i][j] - label_group[i][j]\n",
        "    # print(h_list[i])\n",
        "    # print(label_group[i])\n",
        "    # small_delta_2 = h_list[i] - label_group[i] #여기 label_group 다시 봐야함.\n",
        "    small_delta_1 = np.dot(np.transpose(matrix_w), small_delta_2)\n",
        "    small_delta_0 = np.dot(np.transpose(matrix_v), small_delta_1)*(y_list_tmp[i]*(1 - y_list_tmp[i]))\n",
        "\n",
        "    # print(small_delta_2)\n",
        "    result_w = result_w + np.dot(small_delta_2, np.transpose(z_list[i]))\n",
        "    result_v = result_v + np.dot(small_delta_1, np.transpose(y_list[i]))\n",
        "    result_u = result_u + np.dot(small_delta_0, np.transpose(x_list[i]))\n",
        "\n",
        "  matrix_w = matrix_w - (result_w / 1401 *l_r)\n",
        "  matrix_v = matrix_v - (result_v / 1401 *l_r)\n",
        "  matrix_u = matrix_u - (result_u / 1401 *l_r)\n",
        "\n",
        "\n",
        "####################################################################\n",
        "\n",
        "  for k in range(0, 1401):\n",
        "    bias   = [1]       #bias 1\n",
        "    im_vector = np.concatenate((bias, X_train[k]), axis = None)\n",
        "\n",
        "    im_matrix   = im_vector.reshape((vector_size, 1))\n",
        "\n",
        "    y_ = np.dot(matrix_u, im_matrix)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "    y__list[k] = y_\n",
        "    \n",
        "    y_vector = np.ones((100,1)) #bias\n",
        "    for j in range (1, 100):\n",
        "      y_vector[j][0] = 1 / (1 + math.exp(-y_[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "    \n",
        "    # print(y_vector) #시그모이드 값 확인\n",
        "    y   = y_vector.reshape((100, 1))\n",
        "    y_list[k] = y\n",
        "\n",
        "    z_  = np.dot(matrix_v, y)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "    z__list[k] = z_\n",
        "    z_vector = np.ones((12,1)) #bias\n",
        "    for j in range (1, 12):\n",
        "      z_vector[j][0] = 1 / (1 + math.exp(-z_[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "    z   = z_vector.reshape((12, 1))\n",
        "    z_list[k] = z\n",
        "    \n",
        "    # print(z)\n",
        "\n",
        "    h_  = np.dot(matrix_w, z)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "    h__list[k] = h_\n",
        "    \n",
        "    h_vector = np.ones((1,1)) #bias\n",
        "    for j in range (0, 1):\n",
        "      h_vector[j][0] = 1 / (1 + math.exp(-h_[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "    h   = h_vector.reshape((1, 1))\n",
        "\n",
        "    h_list[k] = h\n",
        "\n",
        "    tmp_label = 0\n",
        "\n",
        "    if h >= 0.5:\n",
        "      # print(h)\n",
        "      tmp_label = 1\n",
        "\n",
        "    l_list[k][0] = tmp_label\n",
        "\n",
        "    y_list_tmp[k] = y_list[k]\n",
        "    z_list_tmp[k] = z_list[k]\n",
        "    h_list_tmp[k] = h_list[k]\n",
        "    y__list_tmp[k] = y__list[k]\n",
        "    z__list_tmp[k] = z__list[k]\n",
        "    h__list_tmp[k] = h__list[k]\n",
        "\n",
        "    if l_list[k][0] == y_train[k]:\n",
        "      number_of_correct = number_of_correct + 1\n",
        "\n",
        "  accuracy_history[t] = number_of_correct / 1401 * 100\n",
        "  print(accuracy_history[t], end=' ')\n",
        "\n",
        "############################\n",
        "  #test data!\n",
        "  \n",
        "  # print(t, end=' ')\n",
        "\n",
        "  result2 = 0.0\n",
        "  for i in range(0, 601):\n",
        "    for k in range(0, 1):\n",
        "      result2 = result2 + ((-1)*label_group2[i][k]*math.log(h_list2[i][k][0]) - (1-label_group2[i][k])*math.log(1-h_list2[i][k][0])) + ((lambda_)/(2 * 91012)) * parameter_sum\n",
        "  result2 = result2 / 601\n",
        "  cost_history2[t] = result2\n",
        "\n",
        "\n",
        "  for k in range(0, 601):\n",
        "  # print(i)\n",
        "    bias2   = [1]       #bias 1\n",
        "    im_vector2 = np.concatenate((bias2, X_test[k]), axis = None)\n",
        "\n",
        "    im_matrix2   = im_vector2.reshape((vector_size, 1))\n",
        "\n",
        "    y_2 = np.dot(matrix_u, im_matrix2)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "    y__list2[k] = y_2\n",
        "    \n",
        "    y_vector2 = np.ones((100,1)) #bias\n",
        "    for j in range (1, 100):\n",
        "      y_vector2[j][0] = 1 / (1 + math.exp(-y_2[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "    \n",
        "    # print(y_vector) #시그모이드 값 확인\n",
        "    y2   = y_vector2.reshape((100, 1))\n",
        "    y_list2[k] = y2\n",
        "\n",
        "    z_2  = np.dot(matrix_v, y2)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "    z__list2[k] = z_2\n",
        "    z_vector2 = np.ones((12,1)) #bias\n",
        "    for j in range (1, 12):\n",
        "      z_vector2[j][0] = 1 / (1 + math.exp(-z_2[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "    z2   = z_vector2.reshape((12, 1))\n",
        "    z_list2[k] = z2\n",
        "    \n",
        "    # print(z)\n",
        "\n",
        "    h_2  = np.dot(matrix_w, z2)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "    h__list2[k] = h_2\n",
        "    \n",
        "    h_vector2 = np.ones((1,1)) #bias\n",
        "    for j in range (0, 1):\n",
        "      h_vector2[j][0] = 1 / (1 + math.exp(-h_2[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "    h2   = h_vector2.reshape((1, 1))\n",
        "\n",
        "    h_list2[k] = h2\n",
        "\n",
        "    tmp_label2 = 0\n",
        "\n",
        "    if h2 >= 0.5:\n",
        "      # print(h)\n",
        "      tmp_label2 = 1\n",
        "\n",
        "    l_list2[k][0] = tmp_label2\n",
        "\n",
        "    y_list_tmp2[k] = y_list2[k]\n",
        "    z_list_tmp2[k] = z_list2[k]\n",
        "    h_list_tmp2[k] = h_list2[k]\n",
        "    y__list_tmp2[k] = y__list2[k]\n",
        "    z__list_tmp2[k] = z__list2[k]\n",
        "    h__list_tmp2[k] = h__list2[k]\n",
        "\n",
        "    if l_list2[k][0] == y_test[k]:\n",
        "      number_of_correct2 = number_of_correct2 + 1\n",
        "\n",
        "  accuracy_history2[t] = number_of_correct2 / 601 * 100\n",
        "  print(accuracy_history2[t])\n",
        "\n",
        "  t = t + 1"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 57.45895788722342 57.07154742096506\n",
            "1 57.53033547466095 56.905158069883534\n",
            "2 57.88722341184867 57.237936772046595\n",
            "3 58.02997858672377 56.905158069883534\n",
            "4 58.02997858672377 56.738768718802\n",
            "5 57.81584582441114 56.905158069883534\n",
            "6 57.88722341184867 56.738768718802\n",
            "7 57.88722341184867 56.738768718802\n",
            "8 57.88722341184867 56.738768718802\n",
            "9 57.958600999286226 56.738768718802\n",
            "10 57.958600999286226 56.57237936772047\n",
            "11 58.10135617416131 56.57237936772047\n",
            "12 58.10135617416131 56.738768718802\n",
            "13 57.958600999286226 56.905158069883534\n",
            "14 57.958600999286226 56.905158069883534\n",
            "15 58.10135617416131 56.905158069883534\n",
            "16 58.17273376159886 56.905158069883534\n",
            "17 58.2441113490364 56.905158069883534\n",
            "18 58.17273376159886 56.905158069883534\n",
            "19 58.10135617416131 57.07154742096506\n",
            "20 58.10135617416131 57.07154742096506\n",
            "21 58.10135617416131 57.07154742096506\n",
            "22 58.10135617416131 57.07154742096506\n",
            "23 58.17273376159886 57.07154742096506\n",
            "24 58.17273376159886 57.237936772046595\n",
            "25 58.10135617416131 57.237936772046595\n",
            "26 58.10135617416131 57.40432612312812\n",
            "27 58.10135617416131 57.40432612312812\n",
            "28 58.10135617416131 57.40432612312812\n",
            "29 58.10135617416131 57.40432612312812\n",
            "30 58.10135617416131 57.237936772046595\n",
            "31 58.10135617416131 57.237936772046595\n",
            "32 58.10135617416131 57.237936772046595\n",
            "33 58.02997858672377 57.237936772046595\n",
            "34 58.02997858672377 57.07154742096506\n",
            "35 58.02997858672377 57.07154742096506\n",
            "36 58.02997858672377 57.07154742096506\n",
            "37 58.02997858672377 57.07154742096506\n",
            "38 58.02997858672377 57.07154742096506\n",
            "39 58.02997858672377 57.07154742096506\n",
            "40 58.02997858672377 57.237936772046595\n",
            "41 57.81584582441114 57.237936772046595\n",
            "42 57.81584582441114 57.237936772046595\n",
            "43 57.81584582441114 57.237936772046595\n",
            "44 57.81584582441114 57.237936772046595\n",
            "45 57.88722341184867 57.40432612312812\n",
            "46 57.88722341184867 57.40432612312812\n",
            "47 58.02997858672377 57.40432612312812\n",
            "48 58.17273376159886 57.570715474209656\n",
            "49 58.17273376159886 57.570715474209656\n",
            "50 58.17273376159886 57.570715474209656\n",
            "51 58.17273376159886 57.73710482529118\n",
            "52 58.2441113490364 57.73710482529118\n",
            "53 58.2441113490364 57.73710482529118\n",
            "54 58.2441113490364 57.73710482529118\n",
            "55 58.2441113490364 57.570715474209656\n",
            "56 58.2441113490364 57.570715474209656\n",
            "57 58.38686652391149 57.570715474209656\n",
            "58 58.38686652391149 57.73710482529118\n",
            "59 58.38686652391149 57.73710482529118\n",
            "60 58.38686652391149 57.73710482529118\n",
            "61 58.38686652391149 57.73710482529118\n",
            "62 58.38686652391149 57.73710482529118\n",
            "63 58.38686652391149 57.570715474209656\n",
            "64 58.45824411134903 57.570715474209656\n",
            "65 58.45824411134903 57.570715474209656\n",
            "66 58.45824411134903 57.40432612312812\n",
            "67 58.31548893647395 57.40432612312812\n",
            "68 58.31548893647395 57.40432612312812\n",
            "69 58.2441113490364 57.40432612312812\n",
            "70 58.10135617416131 57.40432612312812\n",
            "71 58.10135617416131 57.40432612312812\n",
            "72 58.17273376159886 57.40432612312812\n",
            "73 58.17273376159886 57.40432612312812\n",
            "74 58.10135617416131 57.40432612312812\n",
            "75 58.10135617416131 57.40432612312812\n",
            "76 58.02997858672377 57.40432612312812\n",
            "77 58.10135617416131 57.40432612312812\n",
            "78 58.10135617416131 57.40432612312812\n",
            "79 58.17273376159886 57.40432612312812\n",
            "80 58.17273376159886 57.40432612312812\n",
            "81 58.17273376159886 57.40432612312812\n",
            "82 58.17273376159886 57.40432612312812\n",
            "83 58.17273376159886 57.40432612312812\n",
            "84 58.17273376159886 57.237936772046595\n",
            "85 58.10135617416131 57.237936772046595\n",
            "86 58.10135617416131 57.237936772046595\n",
            "87 58.10135617416131 57.237936772046595\n",
            "88 58.10135617416131 57.40432612312812\n",
            "89 58.10135617416131 57.40432612312812\n",
            "90 58.10135617416131 57.40432612312812\n",
            "91 58.17273376159886 57.570715474209656\n",
            "92 58.17273376159886 57.570715474209656\n",
            "93 58.2441113490364 57.570715474209656\n",
            "94 58.2441113490364 57.570715474209656\n",
            "95 58.2441113490364 57.570715474209656\n",
            "96 58.2441113490364 57.570715474209656\n",
            "97 58.2441113490364 57.570715474209656\n",
            "98 58.2441113490364 57.570715474209656\n",
            "99 58.2441113490364 57.570715474209656\n",
            "100 58.2441113490364 57.40432612312812\n",
            "101 58.2441113490364 57.570715474209656\n",
            "102 58.2441113490364 57.570715474209656\n",
            "103 58.2441113490364 57.570715474209656\n",
            "104 58.2441113490364 57.570715474209656\n",
            "105 58.2441113490364 57.570715474209656\n",
            "106 58.31548893647395 57.570715474209656\n",
            "107 58.31548893647395 57.90349417637272\n",
            "108 58.31548893647395 57.90349417637272\n",
            "109 58.38686652391149 57.90349417637272\n",
            "110 58.38686652391149 57.90349417637272\n",
            "111 58.38686652391149 57.90349417637272\n",
            "112 58.529621698786585 58.069883527454245\n",
            "113 58.60099928622412 58.069883527454245\n",
            "114 58.60099928622412 58.069883527454245\n",
            "115 58.60099928622412 58.069883527454245\n",
            "116 58.60099928622412 58.069883527454245\n",
            "117 58.60099928622412 57.73710482529118\n",
            "118 58.60099928622412 57.90349417637272\n",
            "119 58.60099928622412 57.90349417637272\n",
            "120 58.60099928622412 57.90349417637272\n",
            "121 58.60099928622412 57.90349417637272\n",
            "122 58.529621698786585 57.90349417637272\n",
            "123 58.529621698786585 57.90349417637272\n",
            "124 58.60099928622412 57.73710482529118\n",
            "125 58.60099928622412 57.73710482529118\n",
            "126 58.60099928622412 57.73710482529118\n",
            "127 58.60099928622412 57.73710482529118\n",
            "128 58.60099928622412 57.73710482529118\n",
            "129 58.67237687366167 57.73710482529118\n",
            "130 58.67237687366167 57.73710482529118\n",
            "131 58.67237687366167 57.73710482529118\n",
            "132 58.67237687366167 57.90349417637272\n",
            "133 58.67237687366167 57.73710482529118\n",
            "134 58.67237687366167 57.73710482529118\n",
            "135 58.67237687366167 57.73710482529118\n",
            "136 58.67237687366167 57.73710482529118\n",
            "137 58.67237687366167 57.90349417637272\n",
            "138 58.67237687366167 58.069883527454245\n",
            "139 58.67237687366167 58.069883527454245\n",
            "140 58.67237687366167 58.069883527454245\n",
            "141 58.67237687366167 58.069883527454245\n",
            "142 58.67237687366167 58.069883527454245\n",
            "143 58.67237687366167 57.90349417637272\n",
            "144 58.67237687366167 57.90349417637272\n",
            "145 58.67237687366167 57.90349417637272\n",
            "146 58.67237687366167 57.90349417637272\n",
            "147 58.67237687366167 57.90349417637272\n",
            "148 58.67237687366167 57.90349417637272\n",
            "149 58.67237687366167 57.90349417637272\n",
            "150 58.67237687366167 57.90349417637272\n",
            "151 58.67237687366167 57.90349417637272\n",
            "152 58.67237687366167 57.90349417637272\n",
            "153 58.67237687366167 57.90349417637272\n",
            "154 58.60099928622412 57.90349417637272\n",
            "155 58.67237687366167 57.90349417637272\n",
            "156 58.67237687366167 57.90349417637272\n",
            "157 58.67237687366167 57.90349417637272\n",
            "158 58.67237687366167 57.90349417637272\n",
            "159 58.67237687366167 57.90349417637272\n",
            "160 58.67237687366167 57.90349417637272\n",
            "161 58.67237687366167 57.90349417637272\n",
            "162 58.67237687366167 57.90349417637272\n",
            "163 58.67237687366167 58.069883527454245\n",
            "164 58.60099928622412 58.069883527454245\n",
            "165 58.60099928622412 58.069883527454245\n",
            "166 58.67237687366167 58.069883527454245\n",
            "167 58.67237687366167 58.069883527454245\n",
            "168 58.743754461099215 58.069883527454245\n",
            "169 58.743754461099215 58.069883527454245\n",
            "170 58.67237687366167 58.069883527454245\n",
            "171 58.67237687366167 58.069883527454245\n",
            "172 58.67237687366167 58.069883527454245\n",
            "173 58.67237687366167 58.069883527454245\n",
            "174 58.67237687366167 58.069883527454245\n",
            "175 58.67237687366167 58.069883527454245\n",
            "176 58.60099928622412 58.069883527454245\n",
            "177 58.60099928622412 58.069883527454245\n",
            "178 58.60099928622412 58.069883527454245\n",
            "179 58.60099928622412 58.069883527454245\n",
            "180 58.529621698786585 58.069883527454245\n",
            "181 58.45824411134903 57.90349417637272\n",
            "182 58.45824411134903 57.90349417637272\n",
            "183 58.45824411134903 57.73710482529118\n",
            "184 58.45824411134903 57.73710482529118\n",
            "185 58.45824411134903 57.73710482529118\n",
            "186 58.45824411134903 57.73710482529118\n",
            "187 58.45824411134903 57.73710482529118\n",
            "188 58.45824411134903 57.90349417637272\n",
            "189 58.38686652391149 57.73710482529118\n",
            "190 58.38686652391149 57.73710482529118\n",
            "191 58.38686652391149 57.73710482529118\n",
            "192 58.38686652391149 57.73710482529118\n",
            "193 58.38686652391149 57.73710482529118\n",
            "194 58.31548893647395 57.73710482529118\n",
            "195 58.31548893647395 57.73710482529118\n",
            "196 58.31548893647395 57.90349417637272\n",
            "197 58.31548893647395 58.069883527454245\n",
            "198 58.31548893647395 58.069883527454245\n",
            "199 58.2441113490364 58.069883527454245\n",
            "200 58.2441113490364 58.069883527454245\n",
            "201 58.2441113490364 58.069883527454245\n",
            "202 58.2441113490364 58.069883527454245\n",
            "203 58.2441113490364 58.069883527454245\n",
            "204 58.2441113490364 58.23627287853578\n",
            "205 58.2441113490364 58.23627287853578\n",
            "206 58.2441113490364 58.23627287853578\n",
            "207 58.2441113490364 58.23627287853578\n",
            "208 58.2441113490364 58.23627287853578\n",
            "209 58.2441113490364 58.23627287853578\n",
            "210 58.2441113490364 58.23627287853578\n",
            "211 58.2441113490364 58.23627287853578\n",
            "212 58.2441113490364 58.23627287853578\n",
            "213 58.2441113490364 58.23627287853578\n",
            "214 58.2441113490364 58.23627287853578\n",
            "215 58.2441113490364 58.23627287853578\n",
            "216 58.2441113490364 58.23627287853578\n",
            "217 58.2441113490364 58.23627287853578\n",
            "218 58.17273376159886 58.23627287853578\n",
            "219 58.17273376159886 58.23627287853578\n",
            "220 58.17273376159886 58.23627287853578\n",
            "221 58.17273376159886 58.23627287853578\n",
            "222 58.17273376159886 58.23627287853578\n",
            "223 58.17273376159886 58.069883527454245\n",
            "224 58.17273376159886 58.069883527454245\n",
            "225 58.17273376159886 58.069883527454245\n",
            "226 58.17273376159886 58.069883527454245\n",
            "227 58.17273376159886 58.069883527454245\n",
            "228 58.2441113490364 58.069883527454245\n",
            "229 58.2441113490364 58.069883527454245\n",
            "230 58.2441113490364 58.069883527454245\n",
            "231 58.2441113490364 58.069883527454245\n",
            "232 58.17273376159886 58.069883527454245\n",
            "233 58.17273376159886 58.069883527454245\n",
            "234 58.17273376159886 58.069883527454245\n",
            "235 58.17273376159886 58.069883527454245\n",
            "236 58.17273376159886 58.23627287853578\n",
            "237 58.2441113490364 58.23627287853578\n",
            "238 58.2441113490364 58.402662229617306\n",
            "239 58.2441113490364 58.402662229617306\n",
            "240 58.2441113490364 58.402662229617306\n",
            "241 58.2441113490364 58.402662229617306\n",
            "242 58.2441113490364 58.402662229617306\n",
            "243 58.31548893647395 58.402662229617306\n",
            "244 58.31548893647395 58.402662229617306\n",
            "245 58.31548893647395 58.402662229617306\n",
            "246 58.31548893647395 58.402662229617306\n",
            "247 58.31548893647395 58.402662229617306\n",
            "248 58.31548893647395 58.402662229617306\n",
            "249 58.31548893647395 58.402662229617306\n",
            "250 58.31548893647395 58.402662229617306\n",
            "251 58.31548893647395 58.402662229617306\n",
            "252 58.31548893647395 58.56905158069884\n",
            "253 58.31548893647395 58.56905158069884\n",
            "254 58.31548893647395 58.56905158069884\n",
            "255 58.31548893647395 58.56905158069884\n",
            "256 58.31548893647395 58.56905158069884\n",
            "257 58.31548893647395 58.56905158069884\n",
            "258 58.31548893647395 58.56905158069884\n",
            "259 58.31548893647395 58.402662229617306\n",
            "260 58.31548893647395 58.402662229617306\n",
            "261 58.31548893647395 58.402662229617306\n",
            "262 58.31548893647395 58.402662229617306\n",
            "263 58.31548893647395 58.402662229617306\n",
            "264 58.31548893647395 58.402662229617306\n",
            "265 58.38686652391149 58.23627287853578\n",
            "266 58.38686652391149 58.23627287853578\n",
            "267 58.38686652391149 58.23627287853578\n",
            "268 58.38686652391149 58.23627287853578\n",
            "269 58.38686652391149 58.23627287853578\n",
            "270 58.38686652391149 58.23627287853578\n",
            "271 58.38686652391149 58.23627287853578\n",
            "272 58.38686652391149 58.23627287853578\n",
            "273 58.38686652391149 58.23627287853578\n",
            "274 58.38686652391149 58.069883527454245\n",
            "275 58.38686652391149 57.90349417637272\n",
            "276 58.38686652391149 57.73710482529118\n",
            "277 58.38686652391149 57.73710482529118\n",
            "278 58.38686652391149 57.73710482529118\n",
            "279 58.38686652391149 57.73710482529118\n",
            "280 58.38686652391149 57.73710482529118\n",
            "281 58.31548893647395 57.73710482529118\n",
            "282 58.31548893647395 57.73710482529118\n",
            "283 58.2441113490364 57.73710482529118\n",
            "284 58.2441113490364 57.90349417637272\n",
            "285 58.2441113490364 57.90349417637272\n",
            "286 58.2441113490364 57.90349417637272\n",
            "287 58.2441113490364 57.90349417637272\n",
            "288 58.2441113490364 57.90349417637272\n",
            "289 58.17273376159886 57.90349417637272\n",
            "290 58.17273376159886 57.90349417637272\n",
            "291 58.17273376159886 57.90349417637272\n",
            "292 58.10135617416131 57.90349417637272\n",
            "293 58.10135617416131 57.90349417637272\n",
            "294 58.10135617416131 57.90349417637272\n",
            "295 58.10135617416131 57.73710482529118\n",
            "296 58.10135617416131 57.73710482529118\n",
            "297 58.17273376159886 57.73710482529118\n",
            "298 58.17273376159886 57.73710482529118\n",
            "299 58.17273376159886 57.73710482529118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMcbWdbh98uY",
        "colab_type": "text"
      },
      "source": [
        "# **Machine Learning Assignment09** #\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Name : 이현주\n",
        "\n",
        "Student # : 20184060"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CwXihsOJgZ0R"
      },
      "source": [
        "# **1. Plot the loss curve [10pt]**\n",
        "\n",
        "- plot the training loss at every iteration of gradient descent using the training data in blue color (the first 6,000 images) [5pt]\n",
        "- plot the testing loss at every iteration of gradient descent using the testing data in red color (the rest 4,000 images) [5pt]\n",
        "- the both curves should be presented in one figure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_ZPJSBnroog",
        "colab_type": "code",
        "outputId": "41f23a6f-9f76-4908-c2b7-09cc3c7e4114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "source": [
        "#그 코드를 바탕으로 training loss plotting 하기.\n",
        "real_iter = iteration\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "cost_tmp = np.zeros(real_iter)\n",
        "\n",
        "for i in range(0, real_iter):\n",
        "  cost_tmp[i] = cost_history[i]\n",
        "\n",
        "\n",
        "# print(cost_history[1])\n",
        "\n",
        "ax.set_ylabel('J(Theta)')\n",
        "ax.set_xlabel('Iterations')\n",
        "_=ax.plot(range(real_iter),cost_tmp,'b.', color = 'blue')\n",
        "_=ax.plot(range(real_iter),cost_history2,'b.', color = 'red')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OiKcfzyYgaCF"
      },
      "source": [
        "# **2. Plot the accuracy curve [10pt]**\n",
        "\n",
        "- plot the training accuracy (%) at every iteration of gradient descent using the training data in blue color (the first 6,000 images) [5pt]\n",
        "- plot the testing accuracy (%) at every iteration of gradient descent using the testing data in red color (the rest 4,000 images) [5pt]\n",
        "- the both curves should be presented in one figure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L688vFb8rqtO",
        "colab_type": "code",
        "outputId": "54ecc470-ae9f-42eb-c281-aa201983ad6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "source": [
        "#그 코드를 바탕으로 training loss plotting 하기.\n",
        "fig,ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "# print(cost_history[1])\n",
        "\n",
        "ax.set_ylabel('accuracy')\n",
        "ax.set_xlabel('Iterations')\n",
        "_=ax.plot(range(iteration),accuracy_history,'b.', color = 'blue')\n",
        "_=ax.plot(range(iteration),accuracy_history2,'b.', color = 'red')\n",
        "# print(ac_tmp)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAHgCAYAAABuGUHVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dfZBta10f+O+v+7zgIAE8XKn4ckUzqNFhuJgWacWk4Rg05AX1OopJJsRorho1XicpXzKV0hmnCqbiyzVViZFRozEKqJdrKGOAmxNbY9IKfQQFEQtCYOSiEq7AgClOn9v7mT/W3pw+fXd37+5eu/dLfz5Vp5699np71vOstc+v1/7t9VRrLQAAwOmtzLoCAACwLATXAADQE8E1AAD0RHANAAA9EVwDAEBPBNcAANCTC7OuQJ+e9KQntac85SmzrgYAAEvs+vXr72ut3TFu3lIF1095ylOyvb0962oAALDEqupdB82TFgIAAD0RXAMAQE8E1wAA0BPBNQAA9ERwDQAAPRFcAwBATwTXAADQE8E1AAD0RHANAAA9EVwDAEBPBNcAANATwTUAAPREcA0AAD0RXAMAQE8E1wAA0BPBNQAA9OTCrCsAAHttbSWbm8mVK8nDDycbG8n6+qxrBTAZwTUAc2NrK7l6NblxIxkMkpWV5PLl5No1ATawGKSFADA3NjeTnZ0usE66cmenex9gEQiuAZgbGxvJpUvdHeukKy9d6t4HWATSQgCYG+vrXQqInGtgUQmuAZgr6+uCaWBxSQsBAICeCK4BAKAngmsAAOiJ4BoAAHoiuAYAgJ4IrgEAoCeCawAA6InnXANMwdbW7QOhLFo5GhFxc/PWIC59HdP+QWH2b/eo+cfdPsBZElwD9GxrK7l6NblxIxkMkqqktcUpV1aSCxe66Uce6YYfv+++5N57T39MKyvJ5cvdKIyjgH1vWx01/7jbBzhr0kIAera5mezsdMFg0gV9i1QOBsnNm90x7O525f3393NMg0G3nc3Nbnp/Wx01/7jbBzhrgmuAnm1sdHd7V4afsFWLVa6sJBcvdsewutqVd9/dzzGtrHTbGaWd7G+ro+Yfd/sAZ01aCEDP1te7tIRly7l+2tP6z7ke11ZHzZdzDcyzaqPv0pbA2tpa297ennU1AABYYlV1vbW2Nm6etBAAAOiJ4BoAAHoiuAYAgJ4IrgEAoCeCawAA6IngGgAAeiK4BgCAnhhEBlg6W1v9D3Yybrt7B1pZ5oFMRsfd1zGNa8c+2+qo/l+mvgHmj+AaWCpbW8nVq8mNG8lg0A2L3drxy5WV5PLlbnTA9fVHb3dlJblwoVv+5s3b97V/3UU2Ou6dnW5Y8dMe07h27LOtjur/ZeobYD5JCwGWyuZmFwgOBt30aBDa45aDQbedzc3x2x0MuqB63L72r7vIRse9u9vPMY1rxz7b6qj+X6a+AeaT4BpYKhsb3R3WleGnW9XJypWVbjuj1I/9211ZSS5eHL+v/esustFxr672c0zj2rHPtjqq/5epb4D5VG305/wSWFtba9vb27OuBjBjcq77Jeca4HZVdb21tjZ2nuAaAOAETvqXvL/wFt5hwbUfNAIAHNdJfz3tV7VLT841AMBxnfTX035Vu/QE1wAAx3XSX0/7Ve3SkxYCAHBc6+tdaoeca/YRXAMAnMT6uiCZR5EWAgAAPRFcAwBAT6SFAEtj0QY7YTb6Pk8A9hJcA0th9MjZnZ3uh/infYTs/kfYejTtcuj7PAHYT1oIsBRGj5zd3e3nEbL7H2Hr0bTLoe/zBGC/qd65rqp3JvlQkt0kj7TW1qrqriT/IsljkjyS5O+11l43Zt3dJG8aTv6/rbW/Ns26Aott9MjZ0R3J0z5CdrS9vXeuPZp28fV9ngDsV200YtA0Nt4F12uttfftee+1SX6wtfbvqur5Sb69tbYxZt0Pt9Y+9jj7W1tba9vb26esNbCo5FwzCTnXwGlV1fXW2tq4ebPIuW5J/tTw9eOTvGcGdQCWUN+PnPUI2+WkX4FpmnZw3ZK8tqpakh9prb00yb1JXlNV35cu5/vzD1j3MVW1nS515CWttV+Ycl0BAOBUph1cP7u19lBVfXySB6vqrUm+Ism3tdbur6qvTPJjSb5ozLqfMlz305L8h6p6U2vtv+xfqKruSXJPktx5553TOxIAADjCVJ8W0lp7aFi+N8kDSZ6Z5EVJXjlc5OeG7x227juSbCZ5xgHLvbS1ttZaW7vjjjt6rT8AABzH1ILrqnpsVT1u9DrJ85K8OV2O9V8YLvbcJG8bs+4Tq+ry8PWTknxBkrdMq64AANCHaaaFPDnJA1U12s/PtNZeXVUfTvJDVXUhyUcyTOmoqrUk39Ba+7okfzbJj1TVIN0fAC9prQmuAQCYa1N9FN9Z8yg+AACm7bBH8RmhEQAAejKL51zD0tk/2IhBR2AxHHTtTlq6xoH9BNdwSltbydWrt4bJrkpa64bLvnw5uXbNf74wjw66dictXePAONJC4JQ2N5Odne4/56T7Tzfppnd2uvnA/Dno2p20dI0D4wiu4ZQ2NpJLl7q7WEl3Vyvppi9d6uYD8+ega3fS0jUOjCMtBE5pfb37WljONSyWw65dOdfASXkUHwAAHINH8QEAwBkQXAMAQE8E1wAA0BPBNQAA9MTTQgCA5XDaITfP6nExx62nx9IsFME1ALD4Tjvk5lkN0XncehoKdOFICwEAFt9ph9w8qyE6j1tPQ4EuHHeuYY+TfqPoG7uzdVA/6QfO2qSfGc7NMzAacvMs71yfZIjO49ZzuJ83XdnIL7747DJenMMnJ7iGoZN+o+gbu7N1UD/pB87apJ8Zzs0z0seQm2cRcZ6gnm+6spHPu3f9zP5ucA6fjuAahk76jeLeb+x86EzfQf2kHzhrk35mODfP0Pr6YjTyMev5iy8+24wX5/DpyLmGodE3dSvDq6JqsvKk3wxyMgf1k37grE36meHc5LRO+v9TX6Vz+Hiqjf4sWQJra2tte3t71tVggcm5XgxyrpkXcq45K2f9lEHn8OGq6nprbW3sPME1AABM7rDgWloIAAD0RHANAAA9EVwDAEBPBNcAANATwTUAAPREcA0AAD0RXAMAQE8Mfw5TtP+h/x7CfzwGi2FZGGxmvsx6QJZFHMDF/2eTE1zDlGxtJVevJjduJINBN3zs5cvJtWs+kCaxv/2qkta0I4vnoHN5f+ncPhuT9se8lPNwXvj/7HikhcCUbG4mOzvdB1HSlTs73fscbX/7jQaT1Y4smoPO5f2lc/tsTNof81LOw3nh/7PjEVzDlGxsJJcudX/hJ1156VL3Pkfb335VXakdWTQHncv7S+f22Zi0P+alnIfzwv9nx1Nt9KfRElhbW2vb29uzrgZ8lBy105FzzbKQcz1f5Fyfvs3moU6zVFXXW2trY+cJrgEAYHKHBdfSQgAAoCeCawAA6IngGgAAeiK4BgCAngiuAQCgJ4JrAADoieAaAAB6cmHWFaA/owe8n/cHu48zLwM4LEMfHactk9MN1LDI7QQn0ffgJn1ch9O6Ho861uPs77jt5rPl9LT5wQTXS2JrK7l6NdnZ6YYkvXbt/JzERxm1zY0byWDQDSnb2qPLlZXk8uXptd0y9NFx2vLChW765s3Dl51Vf8C8mfT6qko+r23lOdnMO3Ild+Th8WU9nO9Y3UhV8gWPbOYd7ZBljyrr4fzU6pV8wtc9nE95xumjqKOOdaLrfxjdvenKRr7rW5LPv7mZV7WN/EatL+dnS19/efUQ5R7nXF3oNj8hwfWS2Nzsgrbd3a7c3DwfJ/AkRm0zGHTTo0FJ95eDwXTbbhn66DhtefPm4cscVU67P2DeTHp9fV7byrVczaXcyGoG2U1lNe3RZVvJI49cSFK5kJuHL3tU2Sqrj7S0f1FJTh9FHXWsR17/e+5W/Nms5tW7lQt5JDu5lKvtWn4968v12XLcaHbKUe6k5+pCt/kpyLleEhsb3d3Q1dWuHH0VyK22WRme7VXjy5WV6bbdMvTRcdry4sXJlp1Vf8C8mfT6ek42cyk7uZAusrmQdkA5yMXczMWJlp2sXMkxoqhTHOuR1/+euxWrg9Ex7uZidvKcbI7d5kJ/thw3mj1l/xxl0nN1odv8FNy5XhLr690foouezzsNe9tmljlhy9BHx23LRM41TGrS6+sxb9hIfuxSdm/eyEoGGaRSaY8uq8vPqlR2b948fNkJy4nujE4QRU1yrIde/6PobmcntbqaapXdRx5JXbiUz/zajfzIM5bss2V0vH3cue4hyj3O/wUL2+anUG3018wSWFtba9vb27OuBgBM11n+snhe/yre+wvx0TEucwQ3RznXJFV1vbW2Nnae4BoAACZ3WHAt5xoAAHoiuAYAgJ4IrgEAoCeCawAA6IngGgAAeiK4BgCAnhhEZgnsfdTn+vqjp49abxkefXnU4z8X4dhO2h/HffTpcdtimc4T4OT6eszypJ8lx3mUt8+k+XdQfx6n//rYxlkQXC+4ra3k6tVuNNNLl5L77kvuvffW9LVr40+40XqjwZ5WVpLLlw9efp7tP5ZxA1LN+7GdtD+OOvbTtsUynSfAyR33s+a0n0WT7s9n0mI4qD+P0399bOOsSAtZcJubXSC9u9uV999/+/Tm5uHrDQbd9GBw+PLzbP+xjMZFGpWLcGwn7Y+jjv20bbFM5wlwcsf9rDntZ9Gk+/OZtBgO6s/j9F8f2zgrgusFt7HR3aFeXe3Ku+++fXo0KuxB660Mz4CVlcOXn2f7j6Xq9nIRju2k/XHUsZ+2LZbpPAFO7rifNaf9LJp0fz6TFsNB/Xmc/utjG2fF8OdLQM61nGs518C0ybnmNJYt5/qw4c8F1wAAcAyHBdfSQgAAoCeCawAA6IngGgAAeiK4BgCAngiuAQCgJ4JrAADoieAaAAB6cmHWFWC8SQbuOGqwmOMOrHLaAQI8yP94jurjg+afdlCXaQ86AwCT2hvLJIs/IFwiuJ5LW1vJ1avJjRvJYNAN7Xn5cnLt2q2TarTMzk437OfeeeO2UZW0dqvcv82jlj+qHFdHDnZUHx80/777knvvPfzcOM5+9SsAs7I3llld7f7vuXlzsrhlnkkLmUObm92JNhh004NBN725+ehldncfPW/cNkYDcY7K/ds8avmjynF15GBH9fFB8++//+hz4zj71a8AzMreWObmzePFLfNMcD2HNja6u9Erw95ZWemmR1+Z7F1mdfXR88Zto+r2cv82j1r+qHJcHTnYUX180Py77z763DjOfvUrALOyN5a5ePF4ccs8qzb6k2AJrK2tte3t7VlXoxdyrpefnGsAzrtFzbmuquuttbWx8wTXwNKY9C+HefqEPsoyHdNp/4Lvu5yHNjtpm8xD3Q9yVv08z20wz466M8dEDguu/aARWA6T/lpzkX4Vs0zHdNpfTfddzkObnbRN5qHuBzmrfp7nNphnRz0NgV7IuQaWw6S/1lykX8Us0zGd9lfTfZfz0GYnbZN5qPtBzqqf57kN5tlRT0OgF4JrYDlM+mvNRfpVzDId02l/Nd13OQ9tdtI2mYe6H+Ss+nme22CeHfU0BHoh5xpYHsuUnzyyTMck57q/NpmHuh9EzvV8k3PdCz9oBACAnhwWXEsLAQCAngiuAQCgJx7FNyWnHehjEc1iEJrzlDp2no4VABaV4HoK9j/m8zw8jvO0jzY9SRudp8d1nqdjBYBFJi1kCvY/5vM8PI7ztI82PUkbnafHdZ6nYwWARTbV4Lqq3llVb6qqN1bV9vC9u6rq10fvVdUzD1j3RVX1tuG/F02znn3b/5jP8/A4ztM+2vQkbXSeHtd5no4VABbZVB/FV1XvTLLWWnvfnvdem+QHW2v/rqqen+TbW2sb+9b7uCTbSdaStCTXk/y51tr7D9vfPD2KT861nOu+nadjBYB5dtij+GaRc92S/Knh68cnec+YZb44yYOttT9Okqp6MMmXJHnZmdSwB+vr5y8AmsUxn6d2Pk/HCgCLatrBdUvy2qpqSX6ktfbSJPcmeU1VfV+6tJTPH7PeJyb5/T3T7x6+BwAAc2vawfWzW2sPVdXHJ3mwqt6a5CuSfFtr7f6q+sokP5bki066g6q6J8k9SXLnnXf2UWcAADiRqf6gsbX20LB8b5IHkjwzyYuSvHK4yM8N39vvoSSfvGf6k4bvjdvHS1tra621tTvuuKOvqgMAwLFNLbiuqsdW1eNGr5M8L8mb0+VY/4XhYs9N8rYxq78myfOq6olV9cThuq+ZVl0BAKAP00wLeXKSB6p71tqFJD/TWnt1VX04yQ9V1YUkH8kwpaOq1pJ8Q2vt61prf1xV35vk9cNt/Z+jHzcCAMC8muqj+M7aPD2KDwCA5XTYo/iM0AgAAD2ZxXOuGeOoQWf2DiCSGEwEAGAeCa7nwNZWcvVqcuNGMhh0Q4Ffvpxcu9YFz6P5Ozvd8NdVySOPdMNgj5YBAGD2pIXMgc3NLnAeDLrpwaCb3ty8ff7ubnLz5q3Xe5cBAGD2BNdzYGOjuwu9MuyNlZVuepQCMpq/uppcvHjr9d5lAACYPWkhc2B9vUvvOCjneu98OdcAAPPLo/gAAOAYPIoPAADOgOAaAAB6IrgGAICeCK4BAKAnnhYCzL/9Q5geVB73ETqTbvek2z9sn0dta9LlTnoskxxr0v825+0RR0cNj3vUeic9ptOuD6fV1znY12fPuM+cBb0+BNfAfNs/hGlV0tqjy/1Dm/a13ZNu/7B97uwcPsTqpMud9FgmOdYLF7rpmzf72+Zp269vRw2Pe9R6k/ZP3+vDafV1Dvb12TPuM2cePzMmJC0EmG/7hzAdPT50f7l/aNO+tnvS7R+2z6OGWJ10uf3LT3oskxzraDjYPrc5b8PKHjU87lHrnXSo3NOuD6fV1znY12fPuM+cefzMmJA71z076luWg749WdBvPs7cSb/BZYGNhiid5M71cYYtnXS7J93+Yfsc3S06aFuTLnfSY5nVnet5G1Z2f7tNWsfj9k/f68Np9XUO9vXZc9Cd6wW9Pgwi06OjvmU56NuTBf7m40yd9BtcloCc6/6PRc51R84155Wc61M5bBAZwXWPXvzi5B//4+5bltXV5Hu/N/mu7xo/f79xy3O7ce2n3QCAs2aExjMy+nZkdXX8Nxmj+SvDVq/qygX+5uNM7W8/7QYAzBs51z1aX+9SFA76lmXv/EX4tnTejGs/7QYAzBNpIQAAcAzSQgAA4AwIrgEAoCeCawAA6IngGgAAeiK4BgCAngiuAQCgJ4JrAADoiUFkzsjW1sGDy9DZ20aJwXYAgMUjuD4DW1vJ1avJzk43XPe1awLE/fa20epqNzT8zZvJYNC9bq0b7vzyZe0HAMwvaSFnYHOzCxp3d7tyc3PWNZo/e9vo5s3u9WDQzRsNIjoYaD8AYL4Jrs/AxkZ3x3p1tStHaQ/csreNLl7sXq8Mz86qrlxZ0X4AwHyTFnIG1te7VAY51wfb30aJnGsAYPFUG33nvgTW1tba9vb2rKsBAMASq6rrrbW1cfOkhQAAQE8E1wAA0BPBNQAA9ERwDQAAPZkouK6qV1bVX64qwTgAABxg0mD5nyf560neVlUvqarPmGKdAABgIU0UXLfW/n1r7W8k+Zwk70zy76vqP1fV11TVxWlWEAAAFsXEaR5VdSXJ307ydUnekOSH0gXbD06lZgAAsGAmGqGxqh5I8hlJfirJX22t/cFw1iuqyqgtzK+trfFDPR415ONR6xkycrGN+ve4/bf/vDhsONGjzpmTnptM13Gv/UmX17/Mq5Oew2dxLi/o/8WTDn/+T1trvzxuxkGj08DMbW0lV68mN24kg0FSlbR2q1xZSS5f7sZd33tRHrXeUesz30b9u7OTXLo0ef/tPy9WVpILF7rz4ebNw8+V/efMffcl9957/HOT6TrutX9UP/rsYN5Nes7P4lxe4P+LJ00L+ayqesJooqqeWFV/b0p1gn5sbnYB1GDQTbd2ezkYdPM3N4+33lHrM99G/bu7e7z+239eDAZdUD3JubL/nLn//pOdm0zXca/9o/rRZwfzbtJzfhbn8gL/XzxpcP13W2sfGE201t6f5O9Op0rQk42N7s7kyvA0r7q9XFnp5o++2p90vaPWZ76N+nd19Xj9t/+8WFlJLl6c7FzZf87cfffJzk2m67jX/lH96LODeTfpOT+Lc3mB/y+uNor8D1uo6k1J/uc2XLiqVpP8dmvts6dcv2NZW1tr29tSwNlDzjXjyLnmIHKuOW/kXJ9IVV0/KDV60uD6nyT5lCQ/Mnzr65P8fmvtH/RWyx4IrgEAmLbDgutJf9D4HekC6m8cTj+Y5Ed7qBsAACyNiYLr1togyQ8P/wEAAGNM+pzrpyZ5cZLPSvKY0futtU+bUr0AAGDhTPq0kH+Z7q71I0mek+RfJfnX06oUAAAsokmD649prV1L9wPId7XWvifJX55etQAAYPFM+oPGG1W1kuRtVfXNSR5K8rHTqxYAACyeSe9cf2uS/yHJ30/y55L8zSQvmlalAABgER1553o4YMxXtdb+YZIPJ/maqdcKAAAW0JF3rltru0mefQZ1AQCAhTZpzvUbqupVSX4uyZ+M3mytvXIqtQIAgAU0aXD9mCQPJ3nunvdaEsE1AAAMTTpCozxrAAA4wqQjNP7LdHeqb9Na+zu91wgAABbUpGkhv7jn9WOSfFmS9/RfHQAAWFyTpoXcv3e6ql6W5NemUiMAAFhQkw4is99Tk3x8nxUBAIBFN2nO9Ydye871Hyb5jqnUCAAAFtSkaSGPm3ZFAABg0U2UFlJVX1ZVj98z/YSq+tLpVQsAABbPpDnX391a++BoorX2gSTfPZ0qAQDAYpo0uB633KSP8QMAgHNh0uB6u6p+oKr+zPDfDyS5Ps2KAQDAopk0uP6WJDtJXpHk5Uk+kuSbplUpAABYRJM+LeRPknznlOsCAAALbdLnXD+Y5H8Z/pAxVfXEJC9vrX3xNCsHE9vaSjY3kytXkocfvlVubCTr6/2v19f6i+CgYzyq3Njo1t+77qhdjrvNZWpPAJbapD9KfNIosE6S1tr7q8oIjcyHra3k6tXkxo1kMEiqktaSlZXk8uXk2rXxgdlJ1+tr/UVw0DEeVa6sJBcudNM3b3brjtrlvvuSe++dfJvL1J4ALL1Jc64HVXXnaKKqnpLbR2yE2dncTHZ2ukAt6QKypJve2enm97leX+svgoOO8ahyMOiC6r3rjtrl/vuPt81lak8Alt6kwfX/nuTXquqnqupfJ/mVJN81vWrBMWxsJJcudXc4k+6OZ9JNX7p0Kz2hr/X6Wn8RHHSMR5UrK8nFi7evO2qXu+8+3jaXqT0BWHrV2mQ3oIdpIPckeUOSj0ny3tbar06xbse2trbWtre3Z10NZkHO9fTIuQaA21TV9dba2th5kwTXVfV1Sb41yScleWOSZyXZaq09t8+KnpbgGgCAaTssuJ40LeRbk3xukne11p6T5BlJPnD4KgAAcL5MGlx/pLX2kSSpqsuttbcm+YzpVQsAABbPpI/ie3dVPSHJLyR5sKren+Rd06vW4hmlkEoNBQA4vyYdofHLhi+/p6p+Ocnjk7x6arVaMKNHAe/sdA818DheAIDzadK0kI9qrf1Ka+1VrbWdaVRoEY0eBby763G8AADn2aRpISdSVe9M8qEku0keaa2tVdUrcitf+wlJPtBau2uSdadZ19MYPQp4dOfa43gBAM6nqQbXQ89prb1vNNFa+6rR66r6/iQfnHTdebW+3qWCyLkGADjfziK4HquqKslXJpmrZ2Wf1Pq6oBoA4Lw7ds71MbUkr62q61V1z755X5jkj1prbzvBugAAMHemfef62a21h4ZDpz9YVW/dM2T6Vyd52QnX/ahh4H1Pktx555191x8AACY21TvXrbWHhuV7kzyQ5JlJUlUXknx5klccd90xy720tbbWWlu74447+j0AAAA4hqkF11X12Kp63Oh1kuclefNw9hcleWtr7d0nWBcAAObSNNNCnpzkge53i7mQ5Gdaa6OBZ16YfSkhVfUJSX60tfb8I9YFAIC5NLXgurX2jiRPP2De3x7z3nuSPP+odQEAYF5N+2khAABwbgiuT2lrK3nxi7sSAIDzbWaDyCyDra3k6tVbw55fu2YgGQCA88yd61PY3OwC693drtzcnHWNAACYJcH1KWxsdHesV1e7cmNj1jUCAGCWpIWcwvp6lwqyudkF1lJCAADON8H1Ka2vC6oBAOhICwEAgJ4IrgEAoCeCawAA6IngGgAAeuIHjTza1lb3CJQrV5KHH350OctHoxxUt3l7XMskbZgcvkzf5VFtdBZtO9rHvPUXAPREcM3tRsNO3riRDAZJVdLarXJlJbl8eTbDUR5Ut1nW6Tj13FvfCxe66Zs3xy/Td3lUG51F2xrSFIBzQFoItxsNOzkYdNOt3V4OBrMbjvKgus2yTuNM0oY3bx6+TN/lUW10Fm1rSFMAzgHBNbcbDTu5Mjw1qm4vV1ZmNxzlQXWbZZ3GmaQNL148fJm+y6Pa6Cza1pCmAJwD1UZ3qJbA2tpa297ennU1Fp+c69OTc334PuatvwDgGKrqemttbew8wTUAAEzusOBaWggAAPREcA0AAD0RXAMAQE8E1wAA0BPBNQAA9ERwDQAAPRFcAwBATwTXAADQE8E1AAD0RHANAAA9EVwDAEBPBNcAANATwTUAAPREcA0AAD0RXAMAQE8E1wAA0BPBNQAA9ERwDQAAPRFcAwBATwTXAADQE8E1AAD0RHANAAA9EVwDAEBPBNcAANCTC7OuwNLZ2ko2N5ONjWR9fda1mcyozleuJA8/fHTd9y+/vzzusY/bfzJ+H4vUrvOm734DAB5FcN2nra3k6tVkZye5dCm5dm3+g5VRnW/cSAaDZGUluXz54LrvX74qae1WedT6k+z/woVuezdv3r6P426bW/ruNwBgLGkhfdrc7ALr3d2u3NycdY2ONqrzYNBNDwaH133/8q3dXh61/iT7v3lz/D6Ou21u6bvfAICxBNd92tjo7livrnblKL1hno3qvDI8FVZWDq/7/uWrbi+PWn+S/V+8OH4fx902t/TdbwDAWNVGd66WwNraWtve3p5tJeRcy7meV8xmxbkAAA/gSURBVHKuAaAXVXW9tbY2dp7gGgAAJndYcC0tBAAAeiK4BgCAngiuAQCgJ4JrAADoieAaAAB6IrgGAICeCK4BAKAngmsAAOiJ4BoAAHoiuAYAgJ4IrgEAoCeCawAA6IngGgAAeiK4BgCAngiuAQCgJ4JrAADoieAaAAB6IrgGAICeCK4BAKAngmsAAOiJ4BoAAHoiuAYAgJ4IrgEAoCeCawAA6MmFWVdg4W1tJZubycZGsr4+69rcblS3K1eShx8+uOy77pPud1r7BwCYEcH1aWxtJVevJjs7yaVLybVr8xMkjup240YyGCRVSWuPLldWksuX+6v7pPud1v4BAGZIWshpbG52gfXubldubs66RreM6jYYdNOtjS8Hg37rPul+p7V/AIAZElyfxsZGd8d6dbUrNzZmXaNbRnVbGXZx1fhyZaXfuk+632ntHwBghqSFnMb6epfOMI8513vrdpY5z8fZr5xrAGDJVBt9Pb8E1tbW2vb29qyrAQDAEquq6621tXHzpIUAAEBPBNcAANATwTUAAPREcA0AAD0RXAMAQE+m+ii+qnpnkg8l2U3ySGttrapekeQzhos8IckHWmt3jVn3S5L8UJLVJD/aWnvJNOsKAACndRbPuX5Oa+19o4nW2leNXlfV9yf54P4Vqmo1yT9L8heTvDvJ66vqVa21t5xBfQEA4ERmlhZSVZXkK5O8bMzsZyZ5e2vtHa21nSQvT/KCs6wfAAAc17SD65bktVV1varu2TfvC5P8UWvtbWPW+8Qkv79n+t3D9wAAYG5NOy3k2a21h6rq45M8WFVvba396nDeV2f8XetjGQbt9yTJnXfeedrNAQDAiU31znVr7aFh+d4kD6RL90hVXUjy5UleccCqDyX55D3TnzR8b9w+XtpaW2utrd1xxx19VR0AAI5tasF1VT22qh43ep3keUnePJz9RUne2lp79wGrvz7JU6vqU6vqUpIXJnnVtOoKAAB9mOad6ycn+bWq+q0kr0vyb1trrx7Oe2H2pYRU1SdU1S8lSWvtkSTfnOQ1SX43yc+21n5ninUFAIBTq9barOvQm7W1tba9vT3ragAAsMSq6nprbW3cPCM0AgBATwTXAADQE8E1AAD0RHANAAA9EVwDAEBPBNcAANATwTUAAPREcA0AAD0RXAMAQE8E1wAA0JMLs67A0tjaSjY3k42NZH396OnTbD/pXl+5kjz88MHlSfcFAMCJCK77sLWVXL2a7Owkly4l992X3HvvwdPXrh0v6N27/dXVpCq5eTMZDLrXrT26XFlJLl8+/r4AADgxaSF92NzsAt/d3a68//7Dpzc3T779mze714NBN6+18eVgcLJ9AQBwYoLrPmxsdHekV1e78u67D58epXacZPsXL3avV4ZdVzW+XFk52b4AADgxaSF9WF/v0i/25lQ/7WmHT59m+4mcawCAOVRtlEawBNbW1tr29vasqwEAwBKrquuttbVx86SFAABATwTXAADQE8E1AAD0RHANAAA9EVwDAEBPBNcAANATwTUAAPREcA0AAD0RXAMAQE8E1wAA0BPBNQAA9ERwDQAAPRFcAwBATwTXAADQE8E1AAD0RHANAAA9EVwDAEBPBNcAANATwTUAAPREcA0AAD0RXAMAQE8E1wAA0BPBNQAA9ERwDQAAPRFcAwBATy7MugJLb2sr2dxMNjaS9fVb01euJA8//OhytNxB6wMAMLcE19O0tZVcvZrs7CSXLiX33Zfce29y40YyGCRVSWu3ypWV5PLl5Nq1W4H43vVH7wMAMJekhUzT5mYXGO/uduX993flYNDNb+32cjDo5m9ujl9/9D4AAHNJcD1NGxvdHefV1a68++6uXBk2e9Xt5cpKN39jY/z6o/cBAJhL0kKmaX29S+XYmzP9tKdNnnM9bn0AAOZWtVFKwhJYW1tr29vbs64GAABLrKqut9bWxs2TFgIAAD0RXAMAQE8E1wAA0BPBNQAA9ERwDQAAPRFcAwBATwTXAADQE8E1AAD0RHANAAA9EVwDAEBPBNcAANATwTUAAPREcA0AAD0RXAMAQE8E1wAA0BPBNQAA9ERwDQAAPRFcAwBATwTXAADQE8E1AAD0RHANAAA9EVwDAEBPBNcAANATwTUAAPTkwqwrwD5bW8nmZnLlSvLww8nGRrK+PutaAQAwAcH1PNnaSq5eTW7cSAaDZGUluXw5uXZNgA0AsACkhcyTzc1kZ6cLrJOu3Nnp3gcAYO4JrufJxkZy6VJ3xzrpykuXuvcBAJh70kLmyfp6lwIi5xoAYCEJrufN+rpgGgBgQUkLAQCAngiuAQCgJ4JrAADoieAaAAB6IrgGAICeTPVpIVX1ziQfSrKb5JHW2trw/W9J8k3D9/9ta+3bJ10XAADm1Vk8iu85rbX3jSaq6jlJXpDk6a21G1X18ZOuCwAA82wWaSHfmOQlrbUbSdJae+8M6gAAAL2bdnDdkry2qq5X1T3D9z49yRdW1W9U1a9U1eceY10AAJhb004LeXZr7aFh6seDVfXW4T4/Lsmzknxukp+tqk9rrbWj1m2t/er+HQwD73uS5M4775zqwQAAwGGmeue6tfbQsHxvkgeSPDPJu5O8snVel2SQ5EkTrjtuHy9tra211tbuuOOO6RwIAABMYGrBdVU9tqoeN3qd5HlJ3pzkF5I8Z/j+pye5lOR9E64LAABza5ppIU9O8kBVjfbzM621V1fVpSQ/XlVvTrKT5EWttVZVn5DkR1trzz9o3SnWFQAATm1qwXVr7R1Jnj7m/Z0kf3PM++9J8vzD1gUAgHlmhEYAAOiJ4BoAAHpSj34C3uKqqv+W5F0z2PWTsu9HmcwF/TKf9Mv80SfzSb/MJ/0yn866Xz6ltTb2MXVLFVzPSlVtt9bWZl0Pbqdf5pN+mT/6ZD7pl/mkX+bTPPWLtBAAAOiJ4BoAAHoiuO7HS2ddAcbSL/NJv8wffTKf9Mt80i/zaW76Rc41AAD0xJ1rAADoieD6lKrqS6rq96rq7VX1nbOuz3lVVe+sqjdV1Ruranv43sdV1YNV9bZh+cRZ13PZVdWPV9V7q+rNe94b2w/V+afDa+e3q+pzZlfz5XZAv3xPVT00vGbeWFXP3zPvu4b98ntV9cWzqfVyq6pPrqpfrqq3VNXvVNW3Dt93vczQIf3iepmhqnpMVb2uqn5r2C//x/D9T62q3xi2/yuq6tLw/cvD6bcP5z/lLOsruD6FqlpN8s+S/KUkn5Xkq6vqs2Zbq3PtOa21u/Y8iuc7k1xrrT01ybXhNNP1E0m+ZN97B/XDX0ry1OG/e5L88BnV8Tz6iTy6X5LkB4fXzF2ttV9KkuFn2AuTfPZwnX8+/KyjX48k+Qettc9K8qwk3zRse9fLbB3UL4nrZZZuJHlua+3pSe5K8iVV9awk/3e6fvkfk7w/ydcOl//aJO8fvv+Dw+XOjOD6dJ6Z5O2ttXe01naSvDzJC2ZcJ255QZKfHL7+ySRfOsO6nAuttV9N8sf73j6oH16Q5F+1zq8neUJV/emzqen5ckC/HOQFSV7eWrvRWvuvSd6e7rOOHrXW/qC19pvD1x9K8rtJPjGul5k6pF8O4no5A8Pz/sPDyYvDfy3Jc5P8/PD9/dfL6Dr6+SRXq6rOqLqC61P6xCS/v2f63Tn8ImR6WpLXVtX1qrpn+N6TW2t/MHz9h0mePJuqnXsH9YPrZ/a+eZhi8ON70qb0yxkbfmX9jCS/EdfL3NjXL4nrZaaqarWq3pjkvUkeTPJfknygtfbIcJG9bf/RfhnO/2CSK2dVV8E1y+LZrbXPSffV6TdV1Z/fO7N1j8XxaJwZ0w9z5YeT/Jl0X7H+QZLvn211zqeq+tgk9ye5t7X2/+2d53qZnTH94nqZsdbabmvtriSflO7bgc+ccZUOJLg+nYeSfPKe6U8avscZa609NCzfm+SBdBfeH42+Nh2W751dDc+1g/rB9TNDrbU/Gv5nNUjy/+TWV9n65YxU1cV0AdxPt9ZeOXzb9TJj4/rF9TI/WmsfSPLLSdbTpUddGM7a2/Yf7Zfh/Mcnefis6ii4Pp3XJ3nq8Neql9L9qOFVM67TuVNVj62qx41eJ3lekjen64sXDRd7UZJ/M5sannsH9cOrkvyt4VMQnpXkg3u+DmfK9uXrflm6aybp+uWFw1/bf2q6H9C97qzrt+yG+Z8/luR3W2s/sGeW62WGDuoX18tsVdUdVfWE4euPSfIX0+XD/3KSrxgutv96GV1HX5HkP7QzHNjlwtGLcJDW2iNV9c1JXpNkNcmPt9Z+Z8bVOo+enOSB4W8VLiT5mdbaq6vq9Ul+tqq+Nsm7knzlDOt4LlTVy5JsJHlSVb07yXcneUnG98MvJXl+uh8A/fckX3PmFT4nDuiXjaq6K13awTuTfH2StNZ+p6p+Nslb0j054Ztaa7uzqPeS+4Ik/2uSNw3zSJPkH8X1MmsH9ctXu15m6k8n+cnhk1hWkvxsa+0Xq+otSV5eVf9Xkjek+8Mow/Knqurt6X7M/cKzrKwRGgEAoCfSQgAAoCeCawAA6IngGgAAeiK4BgCAngiuAQCgJ4JrgAVQVR8elk+pqr/e87b/0b7p/9zn9gHOE8E1wGJ5SpJjBdd7RjA7yG3BdWvt849ZJwCGBNcAi+UlSb6wqt5YVd9WVatV9U+q6vVV9dtV9fVJUlUbVfUfq+pV6Qa4SFX9QlVdr6rfqap7hu+9JMnHDLf308P3RnfJa7jtN1fVm6rqq/Zse7Oqfr6q3lpVPz0c2S5V9ZKqesuwLt935q0DMGNGaARYLN+Z5B+21v5KkgyD5A+21j63qi4n+U9V9drhsp+T5H9qrf3X4fTfaa398XD44NdX1f2tte+sqm9urd01Zl9fnuSuJE9P8qThOr86nPeMJJ+d5D1J/lOSL6iq3003NPRnttbaaLhigPPEnWuAxfa8JH9rOFTzbyS5kuSpw3mv2xNYJ8nfr6rfSvLrST55z3IHeXaSl7XWdltrf5TkV5J87p5tv7u1NkjyxnTpKh9M8pEkP1ZVX55umG6Ac0VwDbDYKsm3tNbuGv771Nba6M71n3x0oaqNJF+UZL219vQkb0jymFPs98ae17tJLrTWHknyzCQ/n+SvJHn1KbYPsJAE1wCL5UNJHrdn+jVJvrGqLiZJVX16VT12zHqPT/L+1tp/r6rPTPKsPfNujtbf5z8m+aphXvcdSf58ktcdVLGq+tgkj2+t/VKSb0uXTgJwrsi5Blgsv51kd5je8RNJfihdSsZvDn9U+N+SfOmY9V6d5BuGedG/ly41ZOSlSX67qn6ztfY39rz/QJL1JL+VpCX59tbaHw6D83Eel+TfVNVj0t1R/99OdogAi6taa7OuAwAALAVpIQAA0BPBNQAA9ERwDQAAPRFcAwBATwTXAADQE8E1AAD0RHANAAA9EVwDAEBP/n9wc8JmEUgFvAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbg3MT81r0Kk",
        "colab_type": "code",
        "outputId": "51314923-8e7f-4037-eb64-9fb702ee288a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "#Codes for Evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "l_list_tmp = l_list.reshape((1401))\n",
        "l_list2_tmp = l_list2.reshape((601))\n",
        "\n",
        "print(confusion_matrix(y_train,l_list_tmp))\n",
        "print(classification_report(y_train,l_list_tmp))\n",
        "print(accuracy_score(y_train, l_list_tmp))\n",
        "\n",
        "print(confusion_matrix(y_test,l_list2_tmp))\n",
        "print(classification_report(y_test,l_list2_tmp))\n",
        "print(accuracy_score(y_test, l_list2_tmp))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[436 263]\n",
            " [323 379]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.62      0.60       699\n",
            "           1       0.59      0.54      0.56       702\n",
            "\n",
            "    accuracy                           0.58      1401\n",
            "   macro avg       0.58      0.58      0.58      1401\n",
            "weighted avg       0.58      0.58      0.58      1401\n",
            "\n",
            "0.5817273376159886\n",
            "[[177 125]\n",
            " [129 170]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.59      0.58       302\n",
            "           1       0.58      0.57      0.57       299\n",
            "\n",
            "    accuracy                           0.58       601\n",
            "   macro avg       0.58      0.58      0.58       601\n",
            "weighted avg       0.58      0.58      0.58       601\n",
            "\n",
            "0.5773710482529119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN8u62m6r0qk",
        "colab_type": "code",
        "outputId": "62780594-ddc7-462f-9f41-f8f5b518535a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "class_1 = 0\n",
        "\n",
        "for i in range(0, 601):\n",
        "  if l_list2_tmp[i] == 1:\n",
        "    class_1 = class_1 + 1\n",
        "\n",
        "print(class_1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "295\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}