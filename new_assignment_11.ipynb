{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new_assignment_11.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNCDvmcH2BQ72hN2pXeeU5k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlguswn3659/class-MachineLearning/blob/master/new_assignment_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbdQEPZ_o5JX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import math\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re,string,unicodedata\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMRdc5kpo9vM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "7b1d0369-c77b-4016-b71b-c286c4d7304c"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# import zipfile\n",
        "# import io\n",
        "# zf = zipfile.ZipFile(io.BytesIO(uploaded['movie_review.zip']), \"r\")\n",
        "# zf.extractall()\n",
        "\n",
        "review_data = load_files(r\"movie_review\")\n",
        "X, y = review_data.data, review_data.target\n",
        "\n",
        "documents = []\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "for sen in range(0, len(X)):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    \n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    \n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    \n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    \n",
        "    # Lemmatization\n",
        "    document = document.split()\n",
        "    document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    \n",
        "    documents.append(document)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_uv_iLio-76",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc5493d8-ae32-4019-ac77-de0d601b423e"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=40000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "# vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents).toarray()\n",
        "print(X)\n",
        "\n",
        "vector_size = 898\n",
        "\n",
        "# tfidfconverter = TfidfTransformer()\n",
        "# print(tfidfconverter)\n",
        "# X = tfidfconverter.fit_transform(X).toarray()\n",
        "# print(X)\n",
        "\n",
        "X_train_tmp, X_test_tmp, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
        "\n",
        "# print(X_train.shape)\n",
        "\n",
        "X_pos_sum = np.zeros(12638)\n",
        "X_neg_sum = np.zeros(12638)\n",
        "\n",
        "X_train = np.zeros((1401, 897))\n",
        "X_test = np.zeros((601, 897))\n",
        "\n",
        "for i in range(0, 2002):\n",
        "  if y[i] == 0:\n",
        "    X_neg_sum = X_neg_sum + X[i]\n",
        "  else:\n",
        "    X_pos_sum = X_pos_sum + X[i]\n",
        "\n",
        "pos_neg_voca = 0\n",
        "\n",
        "for i in range(0, 12638):\n",
        "  # if abs(X_neg_sum[i] - X_pos_sum[i]) >= 30:\n",
        "  if (X_neg_sum[i]*4 <= X_pos_sum[i] and X_pos_sum[i] > 8) or (X_pos_sum[i]*4 <= X_neg_sum[i] and X_neg_sum[i] > 8):\n",
        "    for j in range(0, 1401):\n",
        "      X_train[j][pos_neg_voca] = X_train_tmp[j][i]\n",
        "    for j in range(0, 601):\n",
        "      X_test[j][pos_neg_voca] = X_test_tmp[j][i]\n",
        "    pos_neg_voca  = pos_neg_voca + 1\n",
        "\n",
        "# tfidfconverter = TfidfTransformer()\n",
        "# X_train = tfidfconverter.fit_transform(X_train).toarray()\n",
        "# X_test = tfidfconverter.fit_transform(X_test).toarray()\n",
        "\n",
        "print(X_neg_sum)\n",
        "print(X_pos_sum)\n",
        "print(pos_neg_voca)\n",
        "\n",
        "print(X_train)\n",
        "print(X_test)\n",
        "\n",
        "print(X_train[0])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[ 7. 60.  4. ...  3.  8.  5.]\n",
            "[ 2. 55.  6. ...  7.  6. 12.]\n",
            "897\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlTm8tUMrWdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#first hidden layer로 가기 위한 초기 fully connected u 생성\n",
        "w_tmp = np.ones(shape=(1401, 100, vector_size))\n",
        "v_tmp = np.ones(shape=(1401, 12, 197))\n",
        "u_tmp = np.ones(shape=(1401, 1, 12))\n",
        "\n",
        "matrix_u = np.zeros((100, vector_size))  #100줄\n",
        "matrix_u[0][0] = 1               #y레이어의 bias를 위한 값 1\n",
        "\n",
        "for i in range(1, 100):\n",
        "  for j in range(0, vector_size):\n",
        "    matrix_u[i][j] = np.random.normal()\n",
        "\n",
        "\n",
        "# matrix_u[196][0] = 0            #줄, 칸\n",
        "##############################################\n",
        "matrix_v = np.zeros((12, 100))  #12줄\n",
        "matrix_v[0][0] = 1               #y레이어의 bias를 위한 값 1\n",
        "\n",
        "for i in range(1, 12):\n",
        "  for j in range(0, 100):\n",
        "    matrix_v[i][j] = np.random.normal()\n",
        "\n",
        "\n",
        "# matrix_v[196][0] = 0            #줄, 칸\n",
        "##############################################\n",
        "matrix_w = np.zeros((1, 12))  #10줄\n",
        "# matrix_w[0][0] = 1               #y레이어의 bias를 위한 값 1\n",
        "\n",
        "for i in range(0, 1):\n",
        "  for j in range(0, 12):\n",
        "    matrix_w[i][j] = np.random.normal()\n",
        "\n",
        "w_tmp = matrix_w\n",
        "v_tmp = matrix_v\n",
        "u_tmp = matrix_u\n",
        "\n",
        "# matrix_w[196][0] = 0            #줄, 칸\n",
        "\n",
        "# print(matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrFN9qH2rXrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "\n",
        "y_ = np.ones((100, 1))\n",
        "\n",
        "x_list = np.ones(shape=(1401, vector_size, 1))\n",
        "\n",
        "y__list = np.ones(shape=(1401, 100, 1))\n",
        "z__list = np.ones(shape=(1401, 12, 1))\n",
        "h__list = np.ones(shape=(1401, 1, 1))\n",
        "\n",
        "y_list = np.ones(shape=(1401, 100, 1))\n",
        "z_list = np.ones(shape=(1401, 12, 1))\n",
        "h_list = np.ones(shape=(1401, 1, 1))\n",
        "l_list = np.zeros(shape=(1401, 1, 1))\n",
        "\n",
        "y_list_tmp = np.ones(shape=(1401, 100, 1))\n",
        "z_list_tmp = np.ones(shape=(1401, 12, 1))\n",
        "h_list_tmp = np.ones(shape=(1401, 1, 1))\n",
        "y__list_tmp = np.ones(shape=(1401, 100, 1))\n",
        "z__list_tmp = np.ones(shape=(1401, 12, 1))\n",
        "h__list_tmp = np.ones(shape=(1401, 1, 1))\n",
        "\n",
        "\n",
        "#training set 1401개에 대해서\n",
        "for i in range(0, 1401):\n",
        "  bias   = [1]       #bias 1\n",
        "  im_vector = np.concatenate((bias, X_train[i]), axis = None)\n",
        "\n",
        "  im_matrix   = im_vector.reshape((vector_size, 1))\n",
        "  \n",
        "  x_list[i] = im_matrix\n",
        "\n",
        "  y_ = np.dot(matrix_u, im_matrix)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "  y__list[i] = y_\n",
        "  \n",
        "  y_vector = np.ones((100,1)) #bias\n",
        "  for j in range (1, 100):\n",
        "    y_vector[j][0] = 1 / (1 + math.exp(-y_[j][0]))\n",
        "    # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  y   = y_vector.reshape((100, 1))\n",
        "  y_list[i] = y\n",
        "\n",
        "  z_  = np.dot(matrix_v, y)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "  z__list[i] = z_\n",
        "  z_vector = np.ones((12,1)) #bias\n",
        "  for j in range (1, 12):\n",
        "    z_vector[j][0] = 1 / (1 + math.exp(-z_[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  z   = z_vector.reshape((12, 1))\n",
        "  z_list[i] = z\n",
        "  \n",
        "  # print(z)\n",
        "\n",
        "  h_  = np.dot(matrix_w, z)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "  h__list[i] = h_\n",
        "  \n",
        "  h_vector = np.ones((1,1)) #bias\n",
        "  for j in range (0, 1):\n",
        "    h_vector[j][0] = 1 / (1 + math.exp(-h_[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "  h   = h_vector.reshape((1, 1))\n",
        "  \n",
        "  h_list[i] = h\n",
        "\n",
        "  tmp_label = 0\n",
        "  # print(h)\n",
        "\n",
        "  if h >= 0.5:\n",
        "    tmp_label = 1\n",
        "\n",
        "  l_list[i][0] = tmp_label\n",
        "\n",
        "  y_list_tmp[i] = y_list[i]\n",
        "  z_list_tmp[i] = z_list[i]\n",
        "  h_list_tmp[i] = h_list[i]\n",
        "  y__list_tmp[i] = y__list[i]\n",
        "  z__list_tmp[i] = z__list[i]\n",
        "  h__list_tmp[i] = h__list[i]\n",
        "\n",
        "# print(l_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB-qa4x8rZD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "\n",
        "y_2 = np.ones((100, 1))\n",
        "\n",
        "x_list2 = np.ones(shape=(601, vector_size, 1))\n",
        "\n",
        "y__list2 = np.ones(shape=(601, 100, 1))\n",
        "z__list2 = np.ones(shape=(601, 12, 1))\n",
        "h__list2 = np.ones(shape=(601, 1, 1))\n",
        "\n",
        "y_list2 = np.ones(shape=(601, 100, 1))\n",
        "z_list2 = np.ones(shape=(601, 12, 1))\n",
        "h_list2 = np.ones(shape=(601, 1, 1))\n",
        "l_list2 = np.zeros(shape=(601, 1, 1))\n",
        "\n",
        "y_list_tmp2 = np.ones(shape=(601, 100, 1))\n",
        "z_list_tmp2 = np.ones(shape=(601, 12, 1))\n",
        "h_list_tmp2 = np.ones(shape=(601, 1, 1))\n",
        "y__list_tmp2 = np.ones(shape=(601, 100, 1))\n",
        "z__list_tmp2 = np.ones(shape=(601, 12, 1))\n",
        "h__list_tmp2 = np.ones(shape=(601, 1, 1))\n",
        "\n",
        "\n",
        "#training set 1000개에 대해서\n",
        "for i in range(0, 601):\n",
        "  bias2   = [1]       #bias 1\n",
        "  im_vector2 = np.concatenate((bias2, X_test[i]), axis = None)\n",
        "\n",
        "  # print(type(list_image[: , i]))\n",
        "  # print(list_image[:, i])\n",
        "  # print(im_vector)\n",
        "  im_matrix2   = im_vector2.reshape((vector_size, 1))\n",
        "  \n",
        "  x_list2[i] = im_matrix2\n",
        "\n",
        "  y_2 = np.dot(matrix_u, im_matrix)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "  y__list2[i] = y_2\n",
        "  \n",
        "  y_vector2 = np.ones((100,1)) #bias\n",
        "  for j in range (1, 100):\n",
        "    y_vector2[j][0] = 1 / (1 + math.exp(-y_2[j][0]))\n",
        "    # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  y2   = y_vector2.reshape((100, 1))\n",
        "  y_list2[i] = y2\n",
        "\n",
        "  z_2  = np.dot(matrix_v, y2)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "  z__list2[i] = z_2\n",
        "  z_vector2 = np.ones((12,1)) #bias\n",
        "  for j in range (1, 12):\n",
        "    z_vector2[j][0] = 1 / (1 + math.exp(-z_2[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  z2   = z_vector2.reshape((12, 1))\n",
        "  z_list2[i] = z2\n",
        "  \n",
        "  # print(z)\n",
        "\n",
        "  h_2  = np.dot(matrix_w, z2)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "  h__list2[i] = h_2\n",
        "  \n",
        "  h_vector2 = np.ones((1,1)) #bias\n",
        "  for j in range (0, 1):\n",
        "    h_vector2[j][0] = 1 / (1 + math.exp(-h_2[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "  h2   = h_vector2.reshape((1, 1))\n",
        "  \n",
        "  h_list2[i] = h2\n",
        "\n",
        "  tmp_label = 0\n",
        "\n",
        "  if h2 >= 0.5:\n",
        "    tmp_label = 1\n",
        "\n",
        "  l_list2[i][0] = tmp_label\n",
        "\n",
        "  y_list_tmp2[i] = y_list2[i]\n",
        "  z_list_tmp2[i] = z_list2[i]\n",
        "  h_list_tmp2[i] = h_list2[i]\n",
        "  y__list_tmp2[i] = y__list2[i]\n",
        "  z__list_tmp2[i] = z__list2[i]\n",
        "  h__list_tmp2[i] = h__list2[i]\n",
        "\n",
        "# print(l_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j28x7DrlraXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "958b0130-14b9-4fd9-d966-6abb010c677d"
      },
      "source": [
        "#이제 여기에다가\n",
        "\n",
        "#iteration에 따라\n",
        "#cost function 값을 배열에 저장하고\n",
        "#세타값을 업데이트 시키고(위의 세개 함수 호출)\n",
        "#업데이트 시킨값을 다시 cost함수에 넣고\n",
        "#업데이트 시키고\n",
        "#이것을 반복하는 함수를 짠다.\n",
        "\n",
        "#아직 해야하는 것 : h값에 따라 가장 큰값의 index로 L값정하는거 위에 추가해야함.\n",
        "\n",
        "iteration = 5000\n",
        "l_r = 0.5\n",
        "lambda_ = 100\n",
        "\n",
        "\n",
        "#####################################\n",
        "\n",
        "cost_history = np.zeros(iteration)    #about train\n",
        "cost_history2 = np.zeros(iteration)   #about test\n",
        "\n",
        "label_group = np.zeros(shape = (1401, 1))\n",
        "label_group2 = np.zeros(shape = (601, 1))\n",
        "\n",
        "accuracy_history = np.zeros(iteration)\n",
        "accuracy_history2 = np.zeros(iteration)\n",
        "\n",
        "for i in range(0, 1401):\n",
        "  label_group[i][0] = l_list[i][0]\n",
        "\n",
        "for i in range(0, 601):\n",
        "  label_group2[i][0] = l_list2[i][0]\n",
        "\n",
        "#############################################\n",
        "\n",
        "\n",
        "for t in range(0, iteration):\n",
        "  number_of_correct = 0\n",
        "  number_of_correct2 = 0\n",
        "  \n",
        "  print(t, end=' ')\n",
        "\n",
        "  parameter_sum = 0\n",
        "\n",
        "  for q in range(0, vector_size):\n",
        "    for w in range(1, 100):\n",
        "      parameter_sum = parameter_sum + matrix_u[w][q]**2\n",
        "\n",
        "  for q in range(0, 100):\n",
        "    for w in range(1, 12):\n",
        "      parameter_sum = parameter_sum + matrix_u[w][q]**2\n",
        "\n",
        "  for q in range(0, 12):\n",
        "    for w in range(0, 1):\n",
        "      parameter_sum = parameter_sum + matrix_u[w][q]**2\n",
        "\n",
        "  result = 0.0\n",
        "  for i in range(0, 1401):\n",
        "    for k in range(0, 1):\n",
        "      # if h_list_tmp[i][k][0] <= 0:\n",
        "      # print(h_list_tmp[i][k][0])\n",
        "      result = result + ((-1)*label_group[i][k]*math.log(h_list[i][k][0]) - (1-label_group[i][k])*math.log(1-h_list[i][k][0])) + ((lambda_)/(2 * 91012)) * parameter_sum\n",
        "  result = result / 1401\n",
        "  cost_history[t] = result\n",
        "\n",
        "##gradient descent\n",
        "  result_w = 0\n",
        "  result_v = 0\n",
        "  result_u = 0\n",
        "\n",
        "  for i in range(0, 1401):\n",
        "    small_delta_2 = np.zeros(shape = (1, 1))\n",
        "    for j in range(0, 1):\n",
        "      small_delta_2[j][0] = h_list[i][j] - label_group[i][j]\n",
        "    # print(h_list[i])\n",
        "    # print(label_group[i])\n",
        "    # small_delta_2 = h_list[i] - label_group[i] #여기 label_group 다시 봐야함.\n",
        "    small_delta_1 = np.dot(np.transpose(matrix_w), small_delta_2)\n",
        "    small_delta_0 = np.dot(np.transpose(matrix_v), small_delta_1)*(y_list_tmp[i]*(1 - y_list_tmp[i]))\n",
        "\n",
        "    # print(small_delta_2)\n",
        "    result_w = result_w + np.dot(small_delta_2, np.transpose(z_list[i]))\n",
        "    result_v = result_v + np.dot(small_delta_1, np.transpose(y_list[i]))\n",
        "    result_u = result_u + np.dot(small_delta_0, np.transpose(x_list[i]))\n",
        "\n",
        "  matrix_w = matrix_w - (result_w / 1401 *l_r)\n",
        "  matrix_v = matrix_v - (result_v / 1401 *l_r)\n",
        "  matrix_u = matrix_u - (result_u / 1401 *l_r)\n",
        "\n",
        "\n",
        "####################################################################\n",
        "\n",
        "  for k in range(0, 1401):\n",
        "    bias   = [1]       #bias 1\n",
        "    im_vector = np.concatenate((bias, X_train[k]), axis = None)\n",
        "\n",
        "    im_matrix   = im_vector.reshape((vector_size, 1))\n",
        "\n",
        "    y_ = np.dot(matrix_u, im_matrix)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "    y__list[k] = y_\n",
        "    \n",
        "    y_vector = np.ones((100,1)) #bias\n",
        "    for j in range (1, 100):\n",
        "      y_vector[j][0] = 1 / (1 + math.exp(-y_[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "    \n",
        "    # print(y_vector) #시그모이드 값 확인\n",
        "    y   = y_vector.reshape((100, 1))\n",
        "    y_list[k] = y\n",
        "\n",
        "    z_  = np.dot(matrix_v, y)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "    z__list[k] = z_\n",
        "    z_vector = np.ones((12,1)) #bias\n",
        "    for j in range (1, 12):\n",
        "      z_vector[j][0] = 1 / (1 + math.exp(-z_[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "    z   = z_vector.reshape((12, 1))\n",
        "    z_list[k] = z\n",
        "    \n",
        "    # print(z)\n",
        "\n",
        "    h_  = np.dot(matrix_w, z)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "    h__list[k] = h_\n",
        "    \n",
        "    h_vector = np.ones((1,1)) #bias\n",
        "    for j in range (0, 1):\n",
        "      h_vector[j][0] = 1 / (1 + math.exp(-h_[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "    h   = h_vector.reshape((1, 1))\n",
        "\n",
        "    h_list[k] = h\n",
        "\n",
        "    tmp_label = 0\n",
        "\n",
        "    if h >= 0.5:\n",
        "      # print(h)\n",
        "      tmp_label = 1\n",
        "\n",
        "    l_list[k][0] = tmp_label\n",
        "\n",
        "    y_list_tmp[k] = y_list[k]\n",
        "    z_list_tmp[k] = z_list[k]\n",
        "    h_list_tmp[k] = h_list[k]\n",
        "    y__list_tmp[k] = y__list[k]\n",
        "    z__list_tmp[k] = z__list[k]\n",
        "    h__list_tmp[k] = h__list[k]\n",
        "\n",
        "    if l_list[k][0] == y_train[k]:\n",
        "      number_of_correct = number_of_correct + 1\n",
        "\n",
        "  accuracy_history[t] = number_of_correct / 1401 * 100\n",
        "  print(accuracy_history[t], end=' ')\n",
        "\n",
        "############################\n",
        "  #test data!\n",
        "  \n",
        "  # print(t, end=' ')\n",
        "\n",
        "  result2 = 0.0\n",
        "  for i in range(0, 601):\n",
        "    for k in range(0, 1):\n",
        "      result2 = result2 + ((-1)*label_group2[i][k]*math.log(h_list2[i][k][0]) - (1-label_group2[i][k])*math.log(1-h_list2[i][k][0])) + ((lambda_)/(2 * 91012)) * parameter_sum\n",
        "  result2 = result2 / 601\n",
        "  cost_history2[t] = result2\n",
        "\n",
        "\n",
        "  for k in range(0, 601):\n",
        "  # print(i)\n",
        "    bias2   = [1]       #bias 1\n",
        "    im_vector2 = np.concatenate((bias2, X_test[k]), axis = None)\n",
        "\n",
        "    im_matrix2   = im_vector2.reshape((vector_size, 1))\n",
        "\n",
        "    y_2 = np.dot(matrix_u, im_matrix2)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "    y__list2[k] = y_2\n",
        "    \n",
        "    y_vector2 = np.ones((100,1)) #bias\n",
        "    for j in range (1, 100):\n",
        "      y_vector2[j][0] = 1 / (1 + math.exp(-y_2[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "    \n",
        "    # print(y_vector) #시그모이드 값 확인\n",
        "    y2   = y_vector2.reshape((100, 1))\n",
        "    y_list2[k] = y2\n",
        "\n",
        "    z_2  = np.dot(matrix_v, y2)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "    z__list2[k] = z_2\n",
        "    z_vector2 = np.ones((12,1)) #bias\n",
        "    for j in range (1, 12):\n",
        "      z_vector2[j][0] = 1 / (1 + math.exp(-z_2[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "    z2   = z_vector2.reshape((12, 1))\n",
        "    z_list2[k] = z2\n",
        "    \n",
        "    # print(z)\n",
        "\n",
        "    h_2  = np.dot(matrix_w, z2)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "    h__list2[k] = h_2\n",
        "    \n",
        "    h_vector2 = np.ones((1,1)) #bias\n",
        "    for j in range (0, 1):\n",
        "      h_vector2[j][0] = 1 / (1 + math.exp(-h_2[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "    h2   = h_vector2.reshape((1, 1))\n",
        "\n",
        "    h_list2[k] = h2\n",
        "\n",
        "    tmp_label2 = 0\n",
        "\n",
        "    if h2 >= 0.5:\n",
        "      # print(h)\n",
        "      tmp_label2 = 1\n",
        "\n",
        "    l_list2[k][0] = tmp_label2\n",
        "\n",
        "    y_list_tmp2[k] = y_list2[k]\n",
        "    z_list_tmp2[k] = z_list2[k]\n",
        "    h_list_tmp2[k] = h_list2[k]\n",
        "    y__list_tmp2[k] = y__list2[k]\n",
        "    z__list_tmp2[k] = z__list2[k]\n",
        "    h__list_tmp2[k] = h__list2[k]\n",
        "\n",
        "    if l_list2[k][0] == y_test[k]:\n",
        "      number_of_correct2 = number_of_correct2 + 1\n",
        "\n",
        "  accuracy_history2[t] = number_of_correct2 / 601 * 100\n",
        "  print(accuracy_history2[t])\n",
        "\n",
        "  t = t + 1"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 48.89364739471806 50.91514143094842\n",
            "1 49.6074232690935 48.41930116472546\n",
            "2 48.394004282655246 49.750415973377706\n",
            "3 49.82155603140614 47.587354409317804\n",
            "4 49.53604568165596 49.91680532445923\n",
            "5 49.46466809421842 48.25291181364393\n",
            "6 49.892933618843685 50.249584026622294\n",
            "7 49.107780157030696 49.91680532445923\n",
            "8 49.67880085653105 50.08319467554077\n",
            "9 48.82226980728051 50.582362728785355\n",
            "10 49.6074232690935 49.91680532445923\n",
            "11 49.107780157030696 50.91514143094842\n",
            "12 49.6074232690935 49.750415973377706\n",
            "13 48.82226980728051 50.582362728785355\n",
            "14 49.393290506780865 50.249584026622294\n",
            "15 48.60813704496788 50.249584026622294\n",
            "16 49.03640256959314 49.91680532445923\n",
            "17 48.46538187009279 50.249584026622294\n",
            "18 49.321912919343326 50.249584026622294\n",
            "19 48.75089221984297 49.25124792013311\n",
            "20 49.03640256959314 50.582362728785355\n",
            "21 48.965024982155605 49.417637271214645\n",
            "22 49.82155603140614 50.41597337770383\n",
            "23 49.03640256959314 49.08485856905158\n",
            "24 49.321912919343326 50.91514143094842\n",
            "25 48.965024982155605 49.08485856905158\n",
            "26 49.03640256959314 51.58069883527454\n",
            "27 49.67880085653105 48.58569051580699\n",
            "28 49.17915774446824 50.41597337770383\n",
            "29 49.67880085653105 47.75374376039934\n",
            "30 49.17915774446824 50.41597337770383\n",
            "31 49.46466809421842 47.587354409317804\n",
            "32 49.6074232690935 49.91680532445923\n",
            "33 49.393290506780865 47.42096505823628\n",
            "34 49.82155603140614 49.750415973377706\n",
            "35 49.25053533190578 47.42096505823628\n",
            "36 49.75017844396859 48.58569051580699\n",
            "37 48.89364739471806 47.25457570715474\n",
            "38 49.46466809421842 48.91846921797005\n",
            "39 48.67951463240542 46.92179700499168\n",
            "40 49.46466809421842 48.91846921797005\n",
            "41 48.67951463240542 46.75540765391015\n",
            "42 49.393290506780865 49.25124792013311\n",
            "43 48.75089221984297 46.75540765391015\n",
            "44 49.393290506780865 49.08485856905158\n",
            "45 48.67951463240542 46.75540765391015\n",
            "46 49.6074232690935 48.75207986688852\n",
            "47 48.67951463240542 46.92179700499168\n",
            "48 49.82155603140614 48.75207986688852\n",
            "49 48.67951463240542 46.92179700499168\n",
            "50 49.46466809421842 48.91846921797005\n",
            "51 48.53675945753034 46.92179700499168\n",
            "52 49.46466809421842 48.91846921797005\n",
            "53 48.394004282655246 47.088186356073216\n",
            "54 49.17915774446824 48.91846921797005\n",
            "55 48.394004282655246 47.25457570715474\n",
            "56 49.46466809421842 49.58402662229617\n",
            "57 48.60813704496788 47.088186356073216\n",
            "58 49.53604568165596 49.25124792013311\n",
            "59 48.394004282655246 47.25457570715474\n",
            "60 49.53604568165596 49.417637271214645\n",
            "61 48.394004282655246 47.25457570715474\n",
            "62 49.53604568165596 49.417637271214645\n",
            "63 48.46538187009279 46.92179700499168\n",
            "64 49.53604568165596 48.91846921797005\n",
            "65 48.53675945753034 46.75540765391015\n",
            "66 49.53604568165596 49.08485856905158\n",
            "67 48.46538187009279 46.75540765391015\n",
            "68 49.53604568165596 49.08485856905158\n",
            "69 48.60813704496788 46.58901830282861\n",
            "70 49.25053533190578 49.08485856905158\n",
            "71 48.75089221984297 46.92179700499168\n",
            "72 49.46466809421842 48.91846921797005\n",
            "73 48.394004282655246 46.92179700499168\n",
            "74 49.393290506780865 48.91846921797005\n",
            "75 48.3226266952177 46.92179700499168\n",
            "76 49.25053533190578 48.75207986688852\n",
            "77 48.17987152034261 47.088186356073216\n",
            "78 49.107780157030696 49.08485856905158\n",
            "79 48.037116345467524 47.25457570715474\n",
            "80 48.82226980728051 49.25124792013311\n",
            "81 48.037116345467524 47.42096505823628\n",
            "82 48.67951463240542 48.91846921797005\n",
            "83 47.96573875802998 47.25457570715474\n",
            "84 48.75089221984297 48.75207986688852\n",
            "85 48.10849393290507 47.088186356073216\n",
            "86 48.60813704496788 48.58569051580699\n",
            "87 48.10849393290507 47.088186356073216\n",
            "88 48.89364739471806 48.75207986688852\n",
            "89 48.3226266952177 46.92179700499168\n",
            "90 49.107780157030696 48.41930116472546\n",
            "91 48.3226266952177 47.25457570715474\n",
            "92 48.82226980728051 48.41930116472546\n",
            "93 48.3226266952177 46.92179700499168\n",
            "94 48.89364739471806 48.41930116472546\n",
            "95 47.82298358315489 46.92179700499168\n",
            "96 48.965024982155605 48.58569051580699\n",
            "97 47.82298358315489 46.92179700499168\n",
            "98 48.965024982155605 48.91846921797005\n",
            "99 47.6802284082798 47.088186356073216\n",
            "100 49.17915774446824 48.75207986688852\n",
            "101 47.60885082084226 46.75540765391015\n",
            "102 49.46466809421842 48.75207986688852\n",
            "103 47.60885082084226 46.75540765391015\n",
            "104 49.67880085653105 49.25124792013311\n",
            "105 47.60885082084226 47.75374376039934\n",
            "106 49.75017844396859 49.417637271214645\n",
            "107 47.39471805852962 47.587354409317804\n",
            "108 49.67880085653105 49.25124792013311\n",
            "109 47.53747323340471 47.75374376039934\n",
            "110 49.46466809421842 49.25124792013311\n",
            "111 47.6802284082798 47.75374376039934\n",
            "112 49.53604568165596 48.91846921797005\n",
            "113 47.6802284082798 47.587354409317804\n",
            "114 49.25053533190578 48.75207986688852\n",
            "115 47.53747323340471 47.587354409317804\n",
            "116 49.393290506780865 48.91846921797005\n",
            "117 47.53747323340471 47.587354409317804\n",
            "118 49.53604568165596 48.41930116472546\n",
            "119 47.75160599571734 47.25457570715474\n",
            "120 49.53604568165596 48.41930116472546\n",
            "121 47.89436117059244 47.25457570715474\n",
            "122 49.53604568165596 48.58569051580699\n",
            "123 48.10849393290507 47.25457570715474\n",
            "124 49.67880085653105 48.25291181364393\n",
            "125 48.10849393290507 47.587354409317804\n",
            "126 49.82155603140614 48.25291181364393\n",
            "127 48.10849393290507 47.587354409317804\n",
            "128 49.67880085653105 48.41930116472546\n",
            "129 48.3226266952177 47.587354409317804\n",
            "130 49.53604568165596 48.25291181364393\n",
            "131 48.3226266952177 47.75374376039934\n",
            "132 49.17915774446824 48.25291181364393\n",
            "133 48.394004282655246 48.0865224625624\n",
            "134 48.965024982155605 48.0865224625624\n",
            "135 48.53675945753034 48.0865224625624\n",
            "136 49.03640256959314 47.920133111480865\n",
            "137 48.60813704496788 48.0865224625624\n",
            "138 49.03640256959314 47.920133111480865\n",
            "139 48.75089221984297 47.920133111480865\n",
            "140 48.965024982155605 48.0865224625624\n",
            "141 48.67951463240542 48.0865224625624\n",
            "142 49.03640256959314 47.920133111480865\n",
            "143 48.67951463240542 47.587354409317804\n",
            "144 48.89364739471806 47.75374376039934\n",
            "145 48.67951463240542 47.75374376039934\n",
            "146 48.89364739471806 47.920133111480865\n",
            "147 48.67951463240542 47.587354409317804\n",
            "148 49.03640256959314 47.587354409317804\n",
            "149 48.67951463240542 47.42096505823628\n",
            "150 49.17915774446824 47.75374376039934\n",
            "151 48.75089221984297 47.25457570715474\n",
            "152 49.393290506780865 47.25457570715474\n",
            "153 48.75089221984297 47.42096505823628\n",
            "154 49.393290506780865 46.92179700499168\n",
            "155 48.89364739471806 47.42096505823628\n",
            "156 49.321912919343326 46.92179700499168\n",
            "157 49.107780157030696 47.587354409317804\n",
            "158 49.25053533190578 47.088186356073216\n",
            "159 49.107780157030696 47.75374376039934\n",
            "160 49.321912919343326 47.088186356073216\n",
            "161 49.17915774446824 47.75374376039934\n",
            "162 49.393290506780865 46.92179700499168\n",
            "163 49.25053533190578 47.42096505823628\n",
            "164 49.25053533190578 47.088186356073216\n",
            "165 49.107780157030696 47.587354409317804\n",
            "166 49.25053533190578 47.25457570715474\n",
            "167 49.25053533190578 47.587354409317804\n",
            "168 49.393290506780865 47.42096505823628\n",
            "169 49.17915774446824 47.75374376039934\n",
            "170 49.25053533190578 47.42096505823628\n",
            "171 49.17915774446824 47.920133111480865\n",
            "172 49.17915774446824 47.088186356073216\n",
            "173 49.107780157030696 47.75374376039934\n",
            "174 49.17915774446824 47.088186356073216\n",
            "175 49.17915774446824 47.75374376039934\n",
            "176 49.107780157030696 47.088186356073216\n",
            "177 49.17915774446824 47.920133111480865\n",
            "178 49.17915774446824 47.25457570715474\n",
            "179 49.25053533190578 47.587354409317804\n",
            "180 49.03640256959314 47.25457570715474\n",
            "181 49.25053533190578 47.587354409317804\n",
            "182 49.107780157030696 47.25457570715474\n",
            "183 49.321912919343326 47.587354409317804\n",
            "184 49.17915774446824 47.42096505823628\n",
            "185 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-d7ec36e76d1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mresult_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_w\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_delta_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mresult_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_v\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_delta_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mresult_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_u\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_delta_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0mmatrix_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_w\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult_w\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1401\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ml_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMcbWdbh98uY",
        "colab_type": "text"
      },
      "source": [
        "# **Machine Learning Assignment09** #\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Name : 이현주\n",
        "\n",
        "Student # : 20184060"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CwXihsOJgZ0R"
      },
      "source": [
        "# **1. Plot the loss curve [10pt]**\n",
        "\n",
        "- plot the training loss at every iteration of gradient descent using the training data in blue color (the first 6,000 images) [5pt]\n",
        "- plot the testing loss at every iteration of gradient descent using the testing data in red color (the rest 4,000 images) [5pt]\n",
        "- the both curves should be presented in one figure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_ZPJSBnroog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#그 코드를 바탕으로 training loss plotting 하기.\n",
        "real_iter = iteration\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "cost_tmp = np.zeros(real_iter)\n",
        "\n",
        "for i in range(0, real_iter):\n",
        "  cost_tmp[i] = cost_history[i]\n",
        "\n",
        "\n",
        "# print(cost_history[1])\n",
        "\n",
        "ax.set_ylabel('J(Theta)')\n",
        "ax.set_xlabel('Iterations')\n",
        "_=ax.plot(range(real_iter),cost_tmp,'b.', color = 'blue')\n",
        "_=ax.plot(range(real_iter),cost_history2,'b.', color = 'red')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OiKcfzyYgaCF"
      },
      "source": [
        "# **2. Plot the accuracy curve [10pt]**\n",
        "\n",
        "- plot the training accuracy (%) at every iteration of gradient descent using the training data in blue color (the first 6,000 images) [5pt]\n",
        "- plot the testing accuracy (%) at every iteration of gradient descent using the testing data in red color (the rest 4,000 images) [5pt]\n",
        "- the both curves should be presented in one figure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L688vFb8rqtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#그 코드를 바탕으로 training loss plotting 하기.\n",
        "fig,ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "# print(cost_history[1])\n",
        "\n",
        "ax.set_ylabel('accuracy')\n",
        "ax.set_xlabel('Iterations')\n",
        "_=ax.plot(range(iteration),accuracy_history,'b.', color = 'blue')\n",
        "_=ax.plot(range(iteration),accuracy_history2,'b.', color = 'red')\n",
        "# print(ac_tmp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbg3MT81r0Kk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Codes for Evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "l_list_tmp = l_list.reshape((1401))\n",
        "l_list2_tmp = l_list2.reshape((601))\n",
        "\n",
        "print(confusion_matrix(y_train,l_list_tmp))\n",
        "print(classification_report(y_train,l_list_tmp))\n",
        "print(accuracy_score(y_train, l_list_tmp))\n",
        "\n",
        "print(confusion_matrix(y_test,l_list2_tmp))\n",
        "print(classification_report(y_test,l_list2_tmp))\n",
        "print(accuracy_score(y_test, l_list2_tmp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN8u62m6r0qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_1 = 0\n",
        "\n",
        "for i in range(0, 601):\n",
        "  if l_list2_tmp[i] == 1:\n",
        "    class_1 = class_1 + 1\n",
        "\n",
        "print(class_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UueZmZy1r4ad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(y_train)\n",
        "print(l_list_tmp)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}