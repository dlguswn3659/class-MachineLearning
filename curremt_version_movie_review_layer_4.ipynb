{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment11-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgOZs3sQBgfPEnHutOHFkn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlguswn3659/class-MachineLearning/blob/master/curremt_version_movie_review_layer_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw-faMg3kxeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re,string,unicodedata\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
      ],
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEmx3Nerk1ud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "25f1680a-6d6a-4506-ca34-9f7aa2d0718e"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# import zipfile\n",
        "# import io\n",
        "# zf = zipfile.ZipFile(io.BytesIO(uploaded['movie_review.zip']), \"r\")\n",
        "# zf.extractall()\n",
        "\n",
        "review_data = load_files(r\"movie_review\")\n",
        "X, y = review_data.data, review_data.target\n",
        "\n",
        "documents = []\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "for sen in range(0, len(X)):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    \n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    \n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    \n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    \n",
        "    # Lemmatization\n",
        "    document = document.split()\n",
        "    document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    \n",
        "    documents.append(document)"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "matjDvyhlDJJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "995f74a0-c161-4dbc-e53b-4880cb580dcd"
      },
      "source": [
        "print(documents)"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt9Wj3kH5q8w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cb4ecd6e-e4a5-4464-a4fb-37f5f18e9cd6"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=40000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "# vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents).toarray()\n",
        "print(X)\n",
        "\n",
        "vector_size = 885\n",
        "\n",
        "# tfidfconverter = TfidfTransformer()\n",
        "# print(tfidfconverter)\n",
        "# X = tfidfconverter.fit_transform(X).toarray()\n",
        "# print(X)\n",
        "\n",
        "X_train_tmp, X_test_tmp, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
        "\n",
        "# print(X_train.shape)\n",
        "\n",
        "X_pos_sum = np.zeros(12638)\n",
        "X_neg_sum = np.zeros(12638)\n",
        "\n",
        "X_train = np.zeros((1401, 884))\n",
        "X_test = np.zeros((601, 884))\n",
        "\n",
        "for i in range(0, 2002):\n",
        "  if y[i] == 0:\n",
        "    X_neg_sum = X_neg_sum + X[i]\n",
        "  else:\n",
        "    X_pos_sum = X_pos_sum + X[i]\n",
        "\n",
        "pos_neg_voca = 0\n",
        "\n",
        "for i in range(0, 12638):\n",
        "  # if abs(X_neg_sum[i] - X_pos_sum[i]) >= 30:\n",
        "  if (X_neg_sum[i]*3 <= X_pos_sum[i] and X_neg_sum[i] > 2) or (X_pos_sum[i]*3 <= X_neg_sum[i] and X_pos_sum[i] > 2):\n",
        "    for j in range(0, 1401):\n",
        "      X_train[j][pos_neg_voca] = X_train_tmp[j][i]\n",
        "    for j in range(0, 601):\n",
        "      X_test[j][pos_neg_voca] = X_test_tmp[j][i]\n",
        "    pos_neg_voca  = pos_neg_voca + 1\n",
        "\n",
        "print(X_neg_sum)\n",
        "print(X_pos_sum)\n",
        "print(pos_neg_voca)\n",
        "\n",
        "print(X_train)\n",
        "print(X_test)\n",
        "\n",
        "print(X_train[0])"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[ 7. 60.  4. ...  3.  8.  5.]\n",
            "[ 2. 55.  6. ...  7.  6. 12.]\n",
            "884\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQpyXu3jGOvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "d5a6887d-61ed-42b1-f25a-d55873517973"
      },
      "source": [
        "print(X_train)\n",
        "print(X_test)\n",
        "print(y_train)\n",
        "print(y_test)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[0 0 1 ... 1 0 1]\n",
            "[1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0\n",
            " 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0\n",
            " 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0\n",
            " 0 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 0]\n",
            "(1401, 884)\n",
            "(601, 884)\n",
            "(1401,)\n",
            "(601,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSA68oWdBX2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_classifier에 대한 함수 정의하기\n",
        "# def train_classifier(X_Train, y_Train):\n"
      ],
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD42xdkL05UT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #Codes for classification\n",
        "\n",
        "# classifier = train_classifier(X_train, y_train)\n",
        "# y_pred_test = classifier.predict(X_test)\n",
        "# y_pred_train = classifier.predict(X_train)"
      ],
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV9K1MVaET0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix_t = np.zeros((350, vector_size))  #350줄\n",
        "matrix_t[0][0] = 1               #y레이어의 bias를 위한 값 1\n",
        "\n",
        "for i in range(1, 350):\n",
        "  for j in range(0, vector_size):\n",
        "    matrix_t[i][j] = np.random.normal()\n",
        "\n",
        "\n",
        "# matrix_u[196][0] = 0            #줄, 칸\n",
        "##############################################\n",
        "matrix_u = np.zeros((50, 350))  #50줄\n",
        "matrix_u[0][0] = 1               #y레이어의 bias를 위한 값 1\n",
        "\n",
        "for i in range(1, 50):\n",
        "  for j in range(0, 350):\n",
        "    matrix_u[i][j] = np.random.normal()\n",
        "\n",
        "\n",
        "# matrix_u[196][0] = 0            #줄, 칸\n",
        "##############################################\n",
        "matrix_v = np.zeros((8, 50))  #8줄\n",
        "matrix_v[0][0] = 1               #y레이어의 bias를 위한 값 1\n",
        "\n",
        "for i in range(1, 8):\n",
        "  for j in range(0, 50):\n",
        "    matrix_v[i][j] = np.random.normal()\n",
        "\n",
        "\n",
        "# matrix_v[196][0] = 0            #줄, 칸\n",
        "##############################################\n",
        "matrix_w = np.zeros((1, 8))  #1줄\n",
        "\n",
        "for i in range(0, 1):\n",
        "  for j in range(0, 8):\n",
        "    matrix_w[i][j] = np.random.normal()\n",
        "\n",
        "# matrix_w[196][0] = 0            #줄, 칸\n",
        "\n",
        "# print(matrix)\n",
        "\n",
        "matrix_t=np.random.normal(0, 0.1,(349,len(X_train[0])+1)) #bias 포함\n",
        "# matrix_t[0] = 0\n",
        "# matrix_t[0][0] = 1\n",
        "matrix_u=np.random.normal(0, 0.1,(49,350))\n",
        "# matrix_u[0] = 0\n",
        "# matrix_u[0][0] = 1\n",
        "matrix_v=np.random.normal(0, 0.1,(7,50))\n",
        "# matrix_v[0] = 0\n",
        "# matrix_v[0][0] = 1\n",
        "matrix_w=np.random.normal(0, 0.1,(8))[np.newaxis]"
      ],
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfHkfVcXRZIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import matplotlib.animation as animation\n",
        "import operator\n",
        "from scipy import stats \n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.datasets.samples_generator import make_regression \n",
        "import csv\n",
        "import math\n",
        "import statistics"
      ],
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLiiakUcEfil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "\n",
        "train_x=X_train\n",
        "test_x=X_test\n",
        "train_l=y_train\n",
        "test_l=y_test\n",
        "\n",
        "train_x=np.transpose(train_x)\n",
        "test_x=np.transpose(test_x)\n",
        "\n",
        "train_bias=np.ones((len(train_x[0])), dtype=float)\n",
        "test_bias=np.ones(len((test_x[0])), dtype=float)\n",
        "\n",
        "train_x=np.insert(train_x,0,train_bias,axis=0)\n",
        "test_x=np.insert(test_x,0,test_bias,axis=0)\n",
        "\n",
        "x_ = np.ones((350, 1))\n",
        "y_ = np.ones((50, 1))\n",
        "\n",
        "j_list = np.ones(shape=(1401, vector_size, 1))\n",
        "x_list = np.ones(shape=(1401, 350, 1))\n",
        "\n",
        "x__list = np.ones(shape=(1401, 350, 1))\n",
        "y__list = np.ones(shape=(1401, 50, 1))\n",
        "z__list = np.ones(shape=(1401, 8, 1))\n",
        "h__list = np.ones(shape=(1401, 1, 1))\n",
        "\n",
        "x_list = np.ones(shape=(1401, 350, 1))\n",
        "y_list = np.ones(shape=(1401, 50, 1))\n",
        "z_list = np.ones(shape=(1401, 8, 1))\n",
        "h_list = np.ones(shape=(1401, 1, 1))\n",
        "l_list = np.zeros(shape=(1401, 1, 1))\n",
        "\n",
        "x_list_tmp = np.ones(shape=(1401, 350, 1))\n",
        "y_list_tmp = np.ones(shape=(1401, 50, 1))\n",
        "z_list_tmp = np.ones(shape=(1401, 8, 1))\n",
        "h_list_tmp = np.ones(shape=(1401, 1, 1))\n",
        "\n",
        "x__list_tmp = np.ones(shape=(1401, 350, 1))\n",
        "y__list_tmp = np.ones(shape=(1401, 50, 1))\n",
        "z__list_tmp = np.ones(shape=(1401, 8, 1))\n",
        "h__list_tmp = np.ones(shape=(1401, 1, 1))\n",
        "\n",
        "x_list = np.zeros((350, len(train_x[0])), dtype=float)\n",
        "y_list = np.zeros((50,len(train_x[0])), dtype=float)\n",
        "z_list = np.zeros((8,len(train_x[0])), dtype=float)\n",
        "h_list = np.zeros((1, len(train_x[0])), dtype=float)\n",
        "\n",
        "x_list2 = np.zeros((350, len(test_x[0])), dtype=float)\n",
        "y_list2 = np.zeros((50,len(test_x[0])), dtype=float)\n",
        "z_list2 = np.zeros((8,len(test_x[0])), dtype=float)\n",
        "h_list2 = (np.zeros((len(test_x[0])), dtype=float))[np.newaxis]\n",
        "\n",
        "for i in range(0,1401):\n",
        "  x_list[0][i]=1\n",
        "  y_list[0][i]=1\n",
        "\n",
        "for i in range(0,601):\n",
        "  x_list2[0][i]=1\n",
        "  y_list2[0][i]=1\n",
        "\n",
        "\n",
        "#training set 1401개에 대해서\n",
        "# for i in range(0, 1401):  \n",
        "#   bias   = [1]       #bias 1\n",
        "#   im_vector = np.concatenate((bias, X_train[i]), axis = None)\n",
        "#   j_list[i] = im_vector.reshape((vector_size, 1))\n",
        "\n",
        "#   x_ = np.dot(matrix_t, j_list[i])  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "#   x__list[i] = x_\n",
        "  \n",
        "#   x_vector = np.ones((350,1)) #bias\n",
        "#   for j in range (1, 350):\n",
        "#     x_vector[j][0] = 1 / (1 + math.exp(-x_[j][0]))\n",
        "#     # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "#   x   = x_vector.reshape((350, 1))\n",
        "#   x_list[i] = x\n",
        "\n",
        "\n",
        "\n",
        "#   y_ = np.dot(matrix_u, x)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "#   y__list[i] = y_\n",
        "  \n",
        "#   y_vector = np.ones((50,1)) #bias\n",
        "#   for j in range (1, 50):\n",
        "#     y_vector[j][0] = 1 / (1 + math.exp(-y_[j][0]))\n",
        "#     # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "#   y   = y_vector.reshape((50, 1))\n",
        "#   y_list[i] = y\n",
        "\n",
        "#   z_  = np.dot(matrix_v, y)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "#   z__list[i] = z_\n",
        "#   z_vector = np.ones((8,1)) #bias\n",
        "#   for j in range (1, 8):\n",
        "#     z_vector[j][0] = 1 / (1 + math.exp(-z_[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "#   z   = z_vector.reshape((8, 1))\n",
        "#   z_list[i] = z\n",
        "  \n",
        "#   # print(z)\n",
        "\n",
        "#   h_  = np.dot(matrix_w, z)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "#   h__list[i] = h_\n",
        "  \n",
        "#   h_vector = np.ones((1,1)) #bias\n",
        "#   for j in range (0, 1):\n",
        "#     h_vector[j][0] = 1 / (1 + math.exp(-h_[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "#   h   = h_vector.reshape((1, 1))\n",
        "#   # tmp = h.ravel()\n",
        "#   # max_value = max(tmp)\n",
        "#   # print(max_value)\n",
        "#   # max_index = tmp.index(max_value)\n",
        "#   h_list[i] = h\n",
        "\n",
        "#   tmp_label = 0\n",
        "#   # print(h)\n",
        "\n",
        "#   if h >= 0.5:\n",
        "#     tmp_label = 1\n",
        "\n",
        "#   l_list[i][0] = tmp_label\n",
        "#   # print(l_list[i])\n",
        "#   # print(h)\n",
        "\n",
        "#   x_list_tmp[i] = x_list[i]\n",
        "#   y_list_tmp[i] = y_list[i]\n",
        "#   z_list_tmp[i] = z_list[i]\n",
        "#   h_list_tmp[i] = h_list[i]\n",
        "#   x__list_tmp[i] = x__list[i]\n",
        "#   y__list_tmp[i] = y__list[i]\n",
        "#   z__list_tmp[i] = z__list[i]\n",
        "#   h__list_tmp[i] = h__list[i]\n",
        "x__list = np.dot(matrix_t,train_x)\n",
        "x__list2 = np.dot(matrix_t,test_x)\n",
        "\n",
        "for a in range(1,350):\n",
        "  for b in range(0,len(train_x[0])):\n",
        "    x_list[a][b]=1 / (1 + math.exp(-x__list[a-1][b]))\n",
        "\n",
        "for a in range(1,350):\n",
        "  for b in range(0,len(test_x[0])):\n",
        "    x_list2[a][b]=1 / (1 + math.exp(-x__list2[a-1][b]))\n",
        "\n",
        "y__list=np.dot(matrix_u,x_list)\n",
        "y__list2=np.dot(matrix_u,x_list2)\n",
        "\n",
        "for a in range(1,50):\n",
        "  for b in range(0,len(train_x[0])):\n",
        "    y_list[a][b]=1 / (1 + math.exp(-y__list[a-1][b]))\n",
        "\n",
        "for a in range(1,50):\n",
        "  for b in range(0,len(test_x[0])):\n",
        "    y_list2[a][b]=1 / (1 + math.exp(-y__list2[a-1][b]))\n",
        "  \n",
        "z__list=np.dot(matrix_v,y_list)\n",
        "z__list2=np.dot(matrix_v,y_list2)\n",
        "\n",
        "for a in range(1,8):\n",
        "  for b in range(0,len(train_x[0])):\n",
        "    z_list[a][b]=1 / (1 + math.exp(-z__list[a-1][b]))\n",
        "\n",
        "for a in range(1,8):\n",
        "  for b in range(0,len(test_x[0])):\n",
        "    z_list2[a][b]=1 / (1 + math.exp(-z__list2[a-1][b]))\n",
        "\n",
        "h__list=np.dot(matrix_w,z_list)\n",
        "h__list2=np.dot(matrix_w,z_list2)\n",
        "\n",
        "for b in range(0,len(train_x[0])):\n",
        "  h_list[0][b]=1/(1 + math.exp(-h__list[0][b]))\n",
        "  \n",
        "for b in range(0,len(test_x[0])):\n",
        "  h_list2[0][b]=1/(1 + math.exp(-h__list2[0][b]))\n",
        "\n",
        "for i in range(0, 1401) :\n",
        "  tmp_label = 0\n",
        "  if h_list[0][i] >= 0.5 :\n",
        "    tmp_label = 1\n",
        "\n",
        "  l_list[i][0] = tmp_label\n",
        "\n",
        "for i in range(0, 601) :\n",
        "  tmp_label = 0\n",
        "  if h_list2[0][i] >= 0.5 :\n",
        "    tmp_label = 1\n",
        "\n",
        "  l_list2[i][0] = tmp_label\n",
        "\n",
        "# print(l_list)"
      ],
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b91lsWSoEg8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "\n",
        "# x_2 = np.ones((350, 1))\n",
        "# y_2 = np.ones((50, 1))\n",
        "\n",
        "# j_list2 = np.ones(shape=(601, vector_size, 1))\n",
        "# x_list2 = np.ones(shape=(601, 350, 1))\n",
        "\n",
        "# x__list2 = np.ones(shape=(601, 350, 1))\n",
        "# y__list2 = np.ones(shape=(601, 50, 1))\n",
        "# z__list2 = np.ones(shape=(601, 8, 1))\n",
        "# h__list2 = np.ones(shape=(601, 1, 1))\n",
        "\n",
        "# x_list2 = np.ones(shape=(601, 350, 1))\n",
        "# y_list2 = np.ones(shape=(601, 50, 1))\n",
        "# z_list2 = np.ones(shape=(601, 8, 1))\n",
        "# h_list2 = np.ones(shape=(601, 1, 1))\n",
        "# l_list2 = np.zeros(shape=(601, 1, 1))\n",
        "\n",
        "# x_list_tmp2 = np.ones(shape=(601, 350, 1))\n",
        "# y_list_tmp2 = np.ones(shape=(601, 50, 1))\n",
        "# z_list_tmp2 = np.ones(shape=(601, 8, 1))\n",
        "# h_list_tmp2 = np.ones(shape=(601, 1, 1))\n",
        "\n",
        "# x__list_tmp2 = np.ones(shape=(601, 350, 1))\n",
        "# y__list_tmp2 = np.ones(shape=(601, 50, 1))\n",
        "# z__list_tmp2 = np.ones(shape=(601, 8, 1))\n",
        "# h__list_tmp2 = np.ones(shape=(601, 1, 1))\n",
        "\n",
        "\n",
        "#training set 1401개에 대해서\n",
        "# for i in range(0, 601):  \n",
        "#   bias2   = [1]       #bias 1\n",
        "#   im_vector2 = np.concatenate((bias2, X_test[i]), axis = None)\n",
        "#   j_list2[i] = im_vector2.reshape((vector_size, 1))\n",
        "\n",
        "#   x_2 = np.dot(matrix_t, j_list2[i])  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "#   x__list2[i] = x_2\n",
        "  \n",
        "#   x_vector2 = np.ones((350,1)) #bias\n",
        "#   for j in range (1, 350):\n",
        "#     x_vector2[j][0] = 1 / (1 + math.exp(-x_2[j][0]))\n",
        "#     # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "#   x2   = x_vector2.reshape((350, 1))\n",
        "#   x_list2[i] = x2\n",
        "\n",
        "\n",
        "\n",
        "#   y_2 = np.dot(matrix_u, x2)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "#   y__list2[i] = y_2\n",
        "  \n",
        "#   y_vector2 = np.ones((50,1)) #bias\n",
        "#   for j in range (1, 50):\n",
        "#     y_vector2[j][0] = 1 / (1 + math.exp(-y_2[j][0]))\n",
        "#     # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "#   y2   = y_vector2.reshape((50, 1))\n",
        "#   y_list2[i] = y2\n",
        "\n",
        "#   z_2  = np.dot(matrix_v, y2)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "#   z__list2[i] = z_2\n",
        "#   z_vector2 = np.ones((8,1)) #bias\n",
        "#   for j in range (1, 8):\n",
        "#     z_vector2[j][0] = 1 / (1 + math.exp(-z_2[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "#   z2   = z_vector2.reshape((8, 1))\n",
        "#   z_list2[i] = z2\n",
        "  \n",
        "#   # print(z)\n",
        "\n",
        "#   h_2 = np.dot(matrix_w, z2)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "#   h__list2[i] = h_2\n",
        "  \n",
        "#   h_vector2 = np.ones((1,1)) #bias\n",
        "#   for j in range (0, 1):\n",
        "#     h_vector2[j][0] = 1 / (1 + math.exp(-h_2[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "#   h2   = h_vector2.reshape((1, 1))\n",
        "#   # tmp = h.ravel()\n",
        "#   # max_value = max(tmp)\n",
        "#   # print(max_value)\n",
        "#   # max_index = tmp.index(max_value)\n",
        "#   h_list2[i] = h2\n",
        "\n",
        "#   tmp_label = 0\n",
        "\n",
        "#   if h2 >= 0.5:\n",
        "#     tmp_label = 1\n",
        "\n",
        "#   l_list2[i][0] = tmp_label\n",
        "#   # print(l_list[i])\n",
        "#   # print(h2)\n",
        "\n",
        "#   x_list_tmp2[i] = x_list2[i]\n",
        "#   y_list_tmp2[i] = y_list2[i]\n",
        "#   z_list_tmp2[i] = z_list2[i]\n",
        "#   h_list_tmp2[i] = h_list2[i]\n",
        "#   x__list_tmp2[i] = x__list2[i]\n",
        "#   y__list_tmp2[i] = y__list2[i]\n",
        "#   z__list_tmp2[i] = z__list2[i]\n",
        "#   h__list_tmp2[i] = h__list2[i]\n",
        "\n",
        "# print(l_list)\n"
      ],
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6UNJjjPq26l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0,1401):\n",
        "  x_list[0][i]=1\n",
        "  y_list[0][i]=1\n",
        "\n",
        "for i in range(0,601):\n",
        "  x_list2[0][i]=1\n",
        "  y_list2[0][i]=1"
      ],
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVgg5PyfCGAn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "22341ebc-889f-4ac1-f703-9c355628f604"
      },
      "source": [
        "# (1401, 1500)\n",
        "# (601, 1500)\n",
        "# (1401,)\n",
        "# (601,)\n",
        "\n",
        "iteration = 1000\n",
        "l_r = 0.4\n",
        "lambda_ = 100\n",
        "\n",
        "\n",
        "cost_history = np.zeros(iteration)    #about train\n",
        "cost_history2 = np.zeros(iteration)   #about test\n",
        "\n",
        "label_group = np.zeros(shape = (1401, 1))\n",
        "label_group2 = np.zeros(shape = (601, 1))\n",
        "\n",
        "accuracy_history = np.zeros(iteration)\n",
        "accuracy_history2 = np.zeros(iteration)\n",
        "\n",
        "for i in range(0, 1401):\n",
        "  label_group[i][0] = l_list[i][0]\n",
        "\n",
        "for i in range(0, 601):\n",
        "  label_group2[i][0] = l_list2[i][0]\n",
        "\n",
        "for t in range(0, iteration):\n",
        "  h_s_t=np.transpose(h_list)\n",
        "  z_s_t=np.transpose(z_list[1:,:])\n",
        "  y_s_t=np.transpose(y_list[1:,:])\n",
        "  x_s_t=np.transpose(x_list[1:,:])\n",
        "  t_s_t=np.transpose(train_x[1:,:])\n",
        "\n",
        "  # h_s_t=h_list[:,:,:].reshape((1401, 1))\n",
        "  # z_s_t=z_list[:,1:,:].reshape((1401, 7))\n",
        "  # y_s_t=y_list[:,1:,:].reshape((1401, 49))\n",
        "  # x_s_t=x_list[:,1:,:].reshape((1401, 349))\n",
        "  # t_s_t=np.transpose(train_x[:,:])\n",
        "  # print(t_s_t.shape)\n",
        "\n",
        "  w_t=matrix_w[:,1:]\n",
        "  v_t=matrix_v[:,1:]\n",
        "  u_t=matrix_u[:,1:]\n",
        "  t_t=matrix_t[:,1:]\n",
        "  # print(t_t.shape)\n",
        "  \n",
        "  one_z=np.transpose(np.ones((7), dtype=float))\n",
        "  one_y=np.transpose(np.ones((49), dtype=float))\n",
        "  one_x=np.transpose(np.ones((349), dtype=float))\n",
        "\n",
        "  delta5=np.transpose((np.zeros((7), dtype=float)))\n",
        "  delta4=np.zeros((49,7), dtype=float)\n",
        "  delta3=np.zeros((349,49), dtype=float)\n",
        "  delta2=np.zeros((len(train_x)-1,349), dtype=float)\n",
        "\n",
        "  J_sum = 0\n",
        "  theta_sum = 0\n",
        "  Jt_sum=0\n",
        "\n",
        "  number_of_correct = 0\n",
        "  number_of_correct2 = 0\n",
        "\n",
        "  # print(t, end=' ')\n",
        "\n",
        "  parameter_sum = 0\n",
        "\n",
        "  # for q in range(0, vector_size):\n",
        "  #   for w in range(1, 350):\n",
        "  #     parameter_sum = parameter_sum + matrix_t[w][q]**2\n",
        "\n",
        "  # for q in range(0, 350):\n",
        "  #   for w in range(1, 50):\n",
        "  #     parameter_sum = parameter_sum + matrix_u[w][q]**2\n",
        "\n",
        "  # for q in range(0, 50):\n",
        "  #   for w in range(1, 8):\n",
        "  #     parameter_sum = parameter_sum + matrix_v[w][q]**2\n",
        "\n",
        "  # for q in range(0, 8):\n",
        "  #   for w in range(0, 1):\n",
        "  #     parameter_sum = parameter_sum + matrix_w[w][q]**2\n",
        "\n",
        "  # result = 0.0\n",
        "  # for i in range(0, 1401):\n",
        "  #   for k in range(0, 1):\n",
        "  #     result = result + ((-1)*label_group[i][k]*math.log(h_list[i][k][0]) - (1-label_group[i][k])*math.log(1-h_list[i][k][0])) + ((lambda_)/(2 * 543258)) * parameter_sum\n",
        "  # result = result / 1401\n",
        "  # cost_history[t] = result\n",
        "\n",
        "  for i in range(0,len(train_x[0])):\n",
        "    lable=train_l[i]\n",
        "    temp5=h_s_t[i]-train_l[i]\n",
        "    temp4=np.multiply( np.dot( np.transpose(w_t ) , temp5 ) ,np.transpose(np.multiply(z_s_t[i] , one_z - z_s_t[i])))\n",
        "    temp3=np.multiply( np.dot( np.transpose(v_t) , temp4) , np.transpose(np.multiply(y_s_t[i] , one_y - y_s_t[i])))\n",
        "    temp2=np.multiply( np.dot( np.transpose( u_t ) , temp3) , np.transpose(np.multiply(x_s_t[i] , one_x - x_s_t[i])))\n",
        "\n",
        "    delta5=delta5+ np.dot( np.transpose(z_s_t[i][np.newaxis]),temp5[np.newaxis])\n",
        "    delta4=delta4 + np.dot( np.transpose(y_s_t[i][np.newaxis]), temp4[np.newaxis])\n",
        "    delta3=delta3+ np.dot( np.transpose(x_s_t[i][np.newaxis]), temp3[np.newaxis] )\n",
        "    delta2=delta2+ np.dot(np.transpose(t_s_t[i][np.newaxis]), temp2[np.newaxis])\n",
        "\n",
        "    J_sum=J_sum+(-train_l[i]*math.log(h_list[0][i])-(1-train_l[i])*math.log(1-h_list[0][i]))\n",
        "\n",
        "    if i < len(test_x[0]):\n",
        "      Jt_sum=Jt_sum+(-test_l[i]*math.log(h_list2[0][i])-(1-test_l[i])*math.log(1-h_list2[0][i]))\n",
        "\n",
        "  u2=np.multiply(u_t,u_t)\n",
        "  v2=np.multiply(v_t,v_t)\n",
        "  w2=np.multiply(w_t,w_t)\n",
        "  t2=np.multiply(t_t,t_t)\n",
        "  theta_sum=np.sum(t2)+np.sum(u2)+np.sum(v2)+np.sum(w2)\n",
        "\n",
        "  delta5=delta5/len(train_x[0])+lambda_*np.transpose(w_t)/(7+7*49+49*349+349*(len(train_x)-1))\n",
        "  delta4=delta4/len(train_x[0])+lambda_*np.transpose(v_t)/(7+7*49+49*349+349*(len(train_x)-1))\n",
        "  delta3=delta3/len(train_x[0])+lambda_*np.transpose(u_t)/(7+7*49+49*349+349*(len(train_x)-1))\n",
        "  delta2=delta2/len(train_x[0])+lambda_*np.transpose(t_t)/(7+7*49+49*349+349*(len(train_x)-1))\n",
        "\n",
        "  J_sum=J_sum/len(train_x[0])+lambda_*theta_sum/(2*(7+7*49+49*349+349*(len(train_x)-1)))\n",
        "  Jt_sum=Jt_sum/len(test_x[0])+lambda_*theta_sum/(2*(7+7*49+49*349+349*(len(train_x)-1)))\n",
        "\n",
        "\n",
        "  cost_history[t] = J_sum\n",
        "  cost_history2[t] = Jt_sum\n",
        "\n",
        "\n",
        "##gradient descent\n",
        "  result_w = 0\n",
        "  result_v = 0\n",
        "  result_u = 0\n",
        "  result_t = 0\n",
        "\n",
        "  # for i in range(0, 1401):\n",
        "  #   small_delta_2 = np.zeros(shape = (1, 1))\n",
        "  #   for j in range(0, 1):\n",
        "  #     small_delta_2[j][0] = h_list[i][j] - label_group[i][j]\n",
        "  #   # print(h_list[i])\n",
        "  #   # print(label_group[i])\n",
        "  #   # small_delta_2 = h_list[i] - label_group[i] #여기 label_group 다시 봐야함.\n",
        "  #   small_delta_1 = np.dot(np.transpose(matrix_w), small_delta_2)\n",
        "  #   small_delta_0 = np.dot(np.transpose(matrix_v), small_delta_1)*(y_list_tmp[i]*(1 - y_list_tmp[i]))\n",
        "  #   small_delta_ = np.dot(np.transpose(matrix_u), small_delta_0)*(x_list_tmp[i]*(1 - x_list_tmp[i])) #이거..괜찮은가..?\n",
        "\n",
        "  #   # print(small_delta_2)\n",
        "  #   result_w = result_w + np.dot(small_delta_2, np.transpose(z_list[i]))\n",
        "  #   result_v = result_v + np.dot(small_delta_1, np.transpose(y_list[i]))\n",
        "  #   result_u = result_u + np.dot(small_delta_0, np.transpose(x_list[i]))\n",
        "  #   result_t = result_t + np.dot(small_delta_, np.transpose(j_list[i]))\n",
        "\n",
        "  for a in range(0, 349):\n",
        "    for b in range(1,len(train_x)):\n",
        "      if a<1 and b<8:\n",
        "        matrix_w[a][b]=matrix_w[a][b]-l_r*(np.transpose(delta5)[a][b-1])\n",
        "      if a<7 and b<50:\n",
        "        matrix_v[a][b]=matrix_v[a][b]-l_r*(np.transpose(delta4)[a][b-1])\n",
        "      if a<49 and b<350:\n",
        "        matrix_u[a][b]=matrix_u[a][b]-l_r*np.transpose(delta3)[a][b-1]\n",
        "      matrix_t[a][b]=matrix_t[a][b]-l_r*np.transpose(delta2)[a][b-1]\n",
        "\n",
        "  # matrix_w = matrix_w - (result_w / 1401 *l_r)\n",
        "  # matrix_v = matrix_v - (result_v / 1401 *l_r)\n",
        "  # matrix_u = matrix_u - (result_u / 1401 *l_r)\n",
        "  # matrix_t = matrix_t - (result_t / 1401 *l_r)\n",
        "\n",
        "\n",
        "####################################################################\n",
        "\n",
        "  # for k in range(0, 1401):\n",
        "  #   bias   = [1]       #bias 1\n",
        "  #   im_vector = np.concatenate((bias, X_train[k]), axis = None)\n",
        "  #   j_list[k] = im_vector.reshape((vector_size, 1))\n",
        "\n",
        "  #   x_ = np.dot(matrix_t, j_list[k])  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "  #   x__list[k] = x_\n",
        "    \n",
        "  #   x_vector = np.ones((350,1)) #bias\n",
        "  #   for j in range (1, 350):\n",
        "  #     x_vector[j][0] = 1 / (1 + math.exp(-x_[j][0]))\n",
        "  #     # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  #   x   = x_vector.reshape((350, 1))\n",
        "  #   x_list[k] = x\n",
        "\n",
        "\n",
        "\n",
        "  #   y_ = np.dot(matrix_u, x)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "  #   y__list[k] = y_\n",
        "    \n",
        "  #   y_vector = np.ones((50,1)) #bias\n",
        "  #   for j in range (1, 50):\n",
        "  #     y_vector[j][0] = 1 / (1 + math.exp(-y_[j][0]))\n",
        "  #     # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  #   y   = y_vector.reshape((50, 1))\n",
        "  #   y_list[k] = y\n",
        "\n",
        "  #   z_  = np.dot(matrix_v, y)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "  #   z__list[k] = z_\n",
        "  #   z_vector = np.ones((8,1)) #bias\n",
        "  #   for j in range (1, 8):\n",
        "  #     z_vector[j][0] = 1 / (1 + math.exp(-z_[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  #   z   = z_vector.reshape((8, 1))\n",
        "  #   z_list[k] = z\n",
        "    \n",
        "  #   # print(z)\n",
        "\n",
        "  #   h_  = np.dot(matrix_w, z)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "  #   h__list[k] = h_\n",
        "    \n",
        "  #   h_vector = np.ones((1,1)) #bias\n",
        "  #   for j in range (0, 1):\n",
        "  #     h_vector[j][0] = 1 / (1 + math.exp(-h_[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "  #   h   = h_vector.reshape((1, 1))\n",
        "  #   # tmp = h.ravel()\n",
        "  #   # max_value = max(tmp)\n",
        "  #   # print(max_value)\n",
        "  #   # max_index = tmp.index(max_value)\n",
        "\n",
        "  #   h_list11_tmp = h_list.reshape((1401))\n",
        "  #   median_val = statistics.median(h_list11_tmp)\n",
        "\n",
        "  #   h_list[k] = h\n",
        "########################################################\n",
        "  x__list = np.dot(matrix_t,train_x)\n",
        "  x__list2 = np.dot(matrix_t,test_x)\n",
        "\n",
        "  for a in range(1,350):\n",
        "    for b in range(0,len(train_x[0])):\n",
        "      x_list[a][b]=1 / (1 + math.exp(-x__list[a-1][b]))\n",
        "\n",
        "  for a in range(1,350):\n",
        "    for b in range(0,len(test_x[0])):\n",
        "      x_list2[a][b]=1 / (1 + math.exp(-x__list2[a-1][b]))\n",
        "\n",
        "  y__list=np.dot(matrix_u,x_list)\n",
        "  y__list2=np.dot(matrix_u,x_list2)\n",
        "\n",
        "  for a in range(1,50):\n",
        "    for b in range(0,len(train_x[0])):\n",
        "       y_list[a][b]=1 / (1 + math.exp(-y__list[a-1][b]))\n",
        "\n",
        "  for a in range(1,50):\n",
        "    for b in range(0,len(test_x[0])):\n",
        "      y_list2[a][b]=1 / (1 + math.exp(-y__list2[a-1][b]))\n",
        "  \n",
        "  z__list=np.dot(matrix_v,y_list)\n",
        "  z__list2=np.dot(matrix_v,y_list2)\n",
        "\n",
        "  for a in range(1,8):\n",
        "    for b in range(0,len(train_x[0])):\n",
        "      z_list[a][b]=1 / (1 + math.exp(-z__list[a-1][b]))\n",
        "\n",
        "  for a in range(1,8):\n",
        "    for b in range(0,len(test_x[0])):\n",
        "      z_list2[a][b]=1 / (1 + math.exp(-z__list2[a-1][b]))\n",
        "\n",
        "  h__list=np.dot(matrix_w,z_list)\n",
        "  h__list2=np.dot(matrix_w,z_list2)\n",
        "\n",
        "  for b in range(0,len(train_x[0])):\n",
        "    h_list[0][b]=1/(1 + math.exp(-h__list[0][b]))\n",
        "  \n",
        "  for b in range(0,len(test_x[0])):\n",
        "    h_list2[0][b]=1/(1 + math.exp(-h__list2[0][b]))\n",
        "\n",
        "  for i in range(0, 1401) :\n",
        "    tmp_label = 0\n",
        "    if h_list[0][i] >= 0.5 :\n",
        "      tmp_label = 1\n",
        "\n",
        "    l_list[i][0] = tmp_label\n",
        "\n",
        "    # tmp_label = 0\n",
        "\n",
        "    # if h >= median_val:\n",
        "    #     # print(h)\n",
        "    #     tmp_label = 1\n",
        "\n",
        "    # if h >= 0.5:\n",
        "    #   # print(h)\n",
        "    #   tmp_label = 1\n",
        "\n",
        "    # l_list[k][0] = tmp_label\n",
        "    # print(l_list[k])\n",
        "    # print(h)\n",
        "\n",
        "    # x_list_tmp[k] = x_list[k]\n",
        "    # y_list_tmp[k] = y_list[k]\n",
        "    # z_list_tmp[k] = z_list[k]\n",
        "    # h_list_tmp[k] = h_list[k]\n",
        "    # x__list_tmp[k] = x__list[k]\n",
        "    # y__list_tmp[k] = y__list[k]\n",
        "    # z__list_tmp[k] = z__list[k]\n",
        "    # h__list_tmp[k] = h__list[k]\n",
        "\n",
        "    if l_list[i][0] == y_train[i]:\n",
        "      number_of_correct = number_of_correct + 1\n",
        "\n",
        "  accuracy_history[t] = number_of_correct / 1401 * 100\n",
        "  # print(accuracy_history[t])\n",
        "  # accuracy_history[t] = number_of_correct_predictions/total_number_of_predictions*100\n",
        "  # print(accuracy_history[t])\n",
        "\n",
        "############################\n",
        "  #test data!\n",
        "\n",
        "  number_of_correct_predictions2 = 0\n",
        "  \n",
        "  # print(t, end=' ')\n",
        "\n",
        "  # result2 = 0.0\n",
        "  # for i in range(0, 601):\n",
        "  #   for k in range(0, 1):\n",
        "  #     # if h_list_tmp[i][k][0] <= 0:\n",
        "  #     # print(h_list_tmp[i][k][0])\n",
        "  #     result2 = result2 + ((-1)*label_group2[i][k]*math.log(h_list2[i][k][0]) - (1-label_group2[i][k])*math.log(1-h_list2[i][k][0])) + ((lambda_)/(2 * 164995)) * parameter_sum\n",
        "  # result2 = result2 / 601\n",
        "  # cost_history2[t] = result2\n",
        "\n",
        "\n",
        "  # for k in range(0, 601):\n",
        "  # # print(i)\n",
        "  #   bias2   = [1]       #bias 1\n",
        "  #   im_vector2 = np.concatenate((bias2, X_test[k]), axis = None)\n",
        "  #   j_list2[k] = im_vector2.reshape((vector_size, 1))\n",
        "\n",
        "  #   x_2 = np.dot(matrix_t, j_list2[k])  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "  #   x__list2[k] = x_2\n",
        "    \n",
        "  #   x_vector2 = np.ones((350,1)) #bias\n",
        "  #   for j in range (1, 350):\n",
        "  #     x_vector2[j][0] = 1 / (1 + math.exp(-x_2[j][0]))\n",
        "  #     # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  #   x2   = x_vector2.reshape((350, 1))\n",
        "  #   x_list2[k] = x2\n",
        "\n",
        "\n",
        "\n",
        "  #   y_2 = np.dot(matrix_u, x2)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "  #   y__list2[k] = y_2\n",
        "    \n",
        "  #   y_vector2 = np.ones((50,1)) #bias\n",
        "  #   for j in range (1, 50):\n",
        "  #     y_vector2[j][0] = 1 / (1 + math.exp(-y_2[j][0]))\n",
        "  #     # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  #   y2   = y_vector2.reshape((50, 1))\n",
        "  #   y_list2[k] = y2\n",
        "\n",
        "  #   z_2  = np.dot(matrix_v, y2)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "  #   z__list2[k] = z_2\n",
        "  #   z_vector2 = np.ones((8,1)) #bias\n",
        "  #   for j in range (1, 8):\n",
        "  #     z_vector2[j][0] = 1 / (1 + math.exp(-z_2[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  #   z2   = z_vector2.reshape((8, 1))\n",
        "  #   z_list2[k] = z2\n",
        "    \n",
        "  #   # print(z)\n",
        "\n",
        "  #   h_2 = np.dot(matrix_w, z2)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "  #   h__list2[k] = h_2\n",
        "    \n",
        "  #   h_vector2 = np.ones((1,1)) #bias\n",
        "  #   for j in range (0, 1):\n",
        "  #     h_vector2[j][0] = 1 / (1 + math.exp(-h_2[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "  #   h2   = h_vector2.reshape((1, 1))\n",
        "  #   # tmp = h.ravel()\n",
        "  #   # max_value = max(tmp)\n",
        "  #   # print(max_value)\n",
        "  #   # max_index = tmp.index(max_value)\n",
        "\n",
        "  #   h_list22_tmp = h_list2.reshape((601))\n",
        "  #   median_val2 = statistics.median(h_list22_tmp)\n",
        "\n",
        "  #   h_list2[k] = h2\n",
        "\n",
        "  #   # tmp_label = 0\n",
        "\n",
        "  #   # if h2 >= median_val2:\n",
        "  #   #   # print(h2)\n",
        "  #   #   tmp_label = 1\n",
        "\n",
        "  #   if h2 >= 0.5:\n",
        "  #     # print(h2)\n",
        "  #     tmp_label = 1\n",
        "\n",
        "  #   l_list2[k][0] = tmp_label\n",
        "  #   # print(l_list[k])\n",
        "  #   # print(h2)\n",
        "\n",
        "  #   x_list_tmp2[k] = x_list2[k]\n",
        "  #   y_list_tmp2[k] = y_list2[k]\n",
        "  #   z_list_tmp2[k] = z_list2[k]\n",
        "  #   h_list_tmp2[k] = h_list2[k]\n",
        "  #   x__list_tmp2[k] = x__list2[k]\n",
        "  #   y__list_tmp2[k] = y__list2[k]\n",
        "  #   z__list_tmp2[k] = z__list2[k]\n",
        "  #   h__list_tmp2[k] = h__list2[k]\n",
        "\n",
        "  for i in range(0, 601) :\n",
        "    tmp_label = 0\n",
        "    if h_list2[0][i] >= 0.5 :\n",
        "      tmp_label = 1\n",
        "\n",
        "    l_list2[i][0] = tmp_label\n",
        "\n",
        "    if l_list2[i][0] == y_test[i]:\n",
        "      number_of_correct2 = number_of_correct2 + 1\n",
        "\n",
        "  accuracy_history2[t] = number_of_correct2 / 601 * 100\n",
        "  # print(accuracy_history2[t])\n",
        "  print(t,\"train:::acccuracy  \",accuracy_history[t],\"   J   \",J_sum,\"::::: test accuracy\",accuracy_history2[t],\"   J   \",Jt_sum)\n",
        "\n",
        "  # accuracy_history2[t] = number_of_correct_predictions2/total_number_of_predictions2*100\n",
        "  t = t + 1"
      ],
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 train:::acccuracy   49.892933618843685    J    1.2020599005470183 ::::: test accuracy 50.249584026622294    J    1.2011329351519038\n",
            "1 train:::acccuracy   49.892933618843685    J    1.198996758722013 ::::: test accuracy 50.249584026622294    J    1.1982328948094674\n",
            "2 train:::acccuracy   49.892933618843685    J    1.19685349582122 ::::: test accuracy 50.249584026622294    J    1.1962235792833629\n",
            "3 train:::acccuracy   49.892933618843685    J    1.1953319259331971 ::::: test accuracy 50.249584026622294    J    1.1948127176412515\n",
            "4 train:::acccuracy   49.892933618843685    J    1.1942356090242892 ::::: test accuracy 50.249584026622294    J    1.1938083193688767\n",
            "5 train:::acccuracy   49.892933618843685    J    1.1934326996358566 ::::: test accuracy 50.249584026622294    J    1.1930819772155135\n",
            "6 train:::acccuracy   49.892933618843685    J    1.1928334839733943 ::::: test accuracy 50.249584026622294    J    1.1925466909236189\n",
            "7 train:::acccuracy   49.892933618843685    J    1.1923763039257147 ::::: test accuracy 50.249584026622294    J    1.1921429790748708\n",
            "8 train:::acccuracy   49.892933618843685    J    1.1920184769297435 ::::: test accuracy 50.249584026622294    J    1.1918299266694552\n",
            "9 train:::acccuracy   49.892933618843685    J    1.1917303031171127 ::::: test accuracy 50.249584026622294    J    1.1915792822653701\n",
            "10 train:::acccuracy   49.892933618843685    J    1.1914910400305927 ::::: test accuracy 50.249584026622294    J    1.1913714982465462\n",
            "11 train:::acccuracy   49.892933618843685    J    1.1912861630211045 ::::: test accuracy 50.249584026622294    J    1.1911930404038\n",
            "12 train:::acccuracy   49.892933618843685    J    1.191105482408952 ::::: test accuracy 50.249584026622294    J    1.1910345429462375\n",
            "13 train:::acccuracy   49.892933618843685    J    1.190941840228942 ::::: test accuracy 50.249584026622294    J    1.1908895349882744\n",
            "14 train:::acccuracy   49.892933618843685    J    1.1907902034951952 ::::: test accuracy 50.249584026622294    J    1.1907535575759072\n",
            "15 train:::acccuracy   49.892933618843685    J    1.1906470310072141 ::::: test accuracy 50.249584026622294    J    1.1906235497215594\n",
            "16 train:::acccuracy   49.892933618843685    J    1.1905098300063648 ::::: test accuracy 50.249584026622294    J    1.1904974207734094\n",
            "17 train:::acccuracy   50.03568879371877    J    1.1903768451751409 ::::: test accuracy 50.249584026622294    J    1.1903737523478166\n",
            "18 train:::acccuracy   50.17844396859387    J    1.1902468401813522 ::::: test accuracy 50.582362728785355    J    1.1902515905757347\n",
            "19 train:::acccuracy   50.53533190578159    J    1.1901189440826414 ::::: test accuracy 50.74875207986689    J    1.1901303013979694\n",
            "20 train:::acccuracy   50.60670949321913    J    1.189992543261182 ::::: test accuracy 49.417637271214645    J    1.1900094699067767\n",
            "21 train:::acccuracy   50.24982155603141    J    1.1898672053557093 ::::: test accuracy 48.0865224625624    J    1.1898888304612587\n",
            "22 train:::acccuracy   46.96645253390435    J    1.1897426256984718 ::::: test accuracy 48.0865224625624    J    1.1897682182940095\n",
            "23 train:::acccuracy   46.18129907209136    J    1.1896185895898546 ::::: test accuracy 46.089850249584025    J    1.1896475361125671\n",
            "24 train:::acccuracy   47.89436117059244    J    1.1894949457231574 ::::: test accuracy 45.75707154742096    J    1.1895267311484332\n",
            "25 train:::acccuracy   48.3226266952177    J    1.189371587461658 ::::: test accuracy 46.75540765391015    J    1.18940577947161\n",
            "26 train:::acccuracy   48.53675945753034    J    1.1892484396466765 ::::: test accuracy 48.0865224625624    J    1.1892846753454012\n",
            "27 train:::acccuracy   49.321912919343326    J    1.1891254493020884 ::::: test accuracy 47.920133111480865    J    1.189163424066904\n",
            "28 train:::acccuracy   49.46466809421842    J    1.1890025790841343 ::::: test accuracy 48.91846921797005    J    1.1890420372086503\n",
            "29 train:::acccuracy   49.892933618843685    J    1.18887980266556 ::::: test accuracy 49.08485856905158    J    1.1889205295061624\n",
            "30 train:::acccuracy   50.17844396859387    J    1.188757101482808 ::::: test accuracy 48.91846921797005    J    1.1887989168667092\n",
            "31 train:::acccuracy   50.32119914346895    J    1.1886344624436884 ::::: test accuracy 49.25124792013311    J    1.188677215135696\n",
            "32 train:::acccuracy   50.32119914346895    J    1.1885118763119207 ::::: test accuracy 49.08485856905158    J    1.1885554393697764\n",
            "33 train:::acccuracy   50.32119914346895    J    1.1883893365686578 ::::: test accuracy 48.91846921797005    J    1.1884336034441596\n",
            "34 train:::acccuracy   50.32119914346895    J    1.1882668386101605 ::::: test accuracy 48.91846921797005    J    1.1883117198762694\n",
            "35 train:::acccuracy   50.32119914346895    J    1.1881443791823663 ::::: test accuracy 49.25124792013311    J    1.1881897997857478\n",
            "36 train:::acccuracy   50.39257673090649    J    1.1880219559823888 ::::: test accuracy 49.25124792013311    J    1.1880678529370259\n",
            "37 train:::acccuracy   50.39257673090649    J    1.187899567377702 ::::: test accuracy 49.25124792013311    J    1.1879458878287266\n",
            "38 train:::acccuracy   50.32119914346895    J    1.1877772122082046 ::::: test accuracy 49.25124792013311    J    1.1878239118065401\n",
            "39 train:::acccuracy   50.32119914346895    J    1.1876548896467591 ::::: test accuracy 49.25124792013311    J    1.1877019311846353\n",
            "40 train:::acccuracy   50.32119914346895    J    1.187532599100877 ::::: test accuracy 49.25124792013311    J    1.1875799513663725\n",
            "41 train:::acccuracy   50.24982155603141    J    1.1874103401434484 ::::: test accuracy 49.25124792013311    J    1.1874579769588751\n",
            "42 train:::acccuracy   50.24982155603141    J    1.1872881124639245 ::::: test accuracy 49.25124792013311    J    1.1873360118785532\n",
            "43 train:::acccuracy   50.24982155603141    J    1.1871659158339025 ::::: test accuracy 49.25124792013311    J    1.1872140594462641\n",
            "44 train:::acccuracy   50.24982155603141    J    1.1870437500828888 ::::: test accuracy 49.25124792013311    J    1.1870921224718238\n",
            "45 train:::acccuracy   50.24982155603141    J    1.1869216150812012 ::::: test accuracy 49.25124792013311    J    1.1869702033282423\n",
            "46 train:::acccuracy   50.24982155603141    J    1.1867995107279325 ::::: test accuracy 49.25124792013311    J    1.1868483040163247\n",
            "47 train:::acccuracy   50.24982155603141    J    1.1866774369424649 ::::: test accuracy 49.25124792013311    J    1.186726426220546\n",
            "48 train:::acccuracy   50.24982155603141    J    1.1865553936584812 ::::: test accuracy 49.25124792013311    J    1.1866045713570854\n",
            "49 train:::acccuracy   50.24982155603141    J    1.1864333808197598 ::::: test accuracy 49.25124792013311    J    1.186482740614969\n",
            "50 train:::acccuracy   50.24982155603141    J    1.186311398377203 ::::: test accuracy 49.25124792013311    J    1.1863609349911763\n",
            "51 train:::acccuracy   50.24982155603141    J    1.1861894462867353 ::::: test accuracy 49.25124792013311    J    1.186239155320502\n",
            "52 train:::acccuracy   50.24982155603141    J    1.1860675245078394 ::::: test accuracy 49.25124792013311    J    1.1861174023009255\n",
            "53 train:::acccuracy   50.24982155603141    J    1.185945633002513 ::::: test accuracy 49.25124792013311    J    1.18599567651509\n",
            "54 train:::acccuracy   50.24982155603141    J    1.1858237717345241 ::::: test accuracy 49.25124792013311    J    1.1858739784484855\n",
            "55 train:::acccuracy   50.24982155603141    J    1.185701940668925 ::::: test accuracy 49.25124792013311    J    1.1857523085048105\n",
            "56 train:::acccuracy   50.24982155603141    J    1.1855801397716499 ::::: test accuracy 49.25124792013311    J    1.185630667018935\n",
            "57 train:::acccuracy   50.32119914346895    J    1.1854583690092868 ::::: test accuracy 49.25124792013311    J    1.1855090542678346\n",
            "58 train:::acccuracy   50.32119914346895    J    1.1853366283488853 ::::: test accuracy 49.25124792013311    J    1.1853874704798013\n",
            "59 train:::acccuracy   50.32119914346895    J    1.1852149177578277 ::::: test accuracy 49.25124792013311    J    1.1852659158422099\n",
            "60 train:::acccuracy   50.32119914346895    J    1.1850932372037457 ::::: test accuracy 49.25124792013311    J    1.1851443905080357\n",
            "61 train:::acccuracy   50.32119914346895    J    1.184971586654443 ::::: test accuracy 49.25124792013311    J    1.1850228946013646\n",
            "62 train:::acccuracy   50.32119914346895    J    1.1848499660778717 ::::: test accuracy 49.25124792013311    J    1.1849014282219987\n",
            "63 train:::acccuracy   50.32119914346895    J    1.184728375442077 ::::: test accuracy 49.25124792013311    J    1.184779991449359\n",
            "64 train:::acccuracy   50.32119914346895    J    1.1846068147151871 ::::: test accuracy 49.25124792013311    J    1.184658584345735\n",
            "65 train:::acccuracy   50.32119914346895    J    1.1844852838654005 ::::: test accuracy 49.25124792013311    J    1.1845372069590392\n",
            "66 train:::acccuracy   50.32119914346895    J    1.184363782860963 ::::: test accuracy 49.25124792013311    J    1.1844158593251142\n",
            "67 train:::acccuracy   50.32119914346895    J    1.1842423116701657 ::::: test accuracy 49.25124792013311    J    1.184294541469665\n",
            "68 train:::acccuracy   50.32119914346895    J    1.1841208702613362 ::::: test accuracy 49.25124792013311    J    1.1841732534098897\n",
            "69 train:::acccuracy   50.32119914346895    J    1.1839994586028406 ::::: test accuracy 49.25124792013311    J    1.1840519951558435\n",
            "70 train:::acccuracy   50.32119914346895    J    1.1838780766630665 ::::: test accuracy 49.25124792013311    J    1.183930766711594\n",
            "71 train:::acccuracy   50.32119914346895    J    1.1837567244104341 ::::: test accuracy 49.25124792013311    J    1.183809568076176\n",
            "72 train:::acccuracy   50.32119914346895    J    1.1836354018133903 ::::: test accuracy 49.25124792013311    J    1.1836883992444065\n",
            "73 train:::acccuracy   50.32119914346895    J    1.1835141088403978 ::::: test accuracy 49.25124792013311    J    1.1835672602075655\n",
            "74 train:::acccuracy   50.32119914346895    J    1.183392845459944 ::::: test accuracy 49.25124792013311    J    1.18344615095396\n",
            "75 train:::acccuracy   50.32119914346895    J    1.183271611640539 ::::: test accuracy 49.25124792013311    J    1.1833250714694117\n",
            "76 train:::acccuracy   50.32119914346895    J    1.1831504073507053 ::::: test accuracy 49.417637271214645    J    1.1832040217376512\n",
            "77 train:::acccuracy   50.32119914346895    J    1.183029232558988 ::::: test accuracy 49.417637271214645    J    1.1830830017406602\n",
            "78 train:::acccuracy   50.32119914346895    J    1.1829080872339475 ::::: test accuracy 49.417637271214645    J    1.1829620114589514\n",
            "79 train:::acccuracy   50.32119914346895    J    1.1827869713441577 ::::: test accuracy 49.417637271214645    J    1.182841050871812\n",
            "80 train:::acccuracy   50.32119914346895    J    1.1826658848582077 ::::: test accuracy 49.417637271214645    J    1.1827201199574944\n",
            "81 train:::acccuracy   50.32119914346895    J    1.1825448277447002 ::::: test accuracy 49.417637271214645    J    1.1825992186933891\n",
            "82 train:::acccuracy   50.32119914346895    J    1.1824237999722507 ::::: test accuracy 49.417637271214645    J    1.1824783470561695\n",
            "83 train:::acccuracy   50.32119914346895    J    1.1823028015094859 ::::: test accuracy 49.417637271214645    J    1.1823575050218973\n",
            "84 train:::acccuracy   50.32119914346895    J    1.1821818323250413 ::::: test accuracy 49.417637271214645    J    1.1822366925661312\n",
            "85 train:::acccuracy   50.32119914346895    J    1.182060892387565 ::::: test accuracy 49.417637271214645    J    1.1821159096640048\n",
            "86 train:::acccuracy   50.32119914346895    J    1.1819399816657141 ::::: test accuracy 49.417637271214645    J    1.181995156290302\n",
            "87 train:::acccuracy   50.32119914346895    J    1.1818191001281464 ::::: test accuracy 49.417637271214645    J    1.181874432419504\n",
            "88 train:::acccuracy   50.32119914346895    J    1.1816982477435363 ::::: test accuracy 49.417637271214645    J    1.1817537380258512\n",
            "89 train:::acccuracy   50.32119914346895    J    1.1815774244805572 ::::: test accuracy 49.417637271214645    J    1.1816330730833733\n",
            "90 train:::acccuracy   50.32119914346895    J    1.181456630307889 ::::: test accuracy 49.417637271214645    J    1.1815124375659272\n",
            "91 train:::acccuracy   50.32119914346895    J    1.1813358651942192 ::::: test accuracy 49.417637271214645    J    1.1813918314472307\n",
            "92 train:::acccuracy   50.32119914346895    J    1.1812151291082307 ::::: test accuracy 49.417637271214645    J    1.1812712547008737\n",
            "93 train:::acccuracy   50.39257673090649    J    1.1810944220186173 ::::: test accuracy 49.417637271214645    J    1.1811507073003518\n",
            "94 train:::acccuracy   50.39257673090649    J    1.180973743894067 ::::: test accuracy 49.417637271214645    J    1.181030189219073\n",
            "95 train:::acccuracy   50.39257673090649    J    1.180853094703271 ::::: test accuracy 49.417637271214645    J    1.180909700430378\n",
            "96 train:::acccuracy   50.39257673090649    J    1.1807324744149232 ::::: test accuracy 49.417637271214645    J    1.1807892409075467\n",
            "97 train:::acccuracy   50.39257673090649    J    1.18061188299771 ::::: test accuracy 49.417637271214645    J    1.1806688106238103\n",
            "98 train:::acccuracy   50.39257673090649    J    1.1804913204203176 ::::: test accuracy 49.417637271214645    J    1.180548409552357\n",
            "99 train:::acccuracy   50.39257673090649    J    1.180370786651432 ::::: test accuracy 49.417637271214645    J    1.1804280376663427\n",
            "100 train:::acccuracy   50.39257673090649    J    1.1802502816597276 ::::: test accuracy 49.417637271214645    J    1.1803076949388933\n",
            "101 train:::acccuracy   50.39257673090649    J    1.180129805413884 ::::: test accuracy 49.417637271214645    J    1.1801873813431072\n",
            "102 train:::acccuracy   50.39257673090649    J    1.1800093578825621 ::::: test accuracy 49.417637271214645    J    1.1800670968520603\n",
            "103 train:::acccuracy   50.39257673090649    J    1.179888939034429 ::::: test accuracy 49.417637271214645    J    1.1799468414388152\n",
            "104 train:::acccuracy   50.39257673090649    J    1.1797685488381346 ::::: test accuracy 49.417637271214645    J    1.1798266150764105\n",
            "105 train:::acccuracy   50.39257673090649    J    1.1796481872623195 ::::: test accuracy 49.417637271214645    J    1.1797064177378744\n",
            "106 train:::acccuracy   50.46395431834404    J    1.179527854275623 ::::: test accuracy 49.417637271214645    J    1.1795862493962193\n",
            "107 train:::acccuracy   50.46395431834404    J    1.1794075498466623 ::::: test accuracy 49.417637271214645    J    1.1794661100244443\n",
            "108 train:::acccuracy   50.46395431834404    J    1.1792872739440536 ::::: test accuracy 49.417637271214645    J    1.1793459995955395\n",
            "109 train:::acccuracy   50.46395431834404    J    1.179167026536395 ::::: test accuracy 49.417637271214645    J    1.179225918082483\n",
            "110 train:::acccuracy   50.46395431834404    J    1.17904680759227 ::::: test accuracy 49.417637271214645    J    1.1791058654582394\n",
            "111 train:::acccuracy   50.46395431834404    J    1.17892661708025 ::::: test accuracy 49.417637271214645    J    1.1789858416957626\n",
            "112 train:::acccuracy   50.46395431834404    J    1.1788064549688924 ::::: test accuracy 49.417637271214645    J    1.1788658467679975\n",
            "113 train:::acccuracy   50.46395431834404    J    1.1786863212267344 ::::: test accuracy 49.417637271214645    J    1.1787458806478774\n",
            "114 train:::acccuracy   50.46395431834404    J    1.1785662158223014 ::::: test accuracy 49.417637271214645    J    1.1786259433083226\n",
            "115 train:::acccuracy   50.46395431834404    J    1.1784461387240914 ::::: test accuracy 49.58402662229617    J    1.1785060347222411\n",
            "116 train:::acccuracy   50.46395431834404    J    1.1783260899005934 ::::: test accuracy 49.58402662229617    J    1.178386154862535\n",
            "117 train:::acccuracy   50.46395431834404    J    1.1782060693202725 ::::: test accuracy 49.58402662229617    J    1.1782663037020862\n",
            "118 train:::acccuracy   50.53533190578159    J    1.178086076951572 ::::: test accuracy 49.58402662229617    J    1.1781464812137652\n",
            "119 train:::acccuracy   50.53533190578159    J    1.1779661127629124 ::::: test accuracy 49.58402662229617    J    1.1780266873704317\n",
            "120 train:::acccuracy   50.53533190578159    J    1.1778461767226955 ::::: test accuracy 49.58402662229617    J    1.17790692214493\n",
            "121 train:::acccuracy   50.53533190578159    J    1.1777262687992966 ::::: test accuracy 49.58402662229617    J    1.1777871855100883\n",
            "122 train:::acccuracy   50.53533190578159    J    1.1776063889610644 ::::: test accuracy 49.58402662229617    J    1.1776674774387221\n",
            "123 train:::acccuracy   50.53533190578159    J    1.1774865371763266 ::::: test accuracy 49.58402662229617    J    1.1775477979036268\n",
            "124 train:::acccuracy   50.53533190578159    J    1.1773667134133805 ::::: test accuracy 49.58402662229617    J    1.177428146877586\n",
            "125 train:::acccuracy   50.53533190578159    J    1.1772469176404985 ::::: test accuracy 49.58402662229617    J    1.1773085243333603\n",
            "126 train:::acccuracy   50.53533190578159    J    1.1771271498259235 ::::: test accuracy 49.58402662229617    J    1.177188930243696\n",
            "127 train:::acccuracy   50.53533190578159    J    1.177007409937868 ::::: test accuracy 49.58402662229617    J    1.1770693645813188\n",
            "128 train:::acccuracy   50.53533190578159    J    1.1768876979445178 ::::: test accuracy 49.58402662229617    J    1.1769498273189387\n",
            "129 train:::acccuracy   50.53533190578159    J    1.1767680138140224 ::::: test accuracy 49.58402662229617    J    1.1768303184292372\n",
            "130 train:::acccuracy   50.53533190578159    J    1.1766483575145026 ::::: test accuracy 49.750415973377706    J    1.1767108378848832\n",
            "131 train:::acccuracy   50.53533190578159    J    1.1765287290140465 ::::: test accuracy 49.750415973377706    J    1.1765913856585182\n",
            "132 train:::acccuracy   50.53533190578159    J    1.1764091282807057 ::::: test accuracy 49.750415973377706    J    1.176471961722763\n",
            "133 train:::acccuracy   50.53533190578159    J    1.1762895552824961 ::::: test accuracy 49.750415973377706    J    1.1763525660502174\n",
            "134 train:::acccuracy   50.53533190578159    J    1.1761700099874037 ::::: test accuracy 49.750415973377706    J    1.1762331986134527\n",
            "135 train:::acccuracy   50.53533190578159    J    1.1760504923633717 ::::: test accuracy 49.750415973377706    J    1.1761138593850178\n",
            "136 train:::acccuracy   50.53533190578159    J    1.1759310023783043 ::::: test accuracy 49.750415973377706    J    1.1759945483374348\n",
            "137 train:::acccuracy   50.53533190578159    J    1.1758115400000717 ::::: test accuracy 49.750415973377706    J    1.1758752654432025\n",
            "138 train:::acccuracy   50.53533190578159    J    1.1756921051965024 ::::: test accuracy 49.750415973377706    J    1.1757560106747875\n",
            "139 train:::acccuracy   50.53533190578159    J    1.1755726979353833 ::::: test accuracy 49.750415973377706    J    1.1756367840046333\n",
            "140 train:::acccuracy   50.53533190578159    J    1.1754533181844593 ::::: test accuracy 49.750415973377706    J    1.1755175854051496\n",
            "141 train:::acccuracy   50.53533190578159    J    1.1753339659114364 ::::: test accuracy 49.750415973377706    J    1.175398414848722\n",
            "142 train:::acccuracy   50.53533190578159    J    1.175214641083969 ::::: test accuracy 49.750415973377706    J    1.1752792723077015\n",
            "143 train:::acccuracy   50.53533190578159    J    1.1750953436696754 ::::: test accuracy 49.750415973377706    J    1.1751601577544093\n",
            "144 train:::acccuracy   50.53533190578159    J    1.1749760736361217 ::::: test accuracy 49.750415973377706    J    1.1750410711611368\n",
            "145 train:::acccuracy   50.53533190578159    J    1.1748568309508332 ::::: test accuracy 49.750415973377706    J    1.1749220125001387\n",
            "146 train:::acccuracy   50.53533190578159    J    1.1747376155812834 ::::: test accuracy 49.750415973377706    J    1.1748029817436394\n",
            "147 train:::acccuracy   50.53533190578159    J    1.1746184274948948 ::::: test accuracy 49.750415973377706    J    1.1746839788638277\n",
            "148 train:::acccuracy   50.53533190578159    J    1.174499266659053 ::::: test accuracy 49.750415973377706    J    1.1745650038328561\n",
            "149 train:::acccuracy   50.53533190578159    J    1.174380133041074 ::::: test accuracy 49.750415973377706    J    1.1744460566228445\n",
            "150 train:::acccuracy   50.53533190578159    J    1.1742610266082356 ::::: test accuracy 49.750415973377706    J    1.1743271372058726\n",
            "151 train:::acccuracy   50.53533190578159    J    1.174141947327763 ::::: test accuracy 49.750415973377706    J    1.1742082455539826\n",
            "152 train:::acccuracy   50.53533190578159    J    1.1740228951668181 ::::: test accuracy 49.750415973377706    J    1.1740893816391833\n",
            "153 train:::acccuracy   50.53533190578159    J    1.1739038700925184 ::::: test accuracy 49.750415973377706    J    1.173970545433438\n",
            "154 train:::acccuracy   50.53533190578159    J    1.1737848720719197 ::::: test accuracy 49.750415973377706    J    1.173851736908672\n",
            "155 train:::acccuracy   50.60670949321913    J    1.1736659010720267 ::::: test accuracy 49.750415973377706    J    1.173732956036774\n",
            "156 train:::acccuracy   50.60670949321913    J    1.1735469570597803 ::::: test accuracy 49.750415973377706    J    1.1736142027895817\n",
            "157 train:::acccuracy   50.53533190578159    J    1.1734280400020645 ::::: test accuracy 49.750415973377706    J    1.1734954771389021\n",
            "158 train:::acccuracy   50.46395431834404    J    1.1733091498657093 ::::: test accuracy 49.750415973377706    J    1.1733767790564889\n",
            "159 train:::acccuracy   50.46395431834404    J    1.1731902866174733 ::::: test accuracy 49.750415973377706    J    1.173258108514056\n",
            "160 train:::acccuracy   50.46395431834404    J    1.1730714502240653 ::::: test accuracy 49.750415973377706    J    1.1731394654832747\n",
            "161 train:::acccuracy   50.46395431834404    J    1.1729526406521211 ::::: test accuracy 49.750415973377706    J    1.1730208499357646\n",
            "162 train:::acccuracy   50.53533190578159    J    1.1728338578682183 ::::: test accuracy 49.750415973377706    J    1.1729022618431018\n",
            "163 train:::acccuracy   50.53533190578159    J    1.17271510183887 ::::: test accuracy 49.750415973377706    J    1.1727837011768165\n",
            "164 train:::acccuracy   50.53533190578159    J    1.17259637253052 ::::: test accuracy 49.750415973377706    J    1.1726651679083875\n",
            "165 train:::acccuracy   50.53533190578159    J    1.1724776699095467 ::::: test accuracy 49.750415973377706    J    1.172546662009247\n",
            "166 train:::acccuracy   50.60670949321913    J    1.1723589939422634 ::::: test accuracy 49.750415973377706    J    1.172428183450773\n",
            "167 train:::acccuracy   50.60670949321913    J    1.1722403445949094 ::::: test accuracy 49.750415973377706    J    1.1723097322042992\n",
            "168 train:::acccuracy   50.60670949321913    J    1.172121721833658 ::::: test accuracy 49.750415973377706    J    1.1721913082411006\n",
            "169 train:::acccuracy   50.60670949321913    J    1.1720031256246082 ::::: test accuracy 49.750415973377706    J    1.172072911532404\n",
            "170 train:::acccuracy   50.60670949321913    J    1.1718845559337883 ::::: test accuracy 49.750415973377706    J    1.171954542049381\n",
            "171 train:::acccuracy   50.60670949321913    J    1.1717660127271536 ::::: test accuracy 49.750415973377706    J    1.1718361997631495\n",
            "172 train:::acccuracy   50.60670949321913    J    1.1716474959705843 ::::: test accuracy 49.750415973377706    J    1.1717178846447713\n",
            "173 train:::acccuracy   50.60670949321913    J    1.1715290056298837 ::::: test accuracy 49.750415973377706    J    1.1715995966652515\n",
            "174 train:::acccuracy   50.678087080656674    J    1.1714105416707827 ::::: test accuracy 49.750415973377706    J    1.1714813357955407\n",
            "175 train:::acccuracy   50.678087080656674    J    1.1712921040589312 ::::: test accuracy 49.750415973377706    J    1.1713631020065287\n",
            "176 train:::acccuracy   50.678087080656674    J    1.1711736927598995 ::::: test accuracy 49.750415973377706    J    1.1712448952690482\n",
            "177 train:::acccuracy   50.678087080656674    J    1.1710553077391825 ::::: test accuracy 49.750415973377706    J    1.1711267155538727\n",
            "178 train:::acccuracy   50.678087080656674    J    1.1709369489621908 ::::: test accuracy 49.750415973377706    J    1.1710085628317122\n",
            "179 train:::acccuracy   50.678087080656674    J    1.170818616394249 ::::: test accuracy 49.750415973377706    J    1.1708904370732185\n",
            "180 train:::acccuracy   50.678087080656674    J    1.1707003100006064 ::::: test accuracy 49.750415973377706    J    1.170772338248978\n",
            "181 train:::acccuracy   50.678087080656674    J    1.1705820297464218 ::::: test accuracy 49.750415973377706    J    1.1706542663295172\n",
            "182 train:::acccuracy   50.678087080656674    J    1.1704637755967726 ::::: test accuracy 49.58402662229617    J    1.1705362212852957\n",
            "183 train:::acccuracy   50.678087080656674    J    1.1703455475166484 ::::: test accuracy 49.58402662229617    J    1.170418203086708\n",
            "184 train:::acccuracy   50.678087080656674    J    1.1702273454709509 ::::: test accuracy 49.58402662229617    J    1.170300211704084\n",
            "185 train:::acccuracy   50.678087080656674    J    1.1701091694244887 ::::: test accuracy 49.58402662229617    J    1.1701822471076837\n",
            "186 train:::acccuracy   50.678087080656674    J    1.169991019341987 ::::: test accuracy 49.58402662229617    J    1.1700643092677032\n",
            "187 train:::acccuracy   50.678087080656674    J    1.1698728951880772 ::::: test accuracy 49.58402662229617    J    1.1699463981542677\n",
            "188 train:::acccuracy   50.678087080656674    J    1.1697547969272966 ::::: test accuracy 49.417637271214645    J    1.1698285137374311\n",
            "189 train:::acccuracy   50.678087080656674    J    1.1696367245240895 ::::: test accuracy 49.417637271214645    J    1.1697106559871782\n",
            "190 train:::acccuracy   50.678087080656674    J    1.1695186779428095 ::::: test accuracy 49.417637271214645    J    1.1695928248734226\n",
            "191 train:::acccuracy   50.678087080656674    J    1.1694006571477074 ::::: test accuracy 49.417637271214645    J    1.169475020366002\n",
            "192 train:::acccuracy   50.678087080656674    J    1.1692826621029422 ::::: test accuracy 49.417637271214645    J    1.1693572424346859\n",
            "193 train:::acccuracy   50.678087080656674    J    1.1691646927725712 ::::: test accuracy 49.417637271214645    J    1.1692394910491632\n",
            "194 train:::acccuracy   50.678087080656674    J    1.1690467491205545 ::::: test accuracy 49.417637271214645    J    1.1691217661790516\n",
            "195 train:::acccuracy   50.678087080656674    J    1.16892883111075 ::::: test accuracy 49.417637271214645    J    1.1690040677938889\n",
            "196 train:::acccuracy   50.678087080656674    J    1.1688109387069148 ::::: test accuracy 49.417637271214645    J    1.1688863958631375\n",
            "197 train:::acccuracy   50.678087080656674    J    1.1686930718727022 ::::: test accuracy 49.417637271214645    J    1.1687687503561812\n",
            "198 train:::acccuracy   50.74946466809421    J    1.1685752305716606 ::::: test accuracy 49.417637271214645    J    1.168651131242323\n",
            "199 train:::acccuracy   50.74946466809421    J    1.168457414767235 ::::: test accuracy 49.417637271214645    J    1.1685335384907864\n",
            "200 train:::acccuracy   50.74946466809421    J    1.168339624422758 ::::: test accuracy 49.417637271214645    J    1.1684159720707123\n",
            "201 train:::acccuracy   50.74946466809421    J    1.1682218595014602 ::::: test accuracy 49.417637271214645    J    1.1682984319511605\n",
            "202 train:::acccuracy   50.74946466809421    J    1.1681041199664595 ::::: test accuracy 49.417637271214645    J    1.1681809181011054\n",
            "203 train:::acccuracy   50.74946466809421    J    1.1679864057807632 ::::: test accuracy 49.417637271214645    J    1.1680634304894375\n",
            "204 train:::acccuracy   50.74946466809421    J    1.1678687169072686 ::::: test accuracy 49.417637271214645    J    1.167945969084962\n",
            "205 train:::acccuracy   50.74946466809421    J    1.1677510533087587 ::::: test accuracy 49.417637271214645    J    1.1678285338563978\n",
            "206 train:::acccuracy   50.74946466809421    J    1.167633414947899 ::::: test accuracy 49.417637271214645    J    1.167711124772376\n",
            "207 train:::acccuracy   50.74946466809421    J    1.167515801787246 ::::: test accuracy 49.417637271214645    J    1.167593741801435\n",
            "208 train:::acccuracy   50.74946466809421    J    1.167398213789229 ::::: test accuracy 49.417637271214645    J    1.1674763849120309\n",
            "209 train:::acccuracy   50.74946466809421    J    1.1672806509161686 ::::: test accuracy 49.417637271214645    J    1.1673590540725218\n",
            "210 train:::acccuracy   50.74946466809421    J    1.1671631131302598 ::::: test accuracy 49.417637271214645    J    1.1672417492511769\n",
            "211 train:::acccuracy   50.74946466809421    J    1.1670456003935827 ::::: test accuracy 49.417637271214645    J    1.1671244704161754\n",
            "212 train:::acccuracy   50.74946466809421    J    1.1669281126680855 ::::: test accuracy 49.417637271214645    J    1.1670072175355948\n",
            "213 train:::acccuracy   50.74946466809421    J    1.1668106499155988 ::::: test accuracy 49.417637271214645    J    1.1668899905774244\n",
            "214 train:::acccuracy   50.74946466809421    J    1.1666932120978277 ::::: test accuracy 49.417637271214645    J    1.1667727895095523\n",
            "215 train:::acccuracy   50.60670949321913    J    1.1665757991763486 ::::: test accuracy 49.417637271214645    J    1.1666556142997742\n",
            "216 train:::acccuracy   50.60670949321913    J    1.1664584111126135 ::::: test accuracy 49.417637271214645    J    1.1665384649157828\n",
            "217 train:::acccuracy   50.60670949321913    J    1.166341047867941 ::::: test accuracy 49.417637271214645    J    1.166421341325174\n",
            "218 train:::acccuracy   50.60670949321913    J    1.1662237094035228 ::::: test accuracy 49.417637271214645    J    1.1663042434954403\n",
            "219 train:::acccuracy   50.60670949321913    J    1.166106395680414 ::::: test accuracy 49.417637271214645    J    1.166187171393974\n",
            "220 train:::acccuracy   50.60670949321913    J    1.1659891066595423 ::::: test accuracy 49.417637271214645    J    1.166070124988066\n",
            "221 train:::acccuracy   50.60670949321913    J    1.165871842301695 ::::: test accuracy 49.58402662229617    J    1.165953104244899\n",
            "222 train:::acccuracy   50.60670949321913    J    1.1657546025675234 ::::: test accuracy 49.58402662229617    J    1.1658361091315548\n",
            "223 train:::acccuracy   50.60670949321913    J    1.1656373874175494 ::::: test accuracy 49.58402662229617    J    1.1657191396150055\n",
            "224 train:::acccuracy   50.60670949321913    J    1.1655201968121407 ::::: test accuracy 49.58402662229617    J    1.165602195662116\n",
            "225 train:::acccuracy   50.74946466809421    J    1.165403030711536 ::::: test accuracy 49.58402662229617    J    1.1654852772396453\n",
            "226 train:::acccuracy   50.74946466809421    J    1.1652858890758333 ::::: test accuracy 49.58402662229617    J    1.1653683843142386\n",
            "227 train:::acccuracy   50.74946466809421    J    1.1651687718649761 ::::: test accuracy 49.58402662229617    J    1.1652515168524304\n",
            "228 train:::acccuracy   50.74946466809421    J    1.1650516790387728 ::::: test accuracy 49.58402662229617    J    1.1651346748206466\n",
            "229 train:::acccuracy   50.74946466809421    J    1.1649346105568765 ::::: test accuracy 49.58402662229617    J    1.1650178581851955\n",
            "230 train:::acccuracy   50.74946466809421    J    1.1648175663788027 ::::: test accuracy 49.58402662229617    J    1.1649010669122715\n",
            "231 train:::acccuracy   50.74946466809421    J    1.1647005464639084 ::::: test accuracy 49.58402662229617    J    1.1647843009679546\n",
            "232 train:::acccuracy   50.74946466809421    J    1.1645835507714055 ::::: test accuracy 49.58402662229617    J    1.1646675603182066\n",
            "233 train:::acccuracy   50.74946466809421    J    1.1644665792603486 ::::: test accuracy 49.58402662229617    J    1.164550844928869\n",
            "234 train:::acccuracy   50.74946466809421    J    1.164349631889639 ::::: test accuracy 49.58402662229617    J    1.1644341547656663\n",
            "235 train:::acccuracy   50.74946466809421    J    1.1642327086180262 ::::: test accuracy 49.58402662229617    J    1.1643174897942015\n",
            "236 train:::acccuracy   50.74946466809421    J    1.1641158094040958 ::::: test accuracy 49.58402662229617    J    1.1642008499799534\n",
            "237 train:::acccuracy   50.74946466809421    J    1.1639989342062802 ::::: test accuracy 49.58402662229617    J    1.1640842352882803\n",
            "238 train:::acccuracy   50.74946466809421    J    1.1638820829828491 ::::: test accuracy 49.58402662229617    J    1.163967645684413\n",
            "239 train:::acccuracy   50.74946466809421    J    1.1637652556919083 ::::: test accuracy 49.58402662229617    J    1.1638510811334561\n",
            "240 train:::acccuracy   50.74946466809421    J    1.163648452291404 ::::: test accuracy 49.58402662229617    J    1.163734541600391\n",
            "241 train:::acccuracy   50.74946466809421    J    1.1635316727391125 ::::: test accuracy 49.58402662229617    J    1.1636180270500656\n",
            "242 train:::acccuracy   50.74946466809421    J    1.1634149169926458 ::::: test accuracy 49.58402662229617    J    1.163501537447198\n",
            "243 train:::acccuracy   50.74946466809421    J    1.163298185009446 ::::: test accuracy 49.58402662229617    J    1.1633850727563788\n",
            "244 train:::acccuracy   50.74946466809421    J    1.1631814767467863 ::::: test accuracy 49.750415973377706    J    1.1632686329420632\n",
            "245 train:::acccuracy   50.74946466809421    J    1.163064792161767 ::::: test accuracy 49.750415973377706    J    1.1631522179685707\n",
            "246 train:::acccuracy   50.82084225553176    J    1.1629481312113144 ::::: test accuracy 49.750415973377706    J    1.1630358278000892\n",
            "247 train:::acccuracy   50.82084225553176    J    1.1628314938521787 ::::: test accuracy 49.750415973377706    J    1.1629194624006685\n",
            "248 train:::acccuracy   50.82084225553176    J    1.1627148800409357 ::::: test accuracy 49.91680532445923    J    1.1628031217342172\n",
            "249 train:::acccuracy   50.82084225553176    J    1.16259828973398 ::::: test accuracy 49.91680532445923    J    1.1626868057645088\n",
            "250 train:::acccuracy   50.82084225553176    J    1.162481722887526 ::::: test accuracy 49.91680532445923    J    1.1625705144551728\n",
            "251 train:::acccuracy   50.82084225553176    J    1.1623651794576055 ::::: test accuracy 49.91680532445923    J    1.1624542477696993\n",
            "252 train:::acccuracy   50.82084225553176    J    1.1622486594000665 ::::: test accuracy 49.91680532445923    J    1.1623380056714279\n",
            "253 train:::acccuracy   50.82084225553176    J    1.1621321626705718 ::::: test accuracy 49.91680532445923    J    1.1622217881235644\n",
            "254 train:::acccuracy   50.82084225553176    J    1.162015689224597 ::::: test accuracy 49.91680532445923    J    1.162105595089156\n",
            "255 train:::acccuracy   50.82084225553176    J    1.1618992390174259 ::::: test accuracy 49.91680532445923    J    1.1619894265311093\n",
            "256 train:::acccuracy   50.82084225553176    J    1.1617828120041518 ::::: test accuracy 49.91680532445923    J    1.1618732824121787\n",
            "257 train:::acccuracy   50.96359743040685    J    1.1616664081396761 ::::: test accuracy 49.91680532445923    J    1.161757162694967\n",
            "258 train:::acccuracy   51.034975017844395    J    1.1615500273787023 ::::: test accuracy 49.91680532445923    J    1.1616410673419277\n",
            "259 train:::acccuracy   51.034975017844395    J    1.1614336696757424 ::::: test accuracy 49.91680532445923    J    1.1615249963153553\n",
            "260 train:::acccuracy   51.034975017844395    J    1.1613173349851058 ::::: test accuracy 49.91680532445923    J    1.1614089495773934\n",
            "261 train:::acccuracy   51.034975017844395    J    1.1612010232608998 ::::: test accuracy 49.91680532445923    J    1.1612929270900234\n",
            "262 train:::acccuracy   51.034975017844395    J    1.1610847344570314 ::::: test accuracy 49.91680532445923    J    1.161176928815073\n",
            "263 train:::acccuracy   51.034975017844395    J    1.1609684685272037 ::::: test accuracy 49.91680532445923    J    1.1610609547142077\n",
            "264 train:::acccuracy   51.034975017844395    J    1.1608522254249125 ::::: test accuracy 49.91680532445923    J    1.1609450047489305\n",
            "265 train:::acccuracy   51.034975017844395    J    1.1607360051034459 ::::: test accuracy 49.91680532445923    J    1.1608290788805822\n",
            "266 train:::acccuracy   51.10635260528195    J    1.1606198075158782 ::::: test accuracy 49.750415973377706    J    1.1607131770703363\n",
            "267 train:::acccuracy   51.17773019271949    J    1.1605036326150762 ::::: test accuracy 49.750415973377706    J    1.1605972992792062\n",
            "268 train:::acccuracy   51.17773019271949    J    1.1603874803536895 ::::: test accuracy 49.750415973377706    J    1.1604814454680292\n",
            "269 train:::acccuracy   51.17773019271949    J    1.160271350684154 ::::: test accuracy 49.750415973377706    J    1.160365615597478\n",
            "270 train:::acccuracy   51.17773019271949    J    1.160155243558684 ::::: test accuracy 49.750415973377706    J    1.1602498096280494\n",
            "271 train:::acccuracy   51.24910778015703    J    1.160039158929274 ::::: test accuracy 49.750415973377706    J    1.1601340275200749\n",
            "272 train:::acccuracy   51.24910778015703    J    1.1599230967476974 ::::: test accuracy 49.750415973377706    J    1.1600182692337035\n",
            "273 train:::acccuracy   51.24910778015703    J    1.1598070569655017 ::::: test accuracy 49.750415973377706    J    1.1599025347289094\n",
            "274 train:::acccuracy   51.24910778015703    J    1.1596910395340092 ::::: test accuracy 49.750415973377706    J    1.1597868239654936\n",
            "275 train:::acccuracy   51.24910778015703    J    1.159575044404311 ::::: test accuracy 49.91680532445923    J    1.159671136903069\n",
            "276 train:::acccuracy   51.39186295503212    J    1.1594590715272686 ::::: test accuracy 49.91680532445923    J    1.1595554735010767\n",
            "277 train:::acccuracy   51.39186295503212    J    1.1593431208535114 ::::: test accuracy 49.91680532445923    J    1.1594398337187668\n",
            "278 train:::acccuracy   51.39186295503212    J    1.1592271923334314 ::::: test accuracy 50.08319467554077    J    1.1593242175152074\n",
            "279 train:::acccuracy   51.39186295503212    J    1.1591112859171857 ::::: test accuracy 50.08319467554077    J    1.1592086248492812\n",
            "280 train:::acccuracy   51.39186295503212    J    1.1589954015546886 ::::: test accuracy 50.08319467554077    J    1.1590930556796788\n",
            "281 train:::acccuracy   51.39186295503212    J    1.1588795391956164 ::::: test accuracy 50.249584026622294    J    1.1589775099649022\n",
            "282 train:::acccuracy   51.39186295503212    J    1.1587636987893941 ::::: test accuracy 50.249584026622294    J    1.1588619876632646\n",
            "283 train:::acccuracy   51.39186295503212    J    1.1586478802852098 ::::: test accuracy 50.249584026622294    J    1.1587464887328796\n",
            "284 train:::acccuracy   51.39186295503212    J    1.1585320836319961 ::::: test accuracy 50.249584026622294    J    1.1586310131316648\n",
            "285 train:::acccuracy   51.39186295503212    J    1.1584163087784338 ::::: test accuracy 50.249584026622294    J    1.1585155608173499\n",
            "286 train:::acccuracy   51.32048536759457    J    1.158300555672958 ::::: test accuracy 50.249584026622294    J    1.1584001317474524\n",
            "287 train:::acccuracy   51.32048536759457    J    1.1581848242637383 ::::: test accuracy 50.249584026622294    J    1.1582847258792954\n",
            "288 train:::acccuracy   51.32048536759457    J    1.1580691144986952 ::::: test accuracy 50.582362728785355    J    1.1581693431699969\n",
            "289 train:::acccuracy   51.32048536759457    J    1.157953426325482 ::::: test accuracy 50.582362728785355    J    1.1580539835764707\n",
            "290 train:::acccuracy   51.32048536759457    J    1.1578377596914897 ::::: test accuracy 50.582362728785355    J    1.157938647055422\n",
            "291 train:::acccuracy   51.32048536759457    J    1.1577221145438512 ::::: test accuracy 50.582362728785355    J    1.1578233335633488\n",
            "292 train:::acccuracy   51.32048536759457    J    1.1576064908294206 ::::: test accuracy 50.582362728785355    J    1.1577080430565345\n",
            "293 train:::acccuracy   51.32048536759457    J    1.1574908884947912 ::::: test accuracy 50.582362728785355    J    1.1575927754910542\n",
            "294 train:::acccuracy   51.32048536759457    J    1.157375307486279 ::::: test accuracy 50.582362728785355    J    1.1574775308227627\n",
            "295 train:::acccuracy   51.32048536759457    J    1.1572597477499236 ::::: test accuracy 50.582362728785355    J    1.157362309007301\n",
            "296 train:::acccuracy   51.39186295503212    J    1.157144209231492 ::::: test accuracy 50.74875207986689    J    1.1572471100000912\n",
            "297 train:::acccuracy   51.39186295503212    J    1.1570286918764636 ::::: test accuracy 50.74875207986689    J    1.157131933756331\n",
            "298 train:::acccuracy   51.39186295503212    J    1.1569131956300436 ::::: test accuracy 50.74875207986689    J    1.1570167802309992\n",
            "299 train:::acccuracy   51.39186295503212    J    1.156797720437143 ::::: test accuracy 50.74875207986689    J    1.156901649378846\n",
            "300 train:::acccuracy   51.39186295503212    J    1.1566822662423906 ::::: test accuracy 50.91514143094842    J    1.156786541154394\n",
            "301 train:::acccuracy   51.39186295503212    J    1.1565668329901229 ::::: test accuracy 50.91514143094842    J    1.1566714555119375\n",
            "302 train:::acccuracy   51.39186295503212    J    1.1564514206243803 ::::: test accuracy 50.91514143094842    J    1.15655639240554\n",
            "303 train:::acccuracy   51.39186295503212    J    1.1563360290889109 ::::: test accuracy 50.91514143094842    J    1.156441351789028\n",
            "304 train:::acccuracy   51.39186295503212    J    1.1562206583271613 ::::: test accuracy 50.91514143094842    J    1.1563263336159944\n",
            "305 train:::acccuracy   51.39186295503212    J    1.1561053082822779 ::::: test accuracy 50.91514143094842    J    1.1562113378397902\n",
            "306 train:::acccuracy   51.39186295503212    J    1.1559899788971022 ::::: test accuracy 50.91514143094842    J    1.1560963644135296\n",
            "307 train:::acccuracy   51.39186295503212    J    1.1558746701141693 ::::: test accuracy 50.91514143094842    J    1.1559814132900845\n",
            "308 train:::acccuracy   51.39186295503212    J    1.1557593818756993 ::::: test accuracy 50.74875207986689    J    1.1558664844220778\n",
            "309 train:::acccuracy   51.39186295503212    J    1.1556441141236085 ::::: test accuracy 50.74875207986689    J    1.1557515777618868\n",
            "310 train:::acccuracy   51.39186295503212    J    1.1555288667994912 ::::: test accuracy 50.74875207986689    J    1.1556366932616404\n",
            "311 train:::acccuracy   51.53461812990721    J    1.1554136398446215 ::::: test accuracy 50.74875207986689    J    1.1555218308732125\n",
            "312 train:::acccuracy   51.605995717344754    J    1.1552984331999552 ::::: test accuracy 50.74875207986689    J    1.1554069905482247\n",
            "313 train:::acccuracy   51.605995717344754    J    1.155183246806125 ::::: test accuracy 50.74875207986689    J    1.155292172238042\n",
            "314 train:::acccuracy   51.67737330478231    J    1.1550680806034321 ::::: test accuracy 50.74875207986689    J    1.1551773758937682\n",
            "315 train:::acccuracy   51.67737330478231    J    1.1549529345318508 ::::: test accuracy 50.91514143094842    J    1.1550626014662473\n",
            "316 train:::acccuracy   51.605995717344754    J    1.154837808531021 ::::: test accuracy 50.91514143094842    J    1.15494784890606\n",
            "317 train:::acccuracy   51.605995717344754    J    1.1547227025402422 ::::: test accuracy 50.91514143094842    J    1.154833118163517\n",
            "318 train:::acccuracy   51.605995717344754    J    1.1546076164984789 ::::: test accuracy 50.91514143094842    J    1.1547184091886635\n",
            "319 train:::acccuracy   51.605995717344754    J    1.1544925503443506 ::::: test accuracy 50.91514143094842    J    1.1546037219312695\n",
            "320 train:::acccuracy   51.605995717344754    J    1.1543775040161313 ::::: test accuracy 50.91514143094842    J    1.1544890563408363\n",
            "321 train:::acccuracy   51.67737330478231    J    1.1542624774517443 ::::: test accuracy 51.08153078202995    J    1.154374412366583\n",
            "322 train:::acccuracy   51.67737330478231    J    1.1541474705887635 ::::: test accuracy 51.08153078202995    J    1.154259789957453\n",
            "323 train:::acccuracy   51.67737330478231    J    1.1540324833644062 ::::: test accuracy 51.08153078202995    J    1.1541451890621053\n",
            "324 train:::acccuracy   51.67737330478231    J    1.1539175157155275 ::::: test accuracy 51.08153078202995    J    1.1540306096289157\n",
            "325 train:::acccuracy   51.67737330478231    J    1.153802567578627 ::::: test accuracy 51.08153078202995    J    1.1539160516059752\n",
            "326 train:::acccuracy   51.67737330478231    J    1.153687638889829 ::::: test accuracy 51.08153078202995    J    1.15380151494108\n",
            "327 train:::acccuracy   51.67737330478231    J    1.1535727295849008 ::::: test accuracy 51.08153078202995    J    1.1536869995817365\n",
            "328 train:::acccuracy   51.67737330478231    J    1.1534578395992288 ::::: test accuracy 51.41430948419301    J    1.1535725054751587\n",
            "329 train:::acccuracy   51.67737330478231    J    1.1533429688678245 ::::: test accuracy 51.41430948419301    J    1.1534580325682562\n",
            "330 train:::acccuracy   51.82012847965739    J    1.1532281173253227 ::::: test accuracy 51.58069883527454    J    1.153343580807644\n",
            "331 train:::acccuracy   51.89150606709493    J    1.1531132849059769 ::::: test accuracy 51.58069883527454    J    1.1532291501396292\n",
            "332 train:::acccuracy   51.89150606709493    J    1.1529984715436499 ::::: test accuracy 51.58069883527454    J    1.1531147405102151\n",
            "333 train:::acccuracy   51.89150606709493    J    1.152883677171819 ::::: test accuracy 51.41430948419301    J    1.1530003518650935\n",
            "334 train:::acccuracy   51.962883654532476    J    1.1527689017235632 ::::: test accuracy 51.41430948419301    J    1.1528859841496453\n",
            "335 train:::acccuracy   52.10563882940756    J    1.1526541451315724 ::::: test accuracy 51.41430948419301    J    1.1527716373089363\n",
            "336 train:::acccuracy   52.10563882940756    J    1.1525394073281248 ::::: test accuracy 51.41430948419301    J    1.1526573112877123\n",
            "337 train:::acccuracy   52.10563882940756    J    1.1524246882451055 ::::: test accuracy 51.41430948419301    J    1.1525430060304014\n",
            "338 train:::acccuracy   52.10563882940756    J    1.1523099878139853 ::::: test accuracy 51.41430948419301    J    1.152428721481107\n",
            "339 train:::acccuracy   52.10563882940756    J    1.152195305965824 ::::: test accuracy 51.41430948419301    J    1.152314457583601\n",
            "340 train:::acccuracy   52.10563882940756    J    1.1520806426312642 ::::: test accuracy 51.41430948419301    J    1.1522002142813306\n",
            "341 train:::acccuracy   52.17701641684511    J    1.1519659977405343 ::::: test accuracy 51.41430948419301    J    1.1520859915174058\n",
            "342 train:::acccuracy   52.17701641684511    J    1.1518513712234344 ::::: test accuracy 51.41430948419301    J    1.1519717892346035\n",
            "343 train:::acccuracy   52.17701641684511    J    1.1517367630093382 ::::: test accuracy 51.41430948419301    J    1.151857607375358\n",
            "344 train:::acccuracy   52.10563882940756    J    1.1516221730271905 ::::: test accuracy 51.41430948419301    J    1.1517434458817624\n",
            "345 train:::acccuracy   52.10563882940756    J    1.151507601205497 ::::: test accuracy 51.41430948419301    J    1.1516293046955628\n",
            "346 train:::acccuracy   52.03426124197003    J    1.1513930474723286 ::::: test accuracy 51.58069883527454    J    1.1515151837581599\n",
            "347 train:::acccuracy   52.03426124197003    J    1.151278511755309 ::::: test accuracy 51.58069883527454    J    1.1514010830105956\n",
            "348 train:::acccuracy   52.10563882940756    J    1.151163993981614 ::::: test accuracy 51.58069883527454    J    1.1512870023935586\n",
            "349 train:::acccuracy   52.17701641684511    J    1.1510494940779745 ::::: test accuracy 51.58069883527454    J    1.1511729418473822\n",
            "350 train:::acccuracy   52.17701641684511    J    1.1509350119706556 ::::: test accuracy 51.58069883527454    J    1.1510589013120314\n",
            "351 train:::acccuracy   52.24839400428265    J    1.1508205475854694 ::::: test accuracy 51.58069883527454    J    1.1509448807271085\n",
            "352 train:::acccuracy   52.3197715917202    J    1.150706100847764 ::::: test accuracy 51.58069883527454    J    1.150830880031846\n",
            "353 train:::acccuracy   52.3197715917202    J    1.1505916716824116 ::::: test accuracy 51.58069883527454    J    1.1507168991651011\n",
            "354 train:::acccuracy   52.3197715917202    J    1.1504772600138218 ::::: test accuracy 51.74708818635607    J    1.150602938065357\n",
            "355 train:::acccuracy   52.3197715917202    J    1.150362865765915 ::::: test accuracy 51.74708818635607    J    1.1504889966707186\n",
            "356 train:::acccuracy   52.3197715917202    J    1.1502484888621398 ::::: test accuracy 51.74708818635607    J    1.1503750749189043\n",
            "357 train:::acccuracy   52.24839400428265    J    1.1501341292254554 ::::: test accuracy 51.74708818635607    J    1.150261172747244\n",
            "358 train:::acccuracy   52.3197715917202    J    1.1500197867783255 ::::: test accuracy 51.9134775374376    J    1.1501472900926784\n",
            "359 train:::acccuracy   52.3197715917202    J    1.1499054614427227 ::::: test accuracy 51.9134775374376    J    1.1500334268917576\n",
            "360 train:::acccuracy   52.3197715917202    J    1.149791153140119 ::::: test accuracy 51.9134775374376    J    1.1499195830806253\n",
            "361 train:::acccuracy   52.3197715917202    J    1.1496768617914797 ::::: test accuracy 51.9134775374376    J    1.1498057585950299\n",
            "362 train:::acccuracy   52.39114917915775    J    1.1495625873172637 ::::: test accuracy 51.9134775374376    J    1.1496919533703087\n",
            "363 train:::acccuracy   52.39114917915775    J    1.149448329637413 ::::: test accuracy 51.9134775374376    J    1.149578167341395\n",
            "364 train:::acccuracy   52.39114917915775    J    1.1493340886713475 ::::: test accuracy 51.9134775374376    J    1.1494644004428034\n",
            "365 train:::acccuracy   52.46252676659529    J    1.14921986433797 ::::: test accuracy 51.9134775374376    J    1.1493506526086348\n",
            "366 train:::acccuracy   52.533904354032835    J    1.149105656555649 ::::: test accuracy 51.9134775374376    J    1.1492369237725641\n",
            "367 train:::acccuracy   52.60528194147038    J    1.1489914652422215 ::::: test accuracy 51.9134775374376    J    1.1491232138678433\n",
            "368 train:::acccuracy   52.67665952890792    J    1.148877290314982 ::::: test accuracy 51.9134775374376    J    1.1490095228272952\n",
            "369 train:::acccuracy   52.67665952890792    J    1.1487631316906852 ::::: test accuracy 51.9134775374376    J    1.1488958505833078\n",
            "370 train:::acccuracy   52.67665952890792    J    1.1486489892855347 ::::: test accuracy 51.9134775374376    J    1.1487821970678318\n",
            "371 train:::acccuracy   52.60528194147038    J    1.1485348630151793 ::::: test accuracy 51.9134775374376    J    1.1486685622123751\n",
            "372 train:::acccuracy   52.67665952890792    J    1.1484207527947072 ::::: test accuracy 51.9134775374376    J    1.1485549459480016\n",
            "373 train:::acccuracy   52.74803711634547    J    1.1483066585386428 ::::: test accuracy 51.9134775374376    J    1.148441348205318\n",
            "374 train:::acccuracy   52.74803711634547    J    1.1481925801609418 ::::: test accuracy 51.9134775374376    J    1.1483277689144846\n",
            "375 train:::acccuracy   52.67665952890792    J    1.1480785175749797 ::::: test accuracy 51.9134775374376    J    1.1482142080052\n",
            "376 train:::acccuracy   52.67665952890792    J    1.147964470693555 ::::: test accuracy 51.9134775374376    J    1.1481006654066943\n",
            "377 train:::acccuracy   52.74803711634547    J    1.1478504394288775 ::::: test accuracy 51.9134775374376    J    1.147987141047734\n",
            "378 train:::acccuracy   52.81941470378301    J    1.1477364236925673 ::::: test accuracy 51.9134775374376    J    1.1478736348566128\n",
            "379 train:::acccuracy   52.890792291220556    J    1.1476224233956422 ::::: test accuracy 52.079866888519135    J    1.1477601467611462\n",
            "380 train:::acccuracy   52.81941470378301    J    1.147508438448517 ::::: test accuracy 51.9134775374376    J    1.147646676688669\n",
            "381 train:::acccuracy   52.890792291220556    J    1.147394468761001 ::::: test accuracy 51.9134775374376    J    1.1475332245660275\n",
            "382 train:::acccuracy   52.890792291220556    J    1.1472805142422846 ::::: test accuracy 51.9134775374376    J    1.1474197903195775\n",
            "383 train:::acccuracy   52.9621698786581    J    1.1471665748009388 ::::: test accuracy 52.079866888519135    J    1.1473063738751834\n",
            "384 train:::acccuracy   52.9621698786581    J    1.1470526503449097 ::::: test accuracy 52.079866888519135    J    1.147192975158203\n",
            "385 train:::acccuracy   53.03354746609564    J    1.1469387407815015 ::::: test accuracy 52.079866888519135    J    1.1470795940934906\n",
            "386 train:::acccuracy   53.104925053533194    J    1.1468248460173922 ::::: test accuracy 52.079866888519135    J    1.1469662306053918\n",
            "387 train:::acccuracy   53.104925053533194    J    1.1467109659586043 ::::: test accuracy 52.079866888519135    J    1.1468528846177333\n",
            "388 train:::acccuracy   53.17630264097074    J    1.1465971005105136 ::::: test accuracy 52.079866888519135    J    1.1467395560538283\n",
            "389 train:::acccuracy   53.17630264097074    J    1.146483249577839 ::::: test accuracy 52.24625623960066    J    1.1466262448364566\n",
            "390 train:::acccuracy   53.17630264097074    J    1.1463694130646298 ::::: test accuracy 52.24625623960066    J    1.146512950887872\n",
            "391 train:::acccuracy   53.24768022840828    J    1.146255590874271 ::::: test accuracy 52.24625623960066    J    1.146399674129794\n",
            "392 train:::acccuracy   53.39043540328336    J    1.1461417829094713 ::::: test accuracy 52.412645590682196    J    1.146286414483397\n",
            "393 train:::acccuracy   53.461812990720915    J    1.1460279890722467 ::::: test accuracy 52.412645590682196    J    1.1461731718693133\n",
            "394 train:::acccuracy   53.461812990720915    J    1.1459142092639356 ::::: test accuracy 52.412645590682196    J    1.1460599462076206\n",
            "395 train:::acccuracy   53.604568165596    J    1.1458004433851712 ::::: test accuracy 52.412645590682196    J    1.1459467374178427\n",
            "396 train:::acccuracy   53.67594575303355    J    1.1456866913358854 ::::: test accuracy 52.57903494176372    J    1.1458335454189403\n",
            "397 train:::acccuracy   53.81870092790864    J    1.145572953015304 ::::: test accuracy 52.57903494176372    J    1.1457203701293053\n",
            "398 train:::acccuracy   53.81870092790864    J    1.145459228321929 ::::: test accuracy 52.57903494176372    J    1.1456072114667555\n",
            "399 train:::acccuracy   53.81870092790864    J    1.145345517153543 ::::: test accuracy 52.57903494176372    J    1.1454940693485334\n",
            "400 train:::acccuracy   53.81870092790864    J    1.1452318194071942 ::::: test accuracy 52.57903494176372    J    1.1453809436912938\n",
            "401 train:::acccuracy   53.81870092790864    J    1.1451181349791961 ::::: test accuracy 52.57903494176372    J    1.1452678344111018\n",
            "402 train:::acccuracy   53.89007851534618    J    1.1450044637651144 ::::: test accuracy 52.57903494176372    J    1.1451547414234284\n",
            "403 train:::acccuracy   53.89007851534618    J    1.1448908056597653 ::::: test accuracy 52.57903494176372    J    1.1450416646431387\n",
            "404 train:::acccuracy   53.89007851534618    J    1.1447771605571986 ::::: test accuracy 52.57903494176372    J    1.1449286039844937\n",
            "405 train:::acccuracy   53.89007851534618    J    1.1446635283507034 ::::: test accuracy 52.74542429284526    J    1.144815559361139\n",
            "406 train:::acccuracy   53.96145610278372    J    1.144549908932788 ::::: test accuracy 52.74542429284526    J    1.144702530686098\n",
            "407 train:::acccuracy   53.96145610278372    J    1.1444363021951856 ::::: test accuracy 52.74542429284526    J    1.1445895178717715\n",
            "408 train:::acccuracy   53.96145610278372    J    1.1443227080288292 ::::: test accuracy 52.74542429284526    J    1.144476520829926\n",
            "409 train:::acccuracy   54.032833690221274    J    1.1442091263238636 ::::: test accuracy 52.74542429284526    J    1.1443635394716907\n",
            "410 train:::acccuracy   54.10421127765882    J    1.1440955569696205 ::::: test accuracy 52.74542429284526    J    1.1442505737075477\n",
            "411 train:::acccuracy   54.032833690221274    J    1.1439819998546223 ::::: test accuracy 52.74542429284526    J    1.14413762344733\n",
            "412 train:::acccuracy   54.10421127765882    J    1.1438684548665687 ::::: test accuracy 52.74542429284526    J    1.144024688600212\n",
            "413 train:::acccuracy   54.032833690221274    J    1.1437549218923269 ::::: test accuracy 52.57903494176372    J    1.1439117690747027\n",
            "414 train:::acccuracy   54.10421127765882    J    1.1436414008179265 ::::: test accuracy 52.57903494176372    J    1.1437988647786403\n",
            "415 train:::acccuracy   54.10421127765882    J    1.143527891528556 ::::: test accuracy 52.57903494176372    J    1.1436859756191875\n",
            "416 train:::acccuracy   54.17558886509636    J    1.1434143939085408 ::::: test accuracy 52.57903494176372    J    1.1435731015028199\n",
            "417 train:::acccuracy   54.246966452533904    J    1.1433009078413499 ::::: test accuracy 52.57903494176372    J    1.1434602423353233\n",
            "418 train:::acccuracy   54.389721627408996    J    1.1431874332095766 ::::: test accuracy 52.57903494176372    J    1.143347398021784\n",
            "419 train:::acccuracy   54.53247680228408    J    1.143073969894934 ::::: test accuracy 52.57903494176372    J    1.143234568466584\n",
            "420 train:::acccuracy   54.53247680228408    J    1.1429605177782487 ::::: test accuracy 52.57903494176372    J    1.1431217535733933\n",
            "421 train:::acccuracy   54.53247680228408    J    1.1428470767394447 ::::: test accuracy 52.57903494176372    J    1.1430089532451615\n",
            "422 train:::acccuracy   54.53247680228408    J    1.1427336466575435 ::::: test accuracy 52.911813643926784    J    1.1428961673841125\n",
            "423 train:::acccuracy   54.389721627408996    J    1.1426202274106445 ::::: test accuracy 52.911813643926784    J    1.142783395891733\n",
            "424 train:::acccuracy   54.31834403997144    J    1.1425068188759293 ::::: test accuracy 52.911813643926784    J    1.1426706386687737\n",
            "425 train:::acccuracy   54.31834403997144    J    1.14239342092964 ::::: test accuracy 52.911813643926784    J    1.1425578956152316\n",
            "426 train:::acccuracy   54.31834403997144    J    1.1422800334470748 ::::: test accuracy 52.911813643926784    J    1.1424451666303481\n",
            "427 train:::acccuracy   54.389721627408996    J    1.142166656302578 ::::: test accuracy 52.911813643926784    J    1.1423324516126039\n",
            "428 train:::acccuracy   54.53247680228408    J    1.142053289369532 ::::: test accuracy 52.911813643926784    J    1.1422197504597016\n",
            "429 train:::acccuracy   54.53247680228408    J    1.1419399325203494 ::::: test accuracy 53.07820299500832    J    1.1421070630685695\n",
            "430 train:::acccuracy   54.53247680228408    J    1.1418265856264525 ::::: test accuracy 53.07820299500832    J    1.141994389335345\n",
            "431 train:::acccuracy   54.53247680228408    J    1.141713248558279 ::::: test accuracy 53.244592346089846    J    1.1418817291553718\n",
            "432 train:::acccuracy   54.53247680228408    J    1.1415999211852594 ::::: test accuracy 53.244592346089846    J    1.1417690824231872\n",
            "433 train:::acccuracy   54.603854389721626    J    1.1414866033758118 ::::: test accuracy 53.244592346089846    J    1.1416564490325205\n",
            "434 train:::acccuracy   54.603854389721626    J    1.1413732949973325 ::::: test accuracy 53.244592346089846    J    1.1415438288762758\n",
            "435 train:::acccuracy   54.675231977159164    J    1.1412599959161873 ::::: test accuracy 53.244592346089846    J    1.14143122184653\n",
            "436 train:::acccuracy   54.675231977159164    J    1.1411467059976905 ::::: test accuracy 53.244592346089846    J    1.1413186278345266\n",
            "437 train:::acccuracy   54.675231977159164    J    1.141033425106106 ::::: test accuracy 53.57737104825291    J    1.1412060467306577\n",
            "438 train:::acccuracy   54.675231977159164    J    1.140920153104635 ::::: test accuracy 53.74376039933444    J    1.141093478424463\n",
            "439 train:::acccuracy   54.74660956459672    J    1.1408068898553987 ::::: test accuracy 53.91014975041597    J    1.140980922804622\n",
            "440 train:::acccuracy   54.81798715203426    J    1.1406936352194352 ::::: test accuracy 54.0765391014975    J    1.1408683797589374\n",
            "441 train:::acccuracy   54.81798715203426    J    1.14058038905668 ::::: test accuracy 54.0765391014975    J    1.1407558491743313\n",
            "442 train:::acccuracy   54.8893647394718    J    1.1404671512259603 ::::: test accuracy 54.24292845257903    J    1.1406433309368382\n",
            "443 train:::acccuracy   54.8893647394718    J    1.1403539215849805 ::::: test accuracy 54.24292845257903    J    1.140530824931592\n",
            "444 train:::acccuracy   54.960742326909354    J    1.1402406999903212 ::::: test accuracy 54.24292845257903    J    1.1404183310428158\n",
            "445 train:::acccuracy   54.960742326909354    J    1.1401274862974073 ::::: test accuracy 54.24292845257903    J    1.1403058491538165\n",
            "446 train:::acccuracy   54.960742326909354    J    1.1400142803605122 ::::: test accuracy 54.24292845257903    J    1.140193379146973\n",
            "447 train:::acccuracy   54.81798715203426    J    1.1399010820327466 ::::: test accuracy 54.24292845257903    J    1.1400809209037273\n",
            "448 train:::acccuracy   54.81798715203426    J    1.1397878911660337 ::::: test accuracy 54.409317803660564    J    1.1399684743045708\n",
            "449 train:::acccuracy   55.0321199143469    J    1.1396747076111096 ::::: test accuracy 54.409317803660564    J    1.139856039229043\n",
            "450 train:::acccuracy   54.960742326909354    J    1.1395615312175036 ::::: test accuracy 54.57570715474209    J    1.1397436155557121\n",
            "451 train:::acccuracy   54.960742326909354    J    1.1394483618335274 ::::: test accuracy 54.57570715474209    J    1.1396312031621725\n",
            "452 train:::acccuracy   54.960742326909354    J    1.1393351993062655 ::::: test accuracy 54.57570715474209    J    1.1395188019250269\n",
            "453 train:::acccuracy   54.960742326909354    J    1.1392220434815574 ::::: test accuracy 54.57570715474209    J    1.1394064117198826\n",
            "454 train:::acccuracy   55.0321199143469    J    1.1391088942039898 ::::: test accuracy 54.409317803660564    J    1.1392940324213412\n",
            "455 train:::acccuracy   55.0321199143469    J    1.1389957513168771 ::::: test accuracy 54.409317803660564    J    1.1391816639029835\n",
            "456 train:::acccuracy   55.0321199143469    J    1.1388826146622582 ::::: test accuracy 54.57570715474209    J    1.1390693060373562\n",
            "457 train:::acccuracy   55.0321199143469    J    1.1387694840808693 ::::: test accuracy 54.57570715474209    J    1.1389569586959742\n",
            "458 train:::acccuracy   55.10349750178444    J    1.1386563594121428 ::::: test accuracy 54.57570715474209    J    1.1388446217492925\n",
            "459 train:::acccuracy   55.24625267665952    J    1.138543240494188 ::::: test accuracy 54.57570715474209    J    1.13873229506671\n",
            "460 train:::acccuracy   55.24625267665952    J    1.1384301271637745 ::::: test accuracy 54.57570715474209    J    1.1386199785165485\n",
            "461 train:::acccuracy   55.38900785153462    J    1.1383170192563254 ::::: test accuracy 54.57570715474209    J    1.138507671966045\n",
            "462 train:::acccuracy   55.38900785153462    J    1.1382039166058966 ::::: test accuracy 54.57570715474209    J    1.138395375281342\n",
            "463 train:::acccuracy   55.46038543897216    J    1.1380908190451688 ::::: test accuracy 54.57570715474209    J    1.1382830883274702\n",
            "464 train:::acccuracy   55.6745182012848    J    1.137977726405421 ::::: test accuracy 54.742096505823625    J    1.1381708109683415\n",
            "465 train:::acccuracy   55.6745182012848    J    1.1378646385165316 ::::: test accuracy 54.742096505823625    J    1.1380585430667378\n",
            "466 train:::acccuracy   55.6745182012848    J    1.1377515552069526 ::::: test accuracy 54.90848585690515    J    1.1379462844842934\n",
            "467 train:::acccuracy   55.74589578872234    J    1.1376384763036937 ::::: test accuracy 54.90848585690515    J    1.1378340350814882\n",
            "468 train:::acccuracy   55.74589578872234    J    1.137525401632317 ::::: test accuracy 54.90848585690515    J    1.1377217947176337\n",
            "469 train:::acccuracy   55.88865096359743    J    1.137412331016912 ::::: test accuracy 54.90848585690515    J    1.1376095632508578\n",
            "470 train:::acccuracy   55.88865096359743    J    1.137299264280083 ::::: test accuracy 54.90848585690515    J    1.1374973405380968\n",
            "471 train:::acccuracy   55.88865096359743    J    1.1371862012429346 ::::: test accuracy 54.90848585690515    J    1.1373851264350776\n",
            "472 train:::acccuracy   55.88865096359743    J    1.1370731417250504 ::::: test accuracy 54.90848585690515    J    1.1372729207963093\n",
            "473 train:::acccuracy   55.81727337615988    J    1.136960085544485 ::::: test accuracy 54.90848585690515    J    1.137160723475068\n",
            "474 train:::acccuracy   55.88865096359743    J    1.1368470325177444 ::::: test accuracy 54.90848585690515    J    1.137048534323382\n",
            "475 train:::acccuracy   55.88865096359743    J    1.136733982459763 ::::: test accuracy 54.90848585690515    J    1.1369363531920202\n",
            "476 train:::acccuracy   55.81727337615988    J    1.1366209351838943 ::::: test accuracy 55.074875207986686    J    1.1368241799304788\n",
            "477 train:::acccuracy   55.81727337615988    J    1.1365078905018937 ::::: test accuracy 55.074875207986686    J    1.136712014386963\n",
            "478 train:::acccuracy   55.88865096359743    J    1.1363948482238915 ::::: test accuracy 55.24126455906821    J    1.1365998564083823\n",
            "479 train:::acccuracy   55.88865096359743    J    1.1362818081583943 ::::: test accuracy 55.24126455906821    J    1.136487705840327\n",
            "480 train:::acccuracy   55.96002855103498    J    1.136168770112244 ::::: test accuracy 55.24126455906821    J    1.1363755625270593\n",
            "481 train:::acccuracy   55.96002855103498    J    1.1360557338906223 ::::: test accuracy 55.24126455906821    J    1.1362634263114924\n",
            "482 train:::acccuracy   56.102783725910065    J    1.1359426992970103 ::::: test accuracy 55.24126455906821    J    1.1361512970351872\n",
            "483 train:::acccuracy   56.38829407566024    J    1.135829666133192 ::::: test accuracy 55.24126455906821    J    1.1360391745383256\n",
            "484 train:::acccuracy   56.38829407566024    J    1.1357166341992186 ::::: test accuracy 55.574043261231274    J    1.1359270586597034\n",
            "485 train:::acccuracy   56.45967166309779    J    1.1356036032934014 ::::: test accuracy 55.574043261231274    J    1.1358149492367111\n",
            "486 train:::acccuracy   56.60242683797287    J    1.1354905732122804 ::::: test accuracy 55.906821963394336    J    1.135702846105316\n",
            "487 train:::acccuracy   56.60242683797287    J    1.1353775437506222 ::::: test accuracy 55.906821963394336    J    1.1355907491000539\n",
            "488 train:::acccuracy   56.673804425410424    J    1.1352645147013805 ::::: test accuracy 55.906821963394336    J    1.1354786580540097\n",
            "489 train:::acccuracy   56.74518201284796    J    1.1351514858556953 ::::: test accuracy 56.07321131447587    J    1.1353665727987994\n",
            "490 train:::acccuracy   56.81655960028551    J    1.1350384570028527 ::::: test accuracy 56.2396006655574    J    1.1352544931645527\n",
            "491 train:::acccuracy   56.88793718772306    J    1.1349254279302892 ::::: test accuracy 56.2396006655574    J    1.1351424189799042\n",
            "492 train:::acccuracy   57.17344753747323    J    1.1348123984235436 ::::: test accuracy 56.2396006655574    J    1.135030350071968\n",
            "493 train:::acccuracy   57.17344753747323    J    1.1346993682662614 ::::: test accuracy 56.2396006655574    J    1.1349182862663274\n",
            "494 train:::acccuracy   57.17344753747323    J    1.1345863372401568 ::::: test accuracy 56.2396006655574    J    1.1348062273870116\n",
            "495 train:::acccuracy   57.24482512491078    J    1.1344733051249982 ::::: test accuracy 56.07321131447587    J    1.1346941732564848\n",
            "496 train:::acccuracy   57.38758029978587    J    1.1343602716985823 ::::: test accuracy 56.07321131447587    J    1.1345821236956244\n",
            "497 train:::acccuracy   57.601713062098504    J    1.1342472367367211 ::::: test accuracy 56.07321131447587    J    1.1344700785237047\n",
            "498 train:::acccuracy   57.74446823697359    J    1.1341342000132062 ::::: test accuracy 56.07321131447587    J    1.134358037558379\n",
            "499 train:::acccuracy   57.81584582441114    J    1.1340211612998035 ::::: test accuracy 56.2396006655574    J    1.1342460006156587\n",
            "500 train:::acccuracy   57.74446823697359    J    1.133908120366212 ::::: test accuracy 56.2396006655574    J    1.1341339675098998\n",
            "501 train:::acccuracy   57.74446823697359    J    1.1337950769800522 ::::: test accuracy 56.40599001663894    J    1.1340219380537815\n",
            "502 train:::acccuracy   57.88722341184867    J    1.1336820309068412 ::::: test accuracy 56.738768718802    J    1.1339099120582856\n",
            "503 train:::acccuracy   58.02997858672377    J    1.133568981909969 ::::: test accuracy 56.738768718802    J    1.1337978893326828\n",
            "504 train:::acccuracy   58.17273376159886    J    1.133455929750669 ::::: test accuracy 56.738768718802    J    1.1336858696845038\n",
            "505 train:::acccuracy   58.17273376159886    J    1.1333428741880012 ::::: test accuracy 56.905158069883534    J    1.1335738529195316\n",
            "506 train:::acccuracy   58.31548893647395    J    1.1332298149788258 ::::: test accuracy 57.07154742096506    J    1.1334618388417708\n",
            "507 train:::acccuracy   58.38686652391149    J    1.1331167518777707 ::::: test accuracy 57.07154742096506    J    1.1333498272534335\n",
            "508 train:::acccuracy   58.38686652391149    J    1.133003684637225 ::::: test accuracy 57.237936772046595    J    1.13323781795492\n",
            "509 train:::acccuracy   58.38686652391149    J    1.1328906130072844 ::::: test accuracy 57.237936772046595    J    1.1331258107447917\n",
            "510 train:::acccuracy   58.45824411134903    J    1.1327775367357558 ::::: test accuracy 57.237936772046595    J    1.1330138054197554\n",
            "511 train:::acccuracy   58.31548893647395    J    1.1326644555681085 ::::: test accuracy 57.40432612312812    J    1.13290180177464\n",
            "512 train:::acccuracy   58.31548893647395    J    1.1325513692474645 ::::: test accuracy 57.40432612312812    J    1.1327897996023784\n",
            "513 train:::acccuracy   58.2441113490364    J    1.1324382775145523 ::::: test accuracy 57.570715474209656    J    1.1326777986939762\n",
            "514 train:::acccuracy   58.2441113490364    J    1.1323251801077023 ::::: test accuracy 57.570715474209656    J    1.1325657988385043\n",
            "515 train:::acccuracy   58.2441113490364    J    1.132212076762798 ::::: test accuracy 57.570715474209656    J    1.132453799823062\n",
            "516 train:::acccuracy   58.38686652391149    J    1.1320989672132624 ::::: test accuracy 57.40432612312812    J    1.1323418014327644\n",
            "517 train:::acccuracy   58.529621698786585    J    1.131985851190027 ::::: test accuracy 57.40432612312812    J    1.1322298034507114\n",
            "518 train:::acccuracy   58.60099928622412    J    1.1318727284214916 ::::: test accuracy 57.40432612312812    J    1.1321178056579768\n",
            "519 train:::acccuracy   58.529621698786585    J    1.1317595986335178 ::::: test accuracy 57.40432612312812    J    1.1320058078335682\n",
            "520 train:::acccuracy   58.67237687366167    J    1.1316464615493718 ::::: test accuracy 57.40432612312812    J    1.131893809754412\n",
            "521 train:::acccuracy   58.743754461099215    J    1.131533316889719 ::::: test accuracy 57.40432612312812    J    1.1317818111953346\n",
            "522 train:::acccuracy   58.81513204853675    J    1.1314201643725783 ::::: test accuracy 57.40432612312812    J    1.1316698119290285\n",
            "523 train:::acccuracy   58.81513204853675    J    1.1313070037133017 ::::: test accuracy 57.570715474209656    J    1.1315578117260285\n",
            "524 train:::acccuracy   58.743754461099215    J    1.1311938346245305 ::::: test accuracy 57.570715474209656    J    1.1314458103546938\n",
            "525 train:::acccuracy   58.81513204853675    J    1.1310806568161782 ::::: test accuracy 57.90349417637272    J    1.1313338075811705\n",
            "526 train:::acccuracy   58.886509635974306    J    1.1309674699953844 ::::: test accuracy 57.90349417637272    J    1.13122180316938\n",
            "527 train:::acccuracy   58.95788722341185    J    1.130854273866498 ::::: test accuracy 57.90349417637272    J    1.1311097968809785\n",
            "528 train:::acccuracy   58.95788722341185    J    1.1307410681310295 ::::: test accuracy 57.90349417637272    J    1.1309977884753428\n",
            "529 train:::acccuracy   59.02926481084939    J    1.1306278524876283 ::::: test accuracy 57.90349417637272    J    1.1308857777095336\n",
            "530 train:::acccuracy   59.02926481084939    J    1.1305146266320407 ::::: test accuracy 58.069883527454245    J    1.1307737643382745\n",
            "531 train:::acccuracy   59.100642398286936    J    1.1304013902570842 ::::: test accuracy 58.069883527454245    J    1.1306617481139196\n",
            "532 train:::acccuracy   59.100642398286936    J    1.130288143052606 ::::: test accuracy 58.23627287853578    J    1.1305497287864332\n",
            "533 train:::acccuracy   59.02926481084939    J    1.1301748847054531 ::::: test accuracy 58.23627287853578    J    1.130437706103349\n",
            "534 train:::acccuracy   59.02926481084939    J    1.1300616148994287 ::::: test accuracy 58.23627287853578    J    1.1303256798097552\n",
            "535 train:::acccuracy   59.02926481084939    J    1.1299483333152678 ::::: test accuracy 58.069883527454245    J    1.1302136496482504\n",
            "536 train:::acccuracy   59.02926481084939    J    1.1298350396305907 ::::: test accuracy 58.23627287853578    J    1.1301016153589312\n",
            "537 train:::acccuracy   59.02926481084939    J    1.1297217335198713 ::::: test accuracy 58.402662229617306    J    1.1299895766793435\n",
            "538 train:::acccuracy   59.02926481084939    J    1.1296084146543968 ::::: test accuracy 58.402662229617306    J    1.129877533344469\n",
            "539 train:::acccuracy   59.100642398286936    J    1.1294950827022316 ::::: test accuracy 58.402662229617306    J    1.1297654850866818\n",
            "540 train:::acccuracy   59.100642398286936    J    1.1293817373281774 ::::: test accuracy 58.402662229617306    J    1.1296534316357245\n",
            "541 train:::acccuracy   59.172019985724475    J    1.1292683781937327 ::::: test accuracy 58.402662229617306    J    1.1295413727186707\n",
            "542 train:::acccuracy   59.24339757316203    J    1.1291550049570618 ::::: test accuracy 58.402662229617306    J    1.1294293080599023\n",
            "543 train:::acccuracy   59.172019985724475    J    1.1290416172729347 ::::: test accuracy 58.56905158069884    J    1.1293172373810627\n",
            "544 train:::acccuracy   59.24339757316203    J    1.1289282147927155 ::::: test accuracy 58.56905158069884    J    1.1292051604010374\n",
            "545 train:::acccuracy   59.24339757316203    J    1.1288147971642901 ::::: test accuracy 58.402662229617306    J    1.129093076835914\n",
            "546 train:::acccuracy   59.314775160599574    J    1.1287013640320502 ::::: test accuracy 58.402662229617306    J    1.1289809863989464\n",
            "547 train:::acccuracy   59.314775160599574    J    1.1285879150368345 ::::: test accuracy 58.402662229617306    J    1.1288688888005258\n",
            "548 train:::acccuracy   59.38615274803711    J    1.1284744498158883 ::::: test accuracy 58.56905158069884    J    1.1287567837481398\n",
            "549 train:::acccuracy   59.38615274803711    J    1.1283609680028293 ::::: test accuracy 58.56905158069884    J    1.1286446709463436\n",
            "550 train:::acccuracy   59.457530335474665    J    1.1282474692275892 ::::: test accuracy 58.56905158069884    J    1.128532550096716\n",
            "551 train:::acccuracy   59.60028551034975    J    1.1281339531163839 ::::: test accuracy 58.402662229617306    J    1.12842042089783\n",
            "552 train:::acccuracy   59.60028551034975    J    1.1280204192916505 ::::: test accuracy 58.56905158069884    J    1.1283082830452145\n",
            "553 train:::acccuracy   59.88579586009993    J    1.1279068673720194 ::::: test accuracy 58.56905158069884    J    1.1281961362313124\n",
            "554 train:::acccuracy   59.88579586009993    J    1.127793296972254 ::::: test accuracy 58.73544093178037    J    1.1280839801454503\n",
            "555 train:::acccuracy   59.95717344753747    J    1.1276797077032064 ::::: test accuracy 58.73544093178037    J    1.127971814473788\n",
            "556 train:::acccuracy   59.81441827266239    J    1.127566099171775 ::::: test accuracy 58.73544093178037    J    1.127859638899296\n",
            "557 train:::acccuracy   59.81441827266239    J    1.1274524709808493 ::::: test accuracy 58.73544093178037    J    1.1277474531016973\n",
            "558 train:::acccuracy   59.88579586009993    J    1.127338822729263 ::::: test accuracy 59.06821963394343    J    1.1276352567574448\n",
            "559 train:::acccuracy   59.81441827266239    J    1.1272251540117408 ::::: test accuracy 59.06821963394343    J    1.1275230495396666\n",
            "560 train:::acccuracy   59.81441827266239    J    1.1271114644188531 ::::: test accuracy 59.23460898502496    J    1.127410831118134\n",
            "561 train:::acccuracy   59.81441827266239    J    1.1269977535369617 ::::: test accuracy 59.73377703826955    J    1.1272986011592132\n",
            "562 train:::acccuracy   60.02855103497502    J    1.126884020948163 ::::: test accuracy 59.900166389351085    J    1.1271863593258264\n",
            "563 train:::acccuracy   59.95717344753747    J    1.126770266230239 ::::: test accuracy 59.900166389351085    J    1.1270741052774091\n",
            "564 train:::acccuracy   59.95717344753747    J    1.1266564889566066 ::::: test accuracy 59.900166389351085    J    1.1269618386698628\n",
            "565 train:::acccuracy   59.95717344753747    J    1.1265426886962562 ::::: test accuracy 60.06655574043261    J    1.1268495591555148\n",
            "566 train:::acccuracy   59.95717344753747    J    1.1264288650136998 ::::: test accuracy 60.232945091514146    J    1.1267372663830688\n",
            "567 train:::acccuracy   59.95717344753747    J    1.126315017468916 ::::: test accuracy 60.232945091514146    J    1.1266249599975604\n",
            "568 train:::acccuracy   59.95717344753747    J    1.1262011456172862 ::::: test accuracy 60.232945091514146    J    1.1265126396403184\n",
            "569 train:::acccuracy   60.17130620985011    J    1.1260872490095504 ::::: test accuracy 60.232945091514146    J    1.126400304948902\n",
            "570 train:::acccuracy   60.52819414703783    J    1.1259733271917287 ::::: test accuracy 60.39933444259567    J    1.1262879555570684\n",
            "571 train:::acccuracy   60.45681655960029    J    1.1258593797050827 ::::: test accuracy 60.39933444259567    J    1.126175591094718\n",
            "572 train:::acccuracy   60.45681655960029    J    1.1257454060860348 ::::: test accuracy 60.732113144758735    J    1.126063211187842\n",
            "573 train:::acccuracy   60.38543897216274    J    1.1256314058661279 ::::: test accuracy 60.89850249584027    J    1.1259508154584794\n",
            "574 train:::acccuracy   60.52819414703783    J    1.1255173785719417 ::::: test accuracy 60.89850249584027    J    1.1258384035246625\n",
            "575 train:::acccuracy   60.52819414703783    J    1.12540332372505 ::::: test accuracy 60.732113144758735    J    1.125725975000365\n",
            "576 train:::acccuracy   60.670949321912914    J    1.125289240841938 ::::: test accuracy 60.732113144758735    J    1.1256135294954497\n",
            "577 train:::acccuracy   60.599571734475376    J    1.1251751294339476 ::::: test accuracy 60.89850249584027    J    1.1255010666156235\n",
            "578 train:::acccuracy   60.81370449678801    J    1.125060989007217 ::::: test accuracy 60.89850249584027    J    1.12538858596237\n",
            "579 train:::acccuracy   60.81370449678801    J    1.1249468190625966 ::::: test accuracy 60.89850249584027    J    1.1252760871329055\n",
            "580 train:::acccuracy   60.81370449678801    J    1.1248326190955986 ::::: test accuracy 60.89850249584027    J    1.1251635697201223\n",
            "581 train:::acccuracy   61.02783725910065    J    1.1247183885963123 ::::: test accuracy 61.064891846921796    J    1.12505103331253\n",
            "582 train:::acccuracy   61.09921484653819    J    1.1246041270493516 ::::: test accuracy 61.064891846921796    J    1.1249384774941995\n",
            "583 train:::acccuracy   61.02783725910065    J    1.1244898339337674 ::::: test accuracy 61.064891846921796    J    1.1248259018447069\n",
            "584 train:::acccuracy   61.02783725910065    J    1.1243755087229848 ::::: test accuracy 61.064891846921796    J    1.1247133059390713\n",
            "585 train:::acccuracy   61.09921484653819    J    1.12426115088473 ::::: test accuracy 61.064891846921796    J    1.1246006893477007\n",
            "586 train:::acccuracy   61.170592433975735    J    1.1241467598809483 ::::: test accuracy 61.23128119800333    J    1.1244880516363267\n",
            "587 train:::acccuracy   61.24197002141327    J    1.124032335167743 ::::: test accuracy 61.23128119800333    J    1.1243753923659479\n",
            "588 train:::acccuracy   61.38472519628837    J    1.1239178761952813 ::::: test accuracy 61.39767054908486    J    1.124262711092763\n",
            "589 train:::acccuracy   61.38472519628837    J    1.1238033824077336 ::::: test accuracy 61.73044925124792    J    1.1241500073681105\n",
            "590 train:::acccuracy   61.527480371163456    J    1.1236888532431797 ::::: test accuracy 61.56405990016639    J    1.1240372807384058\n",
            "591 train:::acccuracy   61.67023554603854    J    1.1235742881335429 ::::: test accuracy 61.56405990016639    J    1.1239245307450716\n",
            "592 train:::acccuracy   61.74161313347609    J    1.1234596865044946 ::::: test accuracy 61.56405990016639    J    1.1238117569244768\n",
            "593 train:::acccuracy   61.67023554603854    J    1.123345047775386 ::::: test accuracy 61.73044925124792    J    1.12369895880787\n",
            "594 train:::acccuracy   61.67023554603854    J    1.1232303713591534 ::::: test accuracy 62.06322795341098    J    1.1235861359213035\n",
            "595 train:::acccuracy   61.81299072091363    J    1.1231156566622411 ::::: test accuracy 62.06322795341098    J    1.123473287785576\n",
            "596 train:::acccuracy   61.88436830835118    J    1.1230009030845067 ::::: test accuracy 62.229617304492514    J    1.1233604139161513\n",
            "597 train:::acccuracy   61.88436830835118    J    1.1228861100191425 ::::: test accuracy 62.39600665557404    J    1.123247513823097\n",
            "598 train:::acccuracy   61.81299072091363    J    1.1227712768525826 ::::: test accuracy 62.39600665557404    J    1.1231345870110054\n",
            "599 train:::acccuracy   61.95574589578873    J    1.1226564029644108 ::::: test accuracy 62.39600665557404    J    1.1230216329789222\n",
            "600 train:::acccuracy   61.95574589578873    J    1.122541487727272 ::::: test accuracy 62.39600665557404    J    1.1229086512202717\n",
            "601 train:::acccuracy   62.098501070663815    J    1.1224265305067789 ::::: test accuracy 62.39600665557404    J    1.1227956412227855\n",
            "602 train:::acccuracy   62.2412562455389    J    1.1223115306614169 ::::: test accuracy 62.562396006655575    J    1.1226826024684176\n",
            "603 train:::acccuracy   62.31263383297645    J    1.1221964875424484 ::::: test accuracy 62.562396006655575    J    1.1225695344332733\n",
            "604 train:::acccuracy   62.31263383297645    J    1.122081400493812 ::::: test accuracy 62.562396006655575    J    1.1224564365875271\n",
            "605 train:::acccuracy   62.669521770164174    J    1.121966268852033 ::::: test accuracy 62.562396006655575    J    1.122343308395343\n",
            "606 train:::acccuracy   62.669521770164174    J    1.1218510919461129 ::::: test accuracy 62.7287853577371    J    1.1222301493147915\n",
            "607 train:::acccuracy   62.74089935760171    J    1.1217358690974308 ::::: test accuracy 62.89517470881864    J    1.1221169587977697\n",
            "608 train:::acccuracy   62.81227694503926    J    1.121620599619643 ::::: test accuracy 62.89517470881864    J    1.1220037362899145\n",
            "609 train:::acccuracy   63.097787294789434    J    1.1215052828185743 ::::: test accuracy 62.89517470881864    J    1.1218904812305173\n",
            "610 train:::acccuracy   63.097787294789434    J    1.1213899179921163 ::::: test accuracy 62.89517470881864    J    1.1217771930524396\n",
            "611 train:::acccuracy   63.311920057102064    J    1.121274504430107 ::::: test accuracy 62.89517470881864    J    1.1216638711820204\n",
            "612 train:::acccuracy   63.38329764453962    J    1.1211590414142356 ::::: test accuracy 62.89517470881864    J    1.121550515038991\n",
            "613 train:::acccuracy   63.5260528194147    J    1.1210435282179216 ::::: test accuracy 63.061564059900164    J    1.1214371240363827\n",
            "614 train:::acccuracy   63.597430406852254    J    1.1209279641062029 ::::: test accuracy 63.061564059900164    J    1.1213236975804322\n",
            "615 train:::acccuracy   63.597430406852254    J    1.120812348335622 ::::: test accuracy 63.061564059900164    J    1.121210235070491\n",
            "616 train:::acccuracy   63.668807994289786    J    1.1206966801541065 ::::: test accuracy 63.061564059900164    J    1.1210967358989257\n",
            "617 train:::acccuracy   63.74018558172734    J    1.120580958800849 ::::: test accuracy 62.89517470881864    J    1.120983199451026\n",
            "618 train:::acccuracy   63.811563169164884    J    1.120465183506194 ::::: test accuracy 62.89517470881864    J    1.1208696251049024\n",
            "619 train:::acccuracy   63.954318344039976    J    1.1203493534915037 ::::: test accuracy 62.89517470881864    J    1.1207560122313924\n",
            "620 train:::acccuracy   64.09707351891505    J    1.1202334679690418 ::::: test accuracy 63.061564059900164    J    1.120642360193948\n",
            "621 train:::acccuracy   64.23982869379014    J    1.1201175261418415 ::::: test accuracy 63.394342762063225    J    1.1205286683485396\n",
            "622 train:::acccuracy   64.38258386866525    J    1.1200015272035797 ::::: test accuracy 63.394342762063225    J    1.1204149360435562\n",
            "623 train:::acccuracy   64.45396145610277    J    1.1198854703384429 ::::: test accuracy 63.394342762063225    J    1.1203011626196866\n",
            "624 train:::acccuracy   64.59671663097787    J    1.1197693547210015 ::::: test accuracy 63.394342762063225    J    1.1201873474098216\n",
            "625 train:::acccuracy   64.66809421841542    J    1.1196531795160665 ::::: test accuracy 63.394342762063225    J    1.120073489738937\n",
            "626 train:::acccuracy   64.81084939329051    J    1.1195369438785494 ::::: test accuracy 63.394342762063225    J    1.119959588923986\n",
            "627 train:::acccuracy   64.95360456816559    J    1.1194206469533352 ::::: test accuracy 63.394342762063225    J    1.1198456442737852\n",
            "628 train:::acccuracy   65.16773733047823    J    1.1193042878751354 ::::: test accuracy 63.394342762063225    J    1.119731655088896\n",
            "629 train:::acccuracy   65.23911491791577    J    1.1191878657683327 ::::: test accuracy 63.56073211314476    J    1.1196176206615105\n",
            "630 train:::acccuracy   65.31049250535332    J    1.119071379746855 ::::: test accuracy 63.56073211314476    J    1.1195035402753317\n",
            "631 train:::acccuracy   65.38187009279086    J    1.118954828914006 ::::: test accuracy 63.56073211314476    J    1.119389413205451\n",
            "632 train:::acccuracy   65.45324768022842    J    1.1188382123623304 ::::: test accuracy 63.727121464226286    J    1.1192752387182283\n",
            "633 train:::acccuracy   65.59600285510349    J    1.118721529173447 ::::: test accuracy 64.22628951747087    J    1.1191610160711623\n",
            "634 train:::acccuracy   65.59600285510349    J    1.1186047784179056 ::::: test accuracy 64.55906821963394    J    1.1190467445127643\n",
            "635 train:::acccuracy   65.81013561741614    J    1.1184879591550163 ::::: test accuracy 64.55906821963394    J    1.1189324232824331\n",
            "636 train:::acccuracy   66.0956459671663    J    1.1183710704326952 ::::: test accuracy 64.39267886855241    J    1.1188180516103192\n",
            "637 train:::acccuracy   66.2384011420414    J    1.1182541112872963 ::::: test accuracy 64.39267886855241    J    1.1187036287171919\n",
            "638 train:::acccuracy   66.38115631691649    J    1.1181370807434508 ::::: test accuracy 64.55906821963394    J    1.1185891538143053\n",
            "639 train:::acccuracy   66.30977872947894    J    1.118019977813886 ::::: test accuracy 64.72545757071548    J    1.118474626103258\n",
            "640 train:::acccuracy   66.45253390435403    J    1.1179028014992707 ::::: test accuracy 64.891846921797    J    1.1183600447758542\n",
            "641 train:::acccuracy   66.59528907922912    J    1.1177855507880186 ::::: test accuracy 64.891846921797    J    1.1182454090139593\n",
            "642 train:::acccuracy   66.73804425410421    J    1.1176682246561285 ::::: test accuracy 64.891846921797    J    1.118130717989358\n",
            "643 train:::acccuracy   66.73804425410421    J    1.1175508220669939 ::::: test accuracy 64.891846921797    J    1.1180159708636048\n",
            "644 train:::acccuracy   66.66666666666666    J    1.1174333419712248 ::::: test accuracy 64.891846921797    J    1.1179011667878722\n",
            "645 train:::acccuracy   66.73804425410421    J    1.1173157833064464 ::::: test accuracy 64.891846921797    J    1.1177863049028027\n",
            "646 train:::acccuracy   66.80942184154176    J    1.1171981449971338 ::::: test accuracy 64.891846921797    J    1.117671384338349\n",
            "647 train:::acccuracy   66.95217701641684    J    1.1170804259543894 ::::: test accuracy 65.05823627287853    J    1.117556404213619\n",
            "648 train:::acccuracy   66.95217701641684    J    1.116962625075771 ::::: test accuracy 65.05823627287853    J    1.1174413636367162\n",
            "649 train:::acccuracy   67.09493219129193    J    1.1168447412450766 ::::: test accuracy 65.05823627287853    J    1.1173262617045732\n",
            "650 train:::acccuracy   67.16630977872948    J    1.1167267733321462 ::::: test accuracy 65.55740432612312    J    1.1172110975027911\n",
            "651 train:::acccuracy   67.23768736616702    J    1.1166087201926564 ::::: test accuracy 65.72379367720465    J    1.1170958701054667\n",
            "652 train:::acccuracy   67.23768736616702    J    1.116490580667905 ::::: test accuracy 65.89018302828619    J    1.11698057857502\n",
            "653 train:::acccuracy   67.5231977159172    J    1.1163723535846017 ::::: test accuracy 65.89018302828619    J    1.1168652219620254\n",
            "654 train:::acccuracy   67.59457530335474    J    1.1162540377546482 ::::: test accuracy 66.05657237936772    J    1.1167497993050288\n",
            "655 train:::acccuracy   67.6659528907923    J    1.116135631974919 ::::: test accuracy 66.22296173044924    J    1.1166343096303728\n",
            "656 train:::acccuracy   67.80870806566737    J    1.116017135027033 ::::: test accuracy 66.22296173044924    J    1.1165187519520032\n",
            "657 train:::acccuracy   67.88008565310493    J    1.1158985456771267 ::::: test accuracy 66.38935108153078    J    1.1164031252712954\n",
            "658 train:::acccuracy   68.09421841541756    J    1.1157798626756203 ::::: test accuracy 66.38935108153078    J    1.1162874285768551\n",
            "659 train:::acccuracy   68.1655960028551    J    1.1156610847569812 ::::: test accuracy 66.38935108153078    J    1.116171660844328\n",
            "660 train:::acccuracy   68.1655960028551    J    1.115542210639483 ::::: test accuracy 66.38935108153078    J    1.1160558210362033\n",
            "661 train:::acccuracy   68.37972876516774    J    1.1154232390249648 ::::: test accuracy 66.22296173044924    J    1.1159399081016168\n",
            "662 train:::acccuracy   68.59386152748037    J    1.1153041685985687 ::::: test accuracy 66.22296173044924    J    1.1158239209761394\n",
            "663 train:::acccuracy   68.66523911491792    J    1.1151849980285022 ::::: test accuracy 66.05657237936772    J    1.1157078585815776\n",
            "664 train:::acccuracy   68.87937187723054    J    1.115065725965767 ::::: test accuracy 66.22296173044924    J    1.1155917198257623\n",
            "665 train:::acccuracy   68.807994289793    J    1.1149463510439062 ::::: test accuracy 66.22296173044924    J    1.1154755036023278\n",
            "666 train:::acccuracy   68.73661670235546    J    1.1148268718787329 ::::: test accuracy 66.38935108153078    J    1.1153592087905078\n",
            "667 train:::acccuracy   68.807994289793    J    1.1147072870680523 ::::: test accuracy 66.55574043261231    J    1.1152428342548963\n",
            "668 train:::acccuracy   68.87937187723054    J    1.1145875951913953 ::::: test accuracy 66.05657237936772    J    1.115126378845233\n",
            "669 train:::acccuracy   69.16488222698072    J    1.1144677948097317 ::::: test accuracy 66.22296173044924    J    1.1150098413961727\n",
            "670 train:::acccuracy   69.16488222698072    J    1.114347884465184 ::::: test accuracy 66.38935108153078    J    1.1148932207270463\n",
            "671 train:::acccuracy   69.09350463954318    J    1.1142278626807356 ::::: test accuracy 66.38935108153078    J    1.1147765156416303\n",
            "672 train:::acccuracy   68.95074946466809    J    1.114107727959933 ::::: test accuracy 66.38935108153078    J    1.1146597249278973\n",
            "673 train:::acccuracy   68.95074946466809    J    1.113987478786591 ::::: test accuracy 66.38935108153078    J    1.1145428473577745\n",
            "674 train:::acccuracy   69.16488222698072    J    1.1138671136244718 ::::: test accuracy 66.55574043261231    J    1.1144258816868908\n",
            "675 train:::acccuracy   69.23625981441828    J    1.1137466309169848 ::::: test accuracy 66.55574043261231    J    1.1143088266543248\n",
            "676 train:::acccuracy   69.30763740185581    J    1.113626029086862 ::::: test accuracy 67.0549084858569    J    1.1141916809823385\n",
            "677 train:::acccuracy   69.37901498929337    J    1.1135053065358353 ::::: test accuracy 67.38768718801997    J    1.1140744433761165\n",
            "678 train:::acccuracy   69.37901498929337    J    1.1133844616443054 ::::: test accuracy 67.55407653910149    J    1.113957112523498\n",
            "679 train:::acccuracy   69.52177016416844    J    1.1132634927710092 ::::: test accuracy 67.22129783693843    J    1.1138396870947018\n",
            "680 train:::acccuracy   69.52177016416844    J    1.1131423982526738 ::::: test accuracy 67.0549084858569    J    1.1137221657420482\n",
            "681 train:::acccuracy   69.593147751606    J    1.1130211764036666 ::::: test accuracy 67.0549084858569    J    1.1136045470996665\n",
            "682 train:::acccuracy   69.66452533904354    J    1.1128998255156544 ::::: test accuracy 67.22129783693843    J    1.1134868297832217\n",
            "683 train:::acccuracy   69.593147751606    J    1.1127783438572192 ::::: test accuracy 67.38768718801997    J    1.1133690123896034\n",
            "684 train:::acccuracy   69.52177016416844    J    1.1126567296735168 ::::: test accuracy 67.38768718801997    J    1.113251093496637\n",
            "685 train:::acccuracy   69.52177016416844    J    1.1125349811858822 ::::: test accuracy 67.55407653910149    J    1.1131330716627756\n",
            "686 train:::acccuracy   69.66452533904354    J    1.1124130965914587 ::::: test accuracy 67.72046589018302    J    1.1130149454267826\n",
            "687 train:::acccuracy   69.66452533904354    J    1.1122910740628094 ::::: test accuracy 67.55407653910149    J    1.1128967133074257\n",
            "688 train:::acccuracy   69.73590292648109    J    1.1121689117475198 ::::: test accuracy 67.55407653910149    J    1.1127783738031458\n",
            "689 train:::acccuracy   69.80728051391864    J    1.1120466077677973 ::::: test accuracy 67.72046589018302    J    1.1126599253917342\n",
            "690 train:::acccuracy   70.09279086366881    J    1.1119241602200594 ::::: test accuracy 67.72046589018302    J    1.1125413665299937\n",
            "691 train:::acccuracy   70.09279086366881    J    1.1118015671745183 ::::: test accuracy 67.72046589018302    J    1.1124226956533978\n",
            "692 train:::acccuracy   70.30692362598144    J    1.1116788266747581 ::::: test accuracy 67.72046589018302    J    1.112303911175753\n",
            "693 train:::acccuracy   70.37830121341898    J    1.1115559367372982 ::::: test accuracy 67.88685524126456    J    1.1121850114888265\n",
            "694 train:::acccuracy   70.37830121341898    J    1.1114328953511556 ::::: test accuracy 67.88685524126456    J    1.1120659949620078\n",
            "695 train:::acccuracy   70.44967880085653    J    1.1113097004773924 ::::: test accuracy 68.21963394342761    J    1.11194685994193\n",
            "696 train:::acccuracy   70.44967880085653    J    1.1111863500486576 ::::: test accuracy 68.21963394342761    J    1.1118276047520892\n",
            "697 train:::acccuracy   70.80656673804425    J    1.111062841968732 ::::: test accuracy 68.38602329450914    J    1.1117082276924815\n",
            "698 train:::acccuracy   70.80656673804425    J    1.1109391741120418 ::::: test accuracy 68.38602329450914    J    1.1115887270392042\n",
            "699 train:::acccuracy   70.94932191291934    J    1.1108153443231743 ::::: test accuracy 68.71880199667221    J    1.1114691010440598\n",
            "700 train:::acccuracy   71.16345467523197    J    1.1106913504163964 ::::: test accuracy 68.71880199667221    J    1.1113493479341572\n",
            "701 train:::acccuracy   71.16345467523197    J    1.1105671901751446 ::::: test accuracy 68.88519134775375    J    1.1112294659115007\n",
            "702 train:::acccuracy   71.16345467523197    J    1.1104428613515105 ::::: test accuracy 69.05158069883528    J    1.1111094531525736\n",
            "703 train:::acccuracy   71.23483226266953    J    1.110318361665733 ::::: test accuracy 69.05158069883528    J    1.1109893078079056\n",
            "704 train:::acccuracy   71.3775874375446    J    1.1101936888056487 ::::: test accuracy 69.05158069883528    J    1.1108690280016482\n",
            "705 train:::acccuracy   71.3775874375446    J    1.110068840426169 ::::: test accuracy 69.05158069883528    J    1.1107486118311218\n",
            "706 train:::acccuracy   71.3775874375446    J    1.1099438141487126 ::::: test accuracy 69.21797004991681    J    1.110628057366377\n",
            "707 train:::acccuracy   71.23483226266953    J    1.1098186075606584 ::::: test accuracy 69.21797004991681    J    1.1105073626497255\n",
            "708 train:::acccuracy   71.23483226266953    J    1.1096932182147623 ::::: test accuracy 69.38435940099833    J    1.1103865256952752\n",
            "709 train:::acccuracy   71.3775874375446    J    1.1095676436285749 ::::: test accuracy 69.38435940099833    J    1.1102655444884517\n",
            "710 train:::acccuracy   71.59172019985725    J    1.1094418812838496 ::::: test accuracy 69.21797004991681    J    1.110144416985516\n",
            "711 train:::acccuracy   71.5203426124197    J    1.1093159286259364 ::::: test accuracy 69.21797004991681    J    1.110023141113062\n",
            "712 train:::acccuracy   71.73447537473233    J    1.1091897830631645 ::::: test accuracy 69.21797004991681    J    1.1099017147675148\n",
            "713 train:::acccuracy   71.73447537473233    J    1.1090634419662013 ::::: test accuracy 69.21797004991681    J    1.1097801358146164\n",
            "714 train:::acccuracy   71.87723054960742    J    1.1089369026674245 ::::: test accuracy 69.38435940099833    J    1.1096584020888942\n",
            "715 train:::acccuracy   71.87723054960742    J    1.108810162460261 ::::: test accuracy 69.21797004991681    J    1.1095365113931355\n",
            "716 train:::acccuracy   71.87723054960742    J    1.1086832185985172 ::::: test accuracy 69.21797004991681    J    1.1094144614978316\n",
            "717 train:::acccuracy   71.87723054960742    J    1.1085560682957003 ::::: test accuracy 69.21797004991681    J    1.1092922501406224\n",
            "718 train:::acccuracy   71.87723054960742    J    1.1084287087243268 ::::: test accuracy 69.38435940099833    J    1.1091698750257357\n",
            "719 train:::acccuracy   71.87723054960742    J    1.1083011370152092 ::::: test accuracy 69.38435940099833    J    1.1090473338233973\n",
            "720 train:::acccuracy   72.01998572448251    J    1.1081733502567388 ::::: test accuracy 69.38435940099833    J    1.1089246241692523\n",
            "721 train:::acccuracy   72.23411848679515    J    1.108045345494146 ::::: test accuracy 69.55074875207987    J    1.1088017436637507\n",
            "722 train:::acccuracy   72.37687366167023    J    1.107917119728755 ::::: test accuracy 69.55074875207987    J    1.108678689871541\n",
            "723 train:::acccuracy   72.30549607423269    J    1.1077886699172117 ::::: test accuracy 69.7171381031614    J    1.1085554603208427\n",
            "724 train:::acccuracy   72.37687366167023    J    1.1076599929707063 ::::: test accuracy 69.88352745424294    J    1.1084320525028049\n",
            "725 train:::acccuracy   72.37687366167023    J    1.1075310857541822 ::::: test accuracy 69.88352745424294    J    1.1083084638708582\n",
            "726 train:::acccuracy   72.44825124910777    J    1.1074019450855137 ::::: test accuracy 69.88352745424294    J    1.1081846918400435\n",
            "727 train:::acccuracy   72.51962883654532    J    1.1072725677346882 ::::: test accuracy 69.88352745424294    J    1.108060733786342\n",
            "728 train:::acccuracy   72.51962883654532    J    1.1071429504229497 ::::: test accuracy 69.88352745424294    J    1.1079365870459774\n",
            "729 train:::acccuracy   72.66238401142041    J    1.1070130898219497 ::::: test accuracy 69.88352745424294    J    1.107812248914711\n",
            "730 train:::acccuracy   72.73376159885797    J    1.1068829825528548 ::::: test accuracy 69.88352745424294    J    1.1076877166471264\n",
            "731 train:::acccuracy   72.73376159885797    J    1.10675262518546 ::::: test accuracy 69.88352745424294    J    1.1075629874558863\n",
            "732 train:::acccuracy   72.73376159885797    J    1.1066220142372696 ::::: test accuracy 69.88352745424294    J    1.1074380585109918\n",
            "733 train:::acccuracy   72.66238401142041    J    1.10649114617256 ::::: test accuracy 70.04991680532446    J    1.1073129269390107\n",
            "734 train:::acccuracy   72.59100642398288    J    1.1063600174014327 ::::: test accuracy 70.04991680532446    J    1.1071875898222978\n",
            "735 train:::acccuracy   72.66238401142041    J    1.1062286242788366 ::::: test accuracy 70.21630615640599    J    1.1070620441982058\n",
            "736 train:::acccuracy   72.59100642398288    J    1.106096963103582 ::::: test accuracy 70.21630615640599    J    1.1069362870582617\n",
            "737 train:::acccuracy   72.73376159885797    J    1.1059650301173214 ::::: test accuracy 70.38269550748753    J    1.1068103153473445\n",
            "738 train:::acccuracy   72.73376159885797    J    1.1058328215035196 ::::: test accuracy 70.38269550748753    J    1.106684125962836\n",
            "739 train:::acccuracy   72.73376159885797    J    1.1057003333863986 ::::: test accuracy 70.38269550748753    J    1.1065577157537576\n",
            "740 train:::acccuracy   72.80513918629549    J    1.1055675618298608 ::::: test accuracy 70.38269550748753    J    1.106431081519885\n",
            "741 train:::acccuracy   72.80513918629549    J    1.1054345028363917 ::::: test accuracy 70.38269550748753    J    1.106304220010853\n",
            "742 train:::acccuracy   72.9478943611706    J    1.1053011523459348 ::::: test accuracy 70.38269550748753    J    1.1061771279252341\n",
            "743 train:::acccuracy   73.09064953604569    J    1.105167506234753 ::::: test accuracy 70.38269550748753    J    1.1060498019095972\n",
            "744 train:::acccuracy   73.09064953604569    J    1.1050335603142531 ::::: test accuracy 70.38269550748753    J    1.1059222385575533\n",
            "745 train:::acccuracy   73.23340471092077    J    1.1048993103297975 ::::: test accuracy 70.38269550748753    J    1.1057944344087756\n",
            "746 train:::acccuracy   73.30478229835832    J    1.1047647519594834 ::::: test accuracy 70.21630615640599    J    1.1056663859480036\n",
            "747 train:::acccuracy   73.59029264810849    J    1.1046298808129005 ::::: test accuracy 70.21630615640599    J    1.1055380896040194\n",
            "748 train:::acccuracy   73.51891506067095    J    1.1044946924298626 ::::: test accuracy 70.38269550748753    J    1.105409541748611\n",
            "749 train:::acccuracy   73.66167023554604    J    1.1043591822791108 ::::: test accuracy 70.54908485856906    J    1.1052807386955048\n",
            "750 train:::acccuracy   73.94718058529621    J    1.1042233457569803 ::::: test accuracy 70.54908485856906    J    1.1051516766992822\n",
            "751 train:::acccuracy   73.94718058529621    J    1.1040871781860662 ::::: test accuracy 70.54908485856906    J    1.1050223519542688\n",
            "752 train:::acccuracy   73.94718058529621    J    1.1039506748138286 ::::: test accuracy 70.71547420965058    J    1.104892760593407\n",
            "753 train:::acccuracy   74.23269093504639    J    1.1038138308111867 ::::: test accuracy 70.71547420965058    J    1.104762898687093\n",
            "754 train:::acccuracy   74.30406852248393    J    1.1036766412710795 ::::: test accuracy 70.71547420965058    J    1.104632762241999\n",
            "755 train:::acccuracy   74.37544610992148    J    1.103539101206996 ::::: test accuracy 70.71547420965058    J    1.1045023471998652\n",
            "756 train:::acccuracy   74.37544610992148    J    1.103401205551464 ::::: test accuracy 70.88186356073211    J    1.104371649436269\n",
            "757 train:::acccuracy   74.44682369735904    J    1.1032629491545316 ::::: test accuracy 70.88186356073211    J    1.1042406647593586\n",
            "758 train:::acccuracy   74.44682369735904    J    1.1031243267821869 ::::: test accuracy 70.88186356073211    J    1.1041093889085787\n",
            "759 train:::acccuracy   74.37544610992148    J    1.1029853331147548 ::::: test accuracy 70.88186356073211    J    1.1039778175533455\n",
            "760 train:::acccuracy   74.44682369735904    J    1.102845962745269 ::::: test accuracy 70.88186356073211    J    1.1038459462917058\n",
            "761 train:::acccuracy   74.44682369735904    J    1.102706210177795 ::::: test accuracy 70.71547420965058    J    1.1037137706489695\n",
            "762 train:::acccuracy   74.44682369735904    J    1.1025660698257265 ::::: test accuracy 70.88186356073211    J    1.1035812860763\n",
            "763 train:::acccuracy   74.51820128479658    J    1.1024255360100355 ::::: test accuracy 71.21464226289518    J    1.103448487949288\n",
            "764 train:::acccuracy   74.66095645967167    J    1.1022846029574807 ::::: test accuracy 71.21464226289518    J    1.103315371566482\n",
            "765 train:::acccuracy   74.58957887223411    J    1.1021432647988108 ::::: test accuracy 71.54742096505824    J    1.103181932147898\n",
            "766 train:::acccuracy   74.51820128479658    J    1.1020015155668716 ::::: test accuracy 71.54742096505824    J    1.1030481648334791\n",
            "767 train:::acccuracy   74.66095645967167    J    1.1018593491947235 ::::: test accuracy 71.8801996672213    J    1.1029140646815452\n",
            "768 train:::acccuracy   74.66095645967167    J    1.1017167595136808 ::::: test accuracy 71.71381031613977    J    1.1027796266671863\n",
            "769 train:::acccuracy   74.7323340471092    J    1.1015737402513408 ::::: test accuracy 71.71381031613977    J    1.1026448456806293\n",
            "770 train:::acccuracy   74.80371163454676    J    1.1014302850295308 ::::: test accuracy 71.8801996672213    J    1.1025097165255757\n",
            "771 train:::acccuracy   74.7323340471092    J    1.1012863873622467 ::::: test accuracy 71.8801996672213    J    1.1023742339174853\n",
            "772 train:::acccuracy   74.80371163454676    J    1.101142040653521 ::::: test accuracy 72.21297836938436    J    1.1022383924818357\n",
            "773 train:::acccuracy   74.66095645967167    J    1.1009972381952535 ::::: test accuracy 72.3793677204659    J    1.102102186752337\n",
            "774 train:::acccuracy   74.66095645967167    J    1.1008519731649917 ::::: test accuracy 72.54575707154743    J    1.1019656111691112\n",
            "775 train:::acccuracy   74.94646680942184    J    1.1007062386236541 ::::: test accuracy 72.54575707154743    J    1.10182866007682\n",
            "776 train:::acccuracy   74.80371163454676    J    1.100560027513228 ::::: test accuracy 72.54575707154743    J    1.1016913277227636\n",
            "777 train:::acccuracy   74.8750892219843    J    1.1004133326543677 ::::: test accuracy 72.54575707154743    J    1.1015536082549229\n",
            "778 train:::acccuracy   74.8750892219843    J    1.1002661467440005 ::::: test accuracy 72.3793677204659    J    1.1014154957199742\n",
            "779 train:::acccuracy   74.94646680942184    J    1.1001184623528153 ::::: test accuracy 72.54575707154743    J    1.1012769840612435\n",
            "780 train:::acccuracy   75.01784439685939    J    1.0999702719227482 ::::: test accuracy 72.3793677204659    J    1.1011380671166164\n",
            "781 train:::acccuracy   75.08922198429693    J    1.0998215677643708 ::::: test accuracy 72.3793677204659    J    1.1009987386164126\n",
            "782 train:::acccuracy   75.23197715917203    J    1.0996723420542458 ::::: test accuracy 72.71214642262895    J    1.100858992181192\n",
            "783 train:::acccuracy   75.08922198429693    J    1.0995225868322065 ::::: test accuracy 72.87853577371048    J    1.100718821319535\n",
            "784 train:::acccuracy   74.94646680942184    J    1.0993722939985788 ::::: test accuracy 72.87853577371048    J    1.1005782194257452\n",
            "785 train:::acccuracy   74.8750892219843    J    1.0992214553113493 ::::: test accuracy 72.87853577371048    J    1.100437179777521\n",
            "786 train:::acccuracy   75.16059957173448    J    1.0990700623832432 ::::: test accuracy 72.87853577371048    J    1.1002956955335583\n",
            "787 train:::acccuracy   75.30335474660956    J    1.098918106678766 ::::: test accuracy 73.04492512479202    J    1.1001537597311102\n",
            "788 train:::acccuracy   75.37473233404711    J    1.098765579511154 ::::: test accuracy 72.87853577371048    J    1.1000113652834778\n",
            "789 train:::acccuracy   75.37473233404711    J    1.0986124720392556 ::::: test accuracy 73.04492512479202    J    1.0998685049774517\n",
            "790 train:::acccuracy   75.37473233404711    J    1.0984587752643706 ::::: test accuracy 73.21131447587355    J    1.0997251714706944\n",
            "791 train:::acccuracy   75.5174875089222    J    1.098304480026957 ::::: test accuracy 73.21131447587355    J    1.099581357289051\n",
            "792 train:::acccuracy   75.66024268379728    J    1.0981495770033256 ::::: test accuracy 73.04492512479202    J    1.0994370548238084\n",
            "793 train:::acccuracy   75.73162027123483    J    1.0979940567022135 ::::: test accuracy 73.04492512479202    J    1.0992922563288876\n",
            "794 train:::acccuracy   75.73162027123483    J    1.0978379094612936 ::::: test accuracy 72.71214642262895    J    1.0991469539179635\n",
            "795 train:::acccuracy   75.73162027123483    J    1.0976811254436014 ::::: test accuracy 72.71214642262895    J    1.0990011395615218\n",
            "796 train:::acccuracy   75.73162027123483    J    1.0975236946338784 ::::: test accuracy 72.87853577371048    J    1.0988548050838525\n",
            "797 train:::acccuracy   75.5174875089222    J    1.097365606834824 ::::: test accuracy 72.87853577371048    J    1.0987079421599588\n",
            "798 train:::acccuracy   75.44610992148465    J    1.097206851663267 ::::: test accuracy 72.87853577371048    J    1.098560542312403\n",
            "799 train:::acccuracy   75.44610992148465    J    1.0970474185462324 ::::: test accuracy 72.71214642262895    J    1.0984125969080742\n",
            "800 train:::acccuracy   75.5174875089222    J    1.0968872967169405 ::::: test accuracy 72.71214642262895    J    1.0982640971548743\n",
            "801 train:::acccuracy   75.5174875089222    J    1.0967264752106798 ::::: test accuracy 72.71214642262895    J    1.0981150340983408\n",
            "802 train:::acccuracy   75.37473233404711    J    1.0965649428606075 ::::: test accuracy 72.71214642262895    J    1.0979653986181637\n",
            "803 train:::acccuracy   75.37473233404711    J    1.0964026882934317 ::::: test accuracy 72.54575707154743    J    1.0978151814246413\n",
            "804 train:::acccuracy   75.30335474660956    J    1.0962396999249993 ::::: test accuracy 72.87853577371048    J    1.097664373055039\n",
            "805 train:::acccuracy   75.44610992148465    J    1.0960759659557806 ::::: test accuracy 73.04492512479202    J    1.097512963869867\n",
            "806 train:::acccuracy   75.66024268379728    J    1.0959114743662381 ::::: test accuracy 72.87853577371048    J    1.0973609440490588\n",
            "807 train:::acccuracy   75.73162027123483    J    1.0957462129120894 ::::: test accuracy 73.04492512479202    J    1.0972083035880682\n",
            "808 train:::acccuracy   75.73162027123483    J    1.0955801691194467 ::::: test accuracy 72.87853577371048    J    1.0970550322938637\n",
            "809 train:::acccuracy   75.73162027123483    J    1.0954133302798579 ::::: test accuracy 72.87853577371048    J    1.0969011197808296\n",
            "810 train:::acccuracy   75.73162027123483    J    1.0952456834452038 ::::: test accuracy 73.04492512479202    J    1.0967465554665679\n",
            "811 train:::acccuracy   75.94575303354748    J    1.0950772154224886 ::::: test accuracy 73.21131447587355    J    1.0965913285675941\n",
            "812 train:::acccuracy   76.1598857958601    J    1.0949079127685009 ::::: test accuracy 73.21131447587355    J    1.0964354280949316\n",
            "813 train:::acccuracy   76.23126338329764    J    1.094737761784339 ::::: test accuracy 73.04492512479202    J    1.096278842849589\n",
            "814 train:::acccuracy   76.37401855817274    J    1.0945667485098007 ::::: test accuracy 72.87853577371048    J    1.0961215614179505\n",
            "815 train:::acccuracy   76.44539614561027    J    1.0943948587176529 ::::: test accuracy 72.87853577371048    J    1.0959635721670171\n",
            "816 train:::acccuracy   76.51677373304783    J    1.0942220779077392 ::::: test accuracy 72.87853577371048    J    1.0958048632395663\n",
            "817 train:::acccuracy   76.58815132048537    J    1.0940483913009522 ::::: test accuracy 72.87853577371048    J    1.0956454225491643\n",
            "818 train:::acccuracy   76.58815132048537    J    1.0938737838330699 ::::: test accuracy 73.04492512479202    J    1.0954852377750723\n",
            "819 train:::acccuracy   76.65952890792292    J    1.0936982401484092 ::::: test accuracy 73.04492512479202    J    1.0953242963570187\n",
            "820 train:::acccuracy   76.73090649536046    J    1.0935217445933592 ::::: test accuracy 73.04492512479202    J    1.0951625854898457\n",
            "821 train:::acccuracy   76.802284082798    J    1.0933442812097305 ::::: test accuracy 73.21131447587355    J    1.095000092118019\n",
            "822 train:::acccuracy   76.802284082798    J    1.0931658337279406 ::::: test accuracy 73.21131447587355    J    1.0948368029300002\n",
            "823 train:::acccuracy   76.802284082798    J    1.0929863855600497 ::::: test accuracy 73.5440931780366    J    1.0946727043524886\n",
            "824 train:::acccuracy   76.87366167023555    J    1.0928059197925921 ::::: test accuracy 73.87687188019967    J    1.094507782544502\n",
            "825 train:::acccuracy   77.01641684511064    J    1.0926244191792667 ::::: test accuracy 73.87687188019967    J    1.0943420233913248\n",
            "826 train:::acccuracy   77.08779443254818    J    1.0924418661334094 ::::: test accuracy 74.04326123128119    J    1.094175412498293\n",
            "827 train:::acccuracy   77.08779443254818    J    1.0922582427203005 ::::: test accuracy 74.20965058236273    J    1.0940079351844352\n",
            "828 train:::acccuracy   77.15917201998572    J    1.0920735306492697 ::::: test accuracy 74.20965058236273    J    1.0938395764759403\n",
            "829 train:::acccuracy   77.15917201998572    J    1.0918877112656131 ::::: test accuracy 74.37603993344426    J    1.093670321099465\n",
            "830 train:::acccuracy   77.23054960742327    J    1.0917007655422843 ::::: test accuracy 74.37603993344426    J    1.093500153475282\n",
            "831 train:::acccuracy   77.30192719486081    J    1.0915126740714167 ::::: test accuracy 74.5424292845258    J    1.0933290577102377\n",
            "832 train:::acccuracy   77.30192719486081    J    1.0913234170555803 ::::: test accuracy 74.5424292845258    J    1.0931570175905454\n",
            "833 train:::acccuracy   77.4446823697359    J    1.091132974298871 ::::: test accuracy 74.70881863560732    J    1.0929840165743872\n",
            "834 train:::acccuracy   77.51605995717344    J    1.0909413251977294 ::::: test accuracy 74.87520798668885    J    1.092810037784333\n",
            "835 train:::acccuracy   77.51605995717344    J    1.0907484487315682 ::::: test accuracy 74.87520798668885    J    1.0926350639995663\n",
            "836 train:::acccuracy   77.51605995717344    J    1.0905543234531216 ::::: test accuracy 74.87520798668885    J    1.0924590776479026\n",
            "837 train:::acccuracy   77.51605995717344    J    1.0903589274785845 ::::: test accuracy 75.04159733777038    J    1.0922820607976216\n",
            "838 train:::acccuracy   77.58743754461099    J    1.090162238477485 ::::: test accuracy 74.70881863560732    J    1.0921039951490719\n",
            "839 train:::acccuracy   77.51605995717344    J    1.0899642336622934 ::::: test accuracy 74.70881863560732    J    1.0919248620260742\n",
            "840 train:::acccuracy   77.4446823697359    J    1.0897648897777876 ::::: test accuracy 74.70881863560732    J    1.0917446423670962\n",
            "841 train:::acccuracy   77.4446823697359    J    1.0895641830901266 ::::: test accuracy 74.70881863560732    J    1.0915633167162087\n",
            "842 train:::acccuracy   77.58743754461099    J    1.0893620893756557 ::::: test accuracy 74.70881863560732    J    1.0913808652137948\n",
            "843 train:::acccuracy   77.58743754461099    J    1.0891585839094273 ::::: test accuracy 74.87520798668885    J    1.091197267587043\n",
            "844 train:::acccuracy   77.58743754461099    J    1.08895364145342 ::::: test accuracy 74.87520798668885    J    1.091012503140173\n",
            "845 train:::acccuracy   77.65881513204855    J    1.0887472362444677 ::::: test accuracy 74.70881863560732    J    1.090826550744421\n",
            "846 train:::acccuracy   77.80157030692362    J    1.0885393419818699 ::::: test accuracy 75.04159733777038    J    1.0906393888277648\n",
            "847 train:::acccuracy   77.87294789436116    J    1.088329931814684 ::::: test accuracy 75.04159733777038    J    1.0904509953643848\n",
            "848 train:::acccuracy   77.94432548179871    J    1.0881189783287064 ::::: test accuracy 74.87520798668885    J    1.0902613478638454\n",
            "849 train:::acccuracy   78.01570306923627    J    1.087906453533102 ::::: test accuracy 74.87520798668885    J    1.0900704233600125\n",
            "850 train:::acccuracy   78.01570306923627    J    1.0876923288466955 ::::: test accuracy 74.87520798668885    J    1.0898781983996568\n",
            "851 train:::acccuracy   78.0870806566738    J    1.0874765750839168 ::::: test accuracy 74.87520798668885    J    1.0896846490307932\n",
            "852 train:::acccuracy   78.0870806566738    J    1.0872591624403825 ::::: test accuracy 74.87520798668885    J    1.0894897507906953\n",
            "853 train:::acccuracy   78.0870806566738    J    1.0870400604780979 ::::: test accuracy 75.04159733777038    J    1.08929347869361\n",
            "854 train:::acccuracy   78.0870806566738    J    1.0868192381102868 ::::: test accuracy 75.04159733777038    J    1.0890958072181411\n",
            "855 train:::acccuracy   78.15845824411134    J    1.0865966635858377 ::::: test accuracy 75.04159733777038    J    1.0888967102943246\n",
            "856 train:::acccuracy   78.15845824411134    J    1.0863723044733302 ::::: test accuracy 75.04159733777038    J    1.088696161290349\n",
            "857 train:::acccuracy   78.15845824411134    J    1.0861461276446662 ::::: test accuracy 75.04159733777038    J    1.088494132998941\n",
            "858 train:::acccuracy   78.22983583154888    J    1.085918099258277 ::::: test accuracy 75.04159733777038    J    1.0882905976233928\n",
            "859 train:::acccuracy   78.37259100642399    J    1.0856881847418856 ::::: test accuracy 74.87520798668885    J    1.0880855267632317\n",
            "860 train:::acccuracy   78.37259100642399    J    1.0854563487748468 ::::: test accuracy 74.87520798668885    J    1.087878891399505\n",
            "861 train:::acccuracy   78.37259100642399    J    1.0852225552700168 ::::: test accuracy 74.87520798668885    J    1.0876706618796996\n",
            "862 train:::acccuracy   78.44396859386153    J    1.0849867673551605 ::::: test accuracy 74.87520798668885    J    1.0874608079022465\n",
            "863 train:::acccuracy   78.51534618129907    J    1.084748947353876 ::::: test accuracy 74.87520798668885    J    1.0872492985006534\n",
            "864 train:::acccuracy   78.58672376873662    J    1.0845090567660427 ::::: test accuracy 74.87520798668885    J    1.0870361020271877\n",
            "865 train:::acccuracy   78.65810135617416    J    1.0842670562477519 ::::: test accuracy 74.87520798668885    J    1.0868211861361683\n",
            "866 train:::acccuracy   78.80085653104925    J    1.0840229055907287 ::::: test accuracy 75.04159733777038    J    1.0866045177668053\n",
            "867 train:::acccuracy   78.72947894361171    J    1.0837765637012282 ::::: test accuracy 75.04159733777038    J    1.0863860631255946\n",
            "868 train:::acccuracy   78.8722341184868    J    1.0835279885783833 ::::: test accuracy 75.20798668885192    J    1.086165787668269\n",
            "869 train:::acccuracy   78.8722341184868    J    1.0832771372920091 ::::: test accuracy 75.20798668885192    J    1.0859436560812614\n",
            "870 train:::acccuracy   78.94361170592434    J    1.0830239659598324 ::::: test accuracy 75.37437603993344    J    1.0857196322627067\n",
            "871 train:::acccuracy   78.80085653104925    J    1.0827684297241396 ::::: test accuracy 75.54076539101497    J    1.0854936793029444\n",
            "872 train:::acccuracy   78.80085653104925    J    1.0825104827278331 ::::: test accuracy 75.54076539101497    J    1.0852657594645112\n",
            "873 train:::acccuracy   78.8722341184868    J    1.0822500780898774 ::::: test accuracy 75.54076539101497    J    1.085035834161629\n",
            "874 train:::acccuracy   78.8722341184868    J    1.081987167880119 ::::: test accuracy 75.20798668885192    J    1.0848038639391628\n",
            "875 train:::acccuracy   78.94361170592434    J    1.0817217030934774 ::::: test accuracy 75.20798668885192    J    1.084569808451025\n",
            "876 train:::acccuracy   78.80085653104925    J    1.081453633623465 ::::: test accuracy 75.20798668885192    J    1.0843336264380408\n",
            "877 train:::acccuracy   78.80085653104925    J    1.0811829082350635 ::::: test accuracy 75.20798668885192    J    1.084095275705239\n",
            "878 train:::acccuracy   78.8722341184868    J    1.0809094745368937 ::::: test accuracy 75.20798668885192    J    1.0838547130985634\n",
            "879 train:::acccuracy   79.01498929336188    J    1.080633278952705 ::::: test accuracy 75.20798668885192    J    1.0836118944809912\n",
            "880 train:::acccuracy   79.15774446823698    J    1.0803542666921464 ::::: test accuracy 75.20798668885192    J    1.0833667747080455\n",
            "881 train:::acccuracy   79.15774446823698    J    1.0800723817208024 ::::: test accuracy 75.20798668885192    J    1.0831193076026786\n",
            "882 train:::acccuracy   79.22912205567452    J    1.0797875667294967 ::::: test accuracy 75.20798668885192    J    1.0828694459295323\n",
            "883 train:::acccuracy   79.22912205567452    J    1.0794997631028194 ::::: test accuracy 75.20798668885192    J    1.0826171413685457\n",
            "884 train:::acccuracy   79.22912205567452    J    1.0792089108869052 ::::: test accuracy 75.37437603993344    J    1.0823623444878887\n",
            "885 train:::acccuracy   79.22912205567452    J    1.078914948756389 ::::: test accuracy 75.04159733777038    J    1.0821050047162386\n",
            "886 train:::acccuracy   79.22912205567452    J    1.0786178139805778 ::::: test accuracy 75.04159733777038    J    1.0818450703143498\n",
            "887 train:::acccuracy   79.30049964311206    J    1.0783174423887962 ::::: test accuracy 75.04159733777038    J    1.081582488345925\n",
            "888 train:::acccuracy   79.22912205567452    J    1.0780137683348765 ::::: test accuracy 75.04159733777038    J    1.0813172046477666\n",
            "889 train:::acccuracy   79.15774446823698    J    1.0777067246608347 ::::: test accuracy 75.20798668885192    J    1.0810491637991917\n",
            "890 train:::acccuracy   79.08636688079943    J    1.0773962426596275 ::::: test accuracy 75.04159733777038    J    1.0807783090907028\n",
            "891 train:::acccuracy   79.22912205567452    J    1.0770822520370726 ::::: test accuracy 75.04159733777038    J    1.0805045824919004\n",
            "892 train:::acccuracy   79.22912205567452    J    1.0767646808728333 ::::: test accuracy 74.87520798668885    J    1.0802279246186093\n",
            "893 train:::acccuracy   79.22912205567452    J    1.0764434555805305 ::::: test accuracy 74.87520798668885    J    1.0799482746992233\n",
            "894 train:::acccuracy   79.22912205567452    J    1.0761185008668832 ::::: test accuracy 74.87520798668885    J    1.079665570540255\n",
            "895 train:::acccuracy   79.22912205567452    J    1.0757897396899436 ::::: test accuracy 74.87520798668885    J    1.0793797484910532\n",
            "896 train:::acccuracy   79.22912205567452    J    1.075457093216368 ::::: test accuracy 74.87520798668885    J    1.0790907434077106\n",
            "897 train:::acccuracy   79.15774446823698    J    1.0751204807777155 ::::: test accuracy 74.87520798668885    J    1.0787984886161093\n",
            "898 train:::acccuracy   79.15774446823698    J    1.074779819825787 ::::: test accuracy 74.87520798668885    J    1.0785029158741404\n",
            "899 train:::acccuracy   79.15774446823698    J    1.0744350258869502 ::::: test accuracy 74.70881863560732    J    1.0782039553330363\n",
            "900 train:::acccuracy   79.15774446823698    J    1.0740860125154972 ::::: test accuracy 74.70881863560732    J    1.077901535497844\n",
            "901 train:::acccuracy   79.08636688079943    J    1.073732691245965 ::::: test accuracy 74.87520798668885    J    1.0775955831870083\n",
            "902 train:::acccuracy   79.08636688079943    J    1.0733749715444691 ::::: test accuracy 75.04159733777038    J    1.0772860234910562\n",
            "903 train:::acccuracy   79.08636688079943    J    1.0730127607589846 ::::: test accuracy 75.20798668885192    J    1.0769727797303859\n",
            "904 train:::acccuracy   79.15774446823698    J    1.0726459640686272 ::::: test accuracy 75.04159733777038    J    1.0766557734121507\n",
            "905 train:::acccuracy   79.3718772305496    J    1.0722744844318923 ::::: test accuracy 75.04159733777038    J    1.0763349241862128\n",
            "906 train:::acccuracy   79.44325481798715    J    1.0718982225338556 ::::: test accuracy 75.04159733777038    J    1.0760101498001853\n",
            "907 train:::acccuracy   79.3718772305496    J    1.0715170767323399 ::::: test accuracy 75.04159733777038    J    1.0756813660535511\n",
            "908 train:::acccuracy   79.3718772305496    J    1.0711309430030596 ::::: test accuracy 75.20798668885192    J    1.0753484867508503\n",
            "909 train:::acccuracy   79.44325481798715    J    1.0707397148837308 ::::: test accuracy 75.20798668885192    J    1.0750114236539325\n",
            "910 train:::acccuracy   79.5146324054247    J    1.0703432834171451 ::::: test accuracy 75.20798668885192    J    1.0746700864333054\n",
            "911 train:::acccuracy   79.5146324054247    J    1.06994153709325 ::::: test accuracy 75.20798668885192    J    1.0743243826185305\n",
            "912 train:::acccuracy   79.5146324054247    J    1.0695343617902153 ::::: test accuracy 75.20798668885192    J    1.073974217547722\n",
            "913 train:::acccuracy   79.5146324054247    J    1.0691216407145079 ::::: test accuracy 75.37437603993344    J    1.0736194943161221\n",
            "914 train:::acccuracy   79.5146324054247    J    1.068703254339997 ::::: test accuracy 75.54076539101497    J    1.0732601137237796\n",
            "915 train:::acccuracy   79.65738758029978    J    1.0682790803461215 ::::: test accuracy 75.54076539101497    J    1.0728959742223396\n",
            "916 train:::acccuracy   79.65738758029978    J    1.0678489935551063 ::::: test accuracy 75.54076539101497    J    1.0725269718609605\n",
            "917 train:::acccuracy   79.65738758029978    J    1.067412865868312 ::::: test accuracy 75.37437603993344    J    1.072153000231374\n",
            "918 train:::acccuracy   79.65738758029978    J    1.0669705662017002 ::::: test accuracy 75.37437603993344    J    1.0717739504121144\n",
            "919 train:::acccuracy   79.87152034261243    J    1.0665219604204954 ::::: test accuracy 75.37437603993344    J    1.0713897109119497\n",
            "920 train:::acccuracy   79.94289793004997    J    1.0660669112730379 ::::: test accuracy 75.37437603993344    J    1.07100016761252\n",
            "921 train:::acccuracy   80.0142755174875    J    1.0656052783239427 ::::: test accuracy 75.37437603993344    J    1.0706052037102531\n",
            "922 train:::acccuracy   80.0142755174875    J    1.065136917886554 ::::: test accuracy 75.37437603993344    J    1.0702046996575674\n",
            "923 train:::acccuracy   80.0142755174875    J    1.0646616829548115 ::::: test accuracy 75.37437603993344    J    1.06979853310342\n",
            "924 train:::acccuracy   80.08565310492506    J    1.0641794231345407 ::::: test accuracy 75.37437603993344    J    1.0693865788332508\n",
            "925 train:::acccuracy   80.08565310492506    J    1.0636899845743113 ::::: test accuracy 75.37437603993344    J    1.0689687087083672\n",
            "926 train:::acccuracy   80.08565310492506    J    1.0631932098958845 ::::: test accuracy 75.37437603993344    J    1.0685447916048478\n",
            "927 train:::acccuracy   80.1570306923626    J    1.0626889381243834 ::::: test accuracy 75.37437603993344    J    1.068114693352018\n",
            "928 train:::acccuracy   80.22840827980015    J    1.0621770046182686 ::::: test accuracy 75.37437603993344    J    1.0676782766705764\n",
            "929 train:::acccuracy   80.22840827980015    J    1.0616572409992298 ::::: test accuracy 75.37437603993344    J    1.0672354011104717\n",
            "930 train:::acccuracy   80.22840827980015    J    1.0611294750821398 ::::: test accuracy 75.37437603993344    J    1.0667859229885945\n",
            "931 train:::acccuracy   80.29978586723769    J    1.0605935308051493 ::::: test accuracy 75.54076539101497    J    1.0663296953264114\n",
            "932 train:::acccuracy   80.29978586723769    J    1.06004922816014 ::::: test accuracy 75.54076539101497    J    1.065866567787623\n",
            "933 train:::acccuracy   80.29978586723769    J    1.0594963831236213 ::::: test accuracy 75.37437603993344    J    1.0653963866159935\n",
            "934 train:::acccuracy   80.37116345467523    J    1.0589348075882852 ::::: test accuracy 75.37437603993344    J    1.0649189945734636\n",
            "935 train:::acccuracy   80.37116345467523    J    1.0583643092953898 ::::: test accuracy 75.54076539101497    J    1.0644342308786903\n",
            "936 train:::acccuracy   80.44254104211278    J    1.0577846917681584 ::::: test accuracy 75.54076539101497    J    1.0639419311461724\n",
            "937 train:::acccuracy   80.51391862955032    J    1.0571957542464236 ::::: test accuracy 75.54076539101497    J    1.0634419273261178\n",
            "938 train:::acccuracy   80.58529621698787    J    1.0565972916227342 ::::: test accuracy 75.54076539101497    J    1.0629340476452505\n",
            "939 train:::acccuracy   80.58529621698787    J    1.0559890943801755 ::::: test accuracy 75.54076539101497    J    1.0624181165487108\n",
            "940 train:::acccuracy   80.58529621698787    J    1.055370948532165 ::::: test accuracy 75.7071547420965    J    1.0618939546433093\n",
            "941 train:::acccuracy   80.65667380442541    J    1.0547426355645149 ::::: test accuracy 75.7071547420965    J    1.0613613786423102\n",
            "942 train:::acccuracy   80.65667380442541    J    1.0541039323800432 ::::: test accuracy 75.7071547420965    J    1.0608202013120078\n",
            "943 train:::acccuracy   80.7994289793005    J    1.0534546112460965 ::::: test accuracy 75.7071547420965    J    1.060270231420351\n",
            "944 train:::acccuracy   80.87080656673804    J    1.0527944397452866 ::::: test accuracy 75.7071547420965    J    1.0597112736878795\n",
            "945 train:::acccuracy   80.87080656673804    J    1.0521231807298306 ::::: test accuracy 75.7071547420965    J    1.0591431287412765\n",
            "946 train:::acccuracy   80.87080656673804    J    1.0514405922799028 ::::: test accuracy 75.7071547420965    J    1.0585655930698448\n",
            "947 train:::acccuracy   80.72805139186295    J    1.050746427666382 ::::: test accuracy 75.87354409317804    J    1.0579784589852417\n",
            "948 train:::acccuracy   80.72805139186295    J    1.050040435318461 ::::: test accuracy 75.87354409317804    J    1.057381514584827\n",
            "949 train:::acccuracy   80.65667380442541    J    1.0493223587965874 ::::: test accuracy 75.87354409317804    J    1.056774543719011\n",
            "950 train:::acccuracy   80.58529621698787    J    1.0485919367712255 ::::: test accuracy 76.03993344425956    J    1.0561573259629862\n",
            "951 train:::acccuracy   80.65667380442541    J    1.047848903007964 ::::: test accuracy 76.2063227953411    J    1.0555296365932854\n",
            "952 train:::acccuracy   80.65667380442541    J    1.0470929863595457 ::::: test accuracy 76.2063227953411    J    1.054891246569608\n",
            "953 train:::acccuracy   80.65667380442541    J    1.0463239107653701 ::::: test accuracy 76.2063227953411    J    1.0542419225224\n",
            "954 train:::acccuracy   80.72805139186295    J    1.045541395259129 ::::: test accuracy 76.37271214642263    J    1.0535814267466779\n",
            "955 train:::acccuracy   80.72805139186295    J    1.0447451539851995 ::::: test accuracy 76.37271214642263    J    1.0529095172026328\n",
            "956 train:::acccuracy   80.58529621698787    J    1.0439348962244617 ::::: test accuracy 76.2063227953411    J    1.052225947523596\n",
            "957 train:::acccuracy   80.65667380442541    J    1.0431103264303054 ::::: test accuracy 76.2063227953411    J    1.0515304670319063\n",
            "958 train:::acccuracy   80.72805139186295    J    1.0422711442755235 ::::: test accuracy 76.2063227953411    J    1.0508228207633485\n",
            "959 train:::acccuracy   80.7994289793005    J    1.0414170447109081 ::::: test accuracy 76.2063227953411    J    1.0501027495007802\n",
            "960 train:::acccuracy   80.87080656673804    J    1.0405477180363363 ::::: test accuracy 76.2063227953411    J    1.0493699898176245\n",
            "961 train:::acccuracy   80.94218415417559    J    1.0396628499852347 ::::: test accuracy 76.37271214642263    J    1.0486242741319487\n",
            "962 train:::acccuracy   80.94218415417559    J    1.0387621218232592 ::::: test accuracy 76.37271214642263    J    1.0478653307718715\n",
            "963 train:::acccuracy   81.15631691648822    J    1.0378452104621398 ::::: test accuracy 76.37271214642263    J    1.0470928840530305\n",
            "964 train:::acccuracy   81.22769450392576    J    1.0369117885896246 ::::: test accuracy 76.53910149750416    J    1.046306654368968\n",
            "965 train:::acccuracy   81.29907209136331    J    1.035961524816492 ::::: test accuracy 77.03826955074875    J    1.0455063582951967\n",
            "966 train:::acccuracy   81.37044967880087    J    1.034994083841652 ::::: test accuracy 77.03826955074875    J    1.0446917087078564\n",
            "967 train:::acccuracy   81.37044967880087    J    1.0340091266363678 ::::: test accuracy 77.3710482529118    J    1.043862414917807\n",
            "968 train:::acccuracy   81.44182726623839    J    1.0330063106486422 ::::: test accuracy 77.3710482529118    J    1.0430181828210892\n",
            "969 train:::acccuracy   81.37044967880087    J    1.0319852900288675 ::::: test accuracy 77.3710482529118    J    1.0421587150666611\n",
            "970 train:::acccuracy   81.44182726623839    J    1.0309457158778303 ::::: test accuracy 77.3710482529118    J    1.0412837112424072\n",
            "971 train:::acccuracy   81.44182726623839    J    1.029887236518179 ::::: test accuracy 77.3710482529118    J    1.0403928680803423\n",
            "972 train:::acccuracy   81.51320485367594    J    1.028809497790501 ::::: test accuracy 77.3710482529118    J    1.039485879682039\n",
            "973 train:::acccuracy   81.51320485367594    J    1.0277121433751228 ::::: test accuracy 77.3710482529118    J    1.03856243776527\n",
            "974 train:::acccuracy   81.44182726623839    J    1.026594815140772 ::::: test accuracy 77.3710482529118    J    1.0376222319328792\n",
            "975 train:::acccuracy   81.44182726623839    J    1.0254571535212431 ::::: test accuracy 77.3710482529118    J    1.036664949964893\n",
            "976 train:::acccuracy   81.51320485367594    J    1.024298797921157 ::::: test accuracy 77.3710482529118    J    1.0356902781348993\n",
            "977 train:::acccuracy   81.51320485367594    J    1.023119387151944 ::::: test accuracy 77.3710482529118    J    1.0346979015517022\n",
            "978 train:::acccuracy   81.44182726623839    J    1.021918559899084 ::::: test accuracy 77.3710482529118    J    1.033687504527244\n",
            "979 train:::acccuracy   81.51320485367594    J    1.0206959552216714 ::::: test accuracy 77.3710482529118    J    1.0326587709717796\n",
            "980 train:::acccuracy   81.5845824411135    J    1.0194512130852598 ::::: test accuracy 77.53743760399334    J    1.0316113848172617\n",
            "981 train:::acccuracy   81.5845824411135    J    1.0181839749289296 ::::: test accuracy 77.70382695507487    J    1.0305450304698232\n",
            "982 train:::acccuracy   81.5845824411135    J    1.0168938842674227 ::::: test accuracy 77.87021630615641    J    1.029459393292268\n",
            "983 train:::acccuracy   81.5845824411135    J    1.0155805873291417 ::::: test accuracy 77.87021630615641    J    1.0283541601173563\n",
            "984 train:::acccuracy   81.5845824411135    J    1.0142437337306518 ::::: test accuracy 77.87021630615641    J    1.0272290197926253\n",
            "985 train:::acccuracy   81.5845824411135    J    1.012882977188289 ::::: test accuracy 77.87021630615641    J    1.0260836637574446\n",
            "986 train:::acccuracy   81.5845824411135    J    1.0114979762672915 ::::: test accuracy 77.87021630615641    J    1.0249177866528563\n",
            "987 train:::acccuracy   81.65596002855104    J    1.0100883951687722 ::::: test accuracy 77.87021630615641    J    1.023731086964687\n",
            "988 train:::acccuracy   81.65596002855104    J    1.008653904554639 ::::: test accuracy 77.87021630615641    J    1.0225232677002738\n",
            "989 train:::acccuracy   81.87009279086367    J    1.0071941824104687 ::::: test accuracy 78.03660565723793    J    1.0212940370990413\n",
            "990 train:::acccuracy   81.94147037830122    J    1.005708914946053 ::::: test accuracy 78.03660565723793    J    1.0200431093769808\n",
            "991 train:::acccuracy   81.87009279086367    J    1.0041977975332175 ::::: test accuracy 78.53577371048253    J    1.0187702055049546\n",
            "992 train:::acccuracy   81.87009279086367    J    1.0026605356801785 ::::: test accuracy 78.70216306156405    J    1.0174750540205217\n",
            "993 train:::acccuracy   82.08422555317631    J    1.0010968460415677 ::::: test accuracy 78.70216306156405    J    1.0161573918728317\n",
            "994 train:::acccuracy   82.08422555317631    J    0.9995064574628518 ::::: test accuracy 78.86855241264558    J    1.01481696529987\n",
            "995 train:::acccuracy   82.22698072805139    J    0.9978891120577121 ::::: test accuracy 79.03494176372712    J    1.0134535307371197\n",
            "996 train:::acccuracy   82.22698072805139    J    0.9962445663165334 ::::: test accuracy 79.03494176372712    J    1.0120668557564791\n",
            "997 train:::acccuracy   82.22698072805139    J    0.9945725922438765 ::::: test accuracy 78.86855241264558    J    1.0106567200339183\n",
            "998 train:::acccuracy   82.22698072805139    J    0.9928729785224824 ::::: test accuracy 79.03494176372712    J    1.009222916344191\n",
            "999 train:::acccuracy   82.29835831548894    J    0.9911455317009175 ::::: test accuracy 79.03494176372712    J    1.0077652515804958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtFVXgejZDIS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "c72ee7ea-5128-4d37-ec3e-71f34366d298"
      },
      "source": [
        "#그 코드를 바탕으로 training loss plotting 하기.\n",
        "real_iter = iteration\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "cost_tmp = np.zeros(real_iter)\n",
        "\n",
        "for i in range(0, real_iter):\n",
        "  cost_tmp[i] = cost_history[i]\n",
        "\n",
        "\n",
        "# print(cost_history[1])\n",
        "\n",
        "ax.set_ylabel('J(Theta)')\n",
        "ax.set_xlabel('Iterations')\n",
        "_=ax.plot(range(real_iter),cost_tmp,'b.', color = 'blue')\n",
        "_=ax.plot(range(real_iter),cost_history2,'b.', color = 'red')"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJEhxJanbFCg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "dcacebd0-9d16-40c1-aeee-301aa5750a23"
      },
      "source": [
        "#그 코드를 바탕으로 training loss plotting 하기.\n",
        "fig,ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "# print(cost_history[1])\n",
        "\n",
        "ax.set_ylabel('accuracy')\n",
        "ax.set_xlabel('Iterations')\n",
        "_=ax.plot(range(iteration),accuracy_history,'b.', color = 'blue')\n",
        "_=ax.plot(range(iteration),accuracy_history2,'b.', color = 'red')\n",
        "# print(ac_tmp)"
      ],
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuTRLc611Ayn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "3b7806d8-6c9b-4cd6-a6b5-91bd11c0101a"
      },
      "source": [
        "#Codes for Evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "l_list_tmp = l_list.reshape((1401))\n",
        "l_list2_tmp = l_list2.reshape((601))\n",
        "\n",
        "print(confusion_matrix(y_train,l_list_tmp))\n",
        "print(classification_report(y_train,l_list_tmp))\n",
        "print(accuracy_score(y_train, l_list_tmp))\n",
        "\n",
        "print(confusion_matrix(y_test,l_list2_tmp))\n",
        "print(classification_report(y_test,l_list2_tmp))\n",
        "print(accuracy_score(y_test, l_list2_tmp))"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[601  98]\n",
            " [150 552]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.86      0.83       699\n",
            "           1       0.85      0.79      0.82       702\n",
            "\n",
            "    accuracy                           0.82      1401\n",
            "   macro avg       0.82      0.82      0.82      1401\n",
            "weighted avg       0.82      0.82      0.82      1401\n",
            "\n",
            "0.8229835831548894\n",
            "[[248  54]\n",
            " [ 72 227]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.82      0.80       302\n",
            "           1       0.81      0.76      0.78       299\n",
            "\n",
            "    accuracy                           0.79       601\n",
            "   macro avg       0.79      0.79      0.79       601\n",
            "weighted avg       0.79      0.79      0.79       601\n",
            "\n",
            "0.7903494176372712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDpSEICHtNlB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "782e9e17-35a9-433d-af2d-4d1420ad7729"
      },
      "source": [
        "class_1 = 0\n",
        "\n",
        "for i in range(0, 601):\n",
        "  if l_list2_tmp[i] == 1:\n",
        "    class_1 = class_1 + 1\n",
        "\n",
        "print(class_1)"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbPP4IIn3lMu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9a38f73d-fd35-4962-b00c-772bdb4c90f9"
      },
      "source": [
        "print(y_train)\n",
        "print(l_list_tmp)"
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 ... 1 0 1]\n",
            "[0. 0. 1. ... 1. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}