{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment11-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN5Mpr3VEiiSNoOT1c7cfYv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlguswn3659/class-MachineLearning/blob/master/new_assignment_11_hidden_layer_four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw-faMg3kxeX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "33b5c2f0-bd2c-4dbc-eac9-45c3e361e5ff"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re,string,unicodedata\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEmx3Nerk1ud",
        "colab_type": "code",
        "outputId": "f0e19c9f-c6a3-40fa-f56b-69d9c6f97415",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "import zipfile\n",
        "import io\n",
        "zf = zipfile.ZipFile(io.BytesIO(uploaded['movie_review.zip']), \"r\")\n",
        "zf.extractall()\n",
        "\n",
        "review_data = load_files(r\"movie_review\")\n",
        "X, y = review_data.data, review_data.target\n",
        "\n",
        "documents = []\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "for sen in range(0, len(X)):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    \n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    \n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    \n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    \n",
        "    # Lemmatization\n",
        "    document = document.split()\n",
        "    document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    \n",
        "    documents.append(document)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c31e7691-3404-45fb-84a0-cad026a2d940\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c31e7691-3404-45fb-84a0-cad026a2d940\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving movie_review.zip to movie_review.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "matjDvyhlDJJ",
        "colab_type": "code",
        "outputId": "f4b12f67-7bcb-4325-e3d0-13af63b259b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "print(documents)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt9Wj3kH5q8w",
        "colab_type": "code",
        "outputId": "a01b752f-5cb0-44ea-934a-91d5b0052e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=40000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "# vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents).toarray()\n",
        "print(X)\n",
        "\n",
        "vector_size = 2616\n",
        "\n",
        "###################################################\n",
        "\n",
        "# # tfidfconverter = TfidfTransformer()\n",
        "# # print(tfidfconverter)\n",
        "# # X = tfidfconverter.fit_transform(X).toarray()\n",
        "# # print(X)\n",
        "\n",
        "# X_train_tmp, X_test_tmp, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
        "\n",
        "# # print(X_train.shape)\n",
        "\n",
        "# X_pos_sum = np.zeros(12638)\n",
        "# X_neg_sum = np.zeros(12638)\n",
        "\n",
        "# X_train = np.zeros((1401, 897))\n",
        "# X_test = np.zeros((601, 897))\n",
        "\n",
        "# for i in range(0, 2002):\n",
        "#   if y[i] == 0:\n",
        "#     X_neg_sum = X_neg_sum + X[i]\n",
        "#   else:\n",
        "#     X_pos_sum = X_pos_sum + X[i]\n",
        "\n",
        "# pos_neg_voca = 0\n",
        "\n",
        "# for i in range(0, 12638):\n",
        "#   # if abs(X_neg_sum[i] - X_pos_sum[i]) >= 30:\n",
        "#   if (X_neg_sum[i]*4 <= X_pos_sum[i] and X_pos_sum[i] > 8) or (X_pos_sum[i]*4 <= X_neg_sum[i] and X_neg_sum[i] > 8):\n",
        "#     for j in range(0, 1401):\n",
        "#       X_train[j][pos_neg_voca] = X_train_tmp[j][i]\n",
        "#     for j in range(0, 601):\n",
        "#       X_test[j][pos_neg_voca] = X_test_tmp[j][i]\n",
        "#     pos_neg_voca  = pos_neg_voca + 1\n",
        "\n",
        "# # tfidfconverter = TfidfTransformer()\n",
        "# # X_train = tfidfconverter.fit_transform(X_train).toarray()\n",
        "# # X_test = tfidfconverter.fit_transform(X_test).toarray()\n",
        "\n",
        "# print(X_neg_sum)\n",
        "# print(X_pos_sum)\n",
        "# print(pos_neg_voca)\n",
        "\n",
        "#################################################\n",
        "\n",
        "list_x=X.tolist()\n",
        "negative=[]\n",
        "positive=[]\n",
        "for i in range(0,2002):\n",
        "  if y[i]==0:\n",
        "    negative.append(list_x[i])\n",
        "  else:\n",
        "    positive.append(list_x[i])\n",
        "negative=np.array(negative)\n",
        "positive=np.array(positive)\n",
        "total_sum=X.sum(axis=0)\n",
        "neg_sum=negative.sum(axis=0)\n",
        "pos_sum=positive.sum(axis=0)\n",
        "\n",
        "neg_ratio=neg_sum/total_sum\n",
        "pos_ratio=pos_sum/total_sum\n",
        "\n",
        "good=[]\n",
        "for i in range(0,len(pos_sum)):\n",
        "  if neg_ratio[i]>pos_ratio[i]*3:\n",
        "    good.append(i)\n",
        "  elif pos_ratio[i]>neg_ratio[i]*3:\n",
        "    good.append(i)\n",
        "print(\"good의 길이:   \" , len(good))\n",
        "new=np.ones((1,2002), dtype=float)\n",
        "for i in range(0,len(good)):\n",
        "  temp=X[:,good[i]]\n",
        "  temp=temp[np.newaxis]\n",
        "  new=np.vstack((new,temp))\n",
        "new=np.transpose(new)\n",
        "new=new[:,1:]\n",
        "# vectorizer = CountVectorizer(max_features=2, min_df=0, stop_words=stopwords.words('english'))\n",
        "# Xp = vectorizer.fit_transform(pluss).toarray()\n",
        "\n",
        "# tfidfconverter = TfidfTransformer()\n",
        "# X = tfidfconverter.fit_transform(X).toarray()\n",
        "X_train, X_test, y_train, y_test = train_test_split(new, y, test_size=0.3, shuffle=False)\n",
        "\n",
        "print(X_train)\n",
        "print(X_test)\n",
        "\n",
        "print(X_train[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "good의 길이:    2615\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[0. 0. 0. ... 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQpyXu3jGOvY",
        "colab_type": "code",
        "outputId": "4afd40aa-fb0b-4cad-9966-a5a08c8600fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        }
      },
      "source": [
        "print(X_train)\n",
        "print(X_test)\n",
        "print(y_train)\n",
        "print(y_test)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[0 0 1 ... 1 0 1]\n",
            "[1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0\n",
            " 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0\n",
            " 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0\n",
            " 0 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 0]\n",
            "(1401, 2615)\n",
            "(601, 2615)\n",
            "(1401,)\n",
            "(601,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSA68oWdBX2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_classifier에 대한 함수 정의하기\n",
        "# def train_classifier(X_Train, y_Train):\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD42xdkL05UT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #Codes for classification\n",
        "\n",
        "# classifier = train_classifier(X_train, y_train)\n",
        "# y_pred_test = classifier.predict(X_test)\n",
        "# y_pred_train = classifier.predict(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV9K1MVaET0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix_t = np.zeros((350, vector_size))  #350줄\n",
        "matrix_t[0][0] = 1               #y레이어의 bias를 위한 값 1\n",
        "\n",
        "for i in range(1, 350):\n",
        "  for j in range(0, vector_size):\n",
        "    matrix_t[i][j] = np.random.normal()\n",
        "\n",
        "\n",
        "# matrix_u[196][0] = 0            #줄, 칸\n",
        "##############################################\n",
        "matrix_u = np.zeros((50, 350))  #50줄\n",
        "matrix_u[0][0] = 1               #y레이어의 bias를 위한 값 1\n",
        "\n",
        "for i in range(1, 50):\n",
        "  for j in range(0, 350):\n",
        "    matrix_u[i][j] = np.random.normal()\n",
        "\n",
        "\n",
        "# matrix_u[196][0] = 0            #줄, 칸\n",
        "##############################################\n",
        "matrix_v = np.zeros((8, 50))  #8줄\n",
        "matrix_v[0][0] = 1               #y레이어의 bias를 위한 값 1\n",
        "\n",
        "for i in range(1, 8):\n",
        "  for j in range(0, 50):\n",
        "    matrix_v[i][j] = np.random.normal()\n",
        "\n",
        "\n",
        "# matrix_v[196][0] = 0            #줄, 칸\n",
        "##############################################\n",
        "matrix_w = np.zeros((1, 8))  #1줄\n",
        "\n",
        "for i in range(0, 1):\n",
        "  for j in range(0, 8):\n",
        "    matrix_w[i][j] = np.random.normal()\n",
        "\n",
        "# matrix_w[196][0] = 0            #줄, 칸\n",
        "\n",
        "# print(matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfHkfVcXRZIw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d1bcee3a-6a40-4c02-faca-619757f902b9"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import matplotlib.animation as animation\n",
        "import operator\n",
        "from scipy import stats \n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.datasets.samples_generator import make_regression \n",
        "import csv\n",
        "import math\n",
        "import statistics"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.samples_generator module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLiiakUcEfil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "\n",
        "x_ = np.ones((350, 1))\n",
        "y_ = np.ones((50, 1))\n",
        "\n",
        "j_list = np.ones(shape=(1401, vector_size, 1))\n",
        "x_list = np.ones(shape=(1401, 350, 1))\n",
        "\n",
        "x__list = np.ones(shape=(1401, 350, 1))\n",
        "y__list = np.ones(shape=(1401, 50, 1))\n",
        "z__list = np.ones(shape=(1401, 8, 1))\n",
        "h__list = np.ones(shape=(1401, 1, 1))\n",
        "\n",
        "x_list = np.ones(shape=(1401, 350, 1))\n",
        "y_list = np.ones(shape=(1401, 50, 1))\n",
        "z_list = np.ones(shape=(1401, 8, 1))\n",
        "h_list = np.ones(shape=(1401, 1, 1))\n",
        "l_list = np.zeros(shape=(1401, 1, 1))\n",
        "\n",
        "x_list_tmp = np.ones(shape=(1401, 350, 1))\n",
        "y_list_tmp = np.ones(shape=(1401, 50, 1))\n",
        "z_list_tmp = np.ones(shape=(1401, 8, 1))\n",
        "h_list_tmp = np.ones(shape=(1401, 1, 1))\n",
        "\n",
        "x__list_tmp = np.ones(shape=(1401, 350, 1))\n",
        "y__list_tmp = np.ones(shape=(1401, 50, 1))\n",
        "z__list_tmp = np.ones(shape=(1401, 8, 1))\n",
        "h__list_tmp = np.ones(shape=(1401, 1, 1))\n",
        "\n",
        "\n",
        "#training set 1401개에 대해서\n",
        "for i in range(0, 1401):  \n",
        "  bias   = [1]       #bias 1\n",
        "  im_vector = np.concatenate((bias, X_train[i]), axis = None)\n",
        "  j_list[i] = im_vector.reshape((vector_size, 1))\n",
        "\n",
        "  x_ = np.dot(matrix_t, j_list[i])  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "  x__list[i] = x_\n",
        "  \n",
        "  x_vector = np.ones((350,1)) #bias\n",
        "  for j in range (1, 350):\n",
        "    x_vector[j][0] = 1 / (1 + math.exp(-x_[j][0]))\n",
        "    # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  x   = x_vector.reshape((350, 1))\n",
        "  x_list[i] = x\n",
        "\n",
        "\n",
        "\n",
        "  y_ = np.dot(matrix_u, x)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "  y__list[i] = y_\n",
        "  \n",
        "  y_vector = np.ones((50,1)) #bias\n",
        "  for j in range (1, 50):\n",
        "    y_vector[j][0] = 1 / (1 + math.exp(-y_[j][0]))\n",
        "    # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  y   = y_vector.reshape((50, 1))\n",
        "  y_list[i] = y\n",
        "\n",
        "  z_  = np.dot(matrix_v, y)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "  z__list[i] = z_\n",
        "  z_vector = np.ones((8,1)) #bias\n",
        "  for j in range (1, 8):\n",
        "    z_vector[j][0] = 1 / (1 + math.exp(-z_[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  z   = z_vector.reshape((8, 1))\n",
        "  z_list[i] = z\n",
        "  \n",
        "  # print(z)\n",
        "\n",
        "  h_  = np.dot(matrix_w, z)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "  h__list[i] = h_\n",
        "  \n",
        "  h_vector = np.ones((1,1)) #bias\n",
        "  for j in range (0, 1):\n",
        "    h_vector[j][0] = 1 / (1 + math.exp(-h_[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "  h   = h_vector.reshape((1, 1))\n",
        "  # tmp = h.ravel()\n",
        "  # max_value = max(tmp)\n",
        "  # print(max_value)\n",
        "  # max_index = tmp.index(max_value)\n",
        "  h_list[i] = h\n",
        "\n",
        "  tmp_label = 0\n",
        "  # print(h)\n",
        "\n",
        "  if h >= 0.5:\n",
        "    tmp_label = 1\n",
        "\n",
        "  l_list[i][0] = tmp_label\n",
        "  # print(l_list[i])\n",
        "  # print(h)\n",
        "\n",
        "  x_list_tmp[i] = x_list[i]\n",
        "  y_list_tmp[i] = y_list[i]\n",
        "  z_list_tmp[i] = z_list[i]\n",
        "  h_list_tmp[i] = h_list[i]\n",
        "  x__list_tmp[i] = x__list[i]\n",
        "  y__list_tmp[i] = y__list[i]\n",
        "  z__list_tmp[i] = z__list[i]\n",
        "  h__list_tmp[i] = h__list[i]\n",
        "\n",
        "# print(l_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b91lsWSoEg8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "\n",
        "x_2 = np.ones((350, 1))\n",
        "y_2 = np.ones((50, 1))\n",
        "\n",
        "j_list2 = np.ones(shape=(601, vector_size, 1))\n",
        "x_list2 = np.ones(shape=(601, 350, 1))\n",
        "\n",
        "x__list2 = np.ones(shape=(601, 350, 1))\n",
        "y__list2 = np.ones(shape=(601, 50, 1))\n",
        "z__list2 = np.ones(shape=(601, 8, 1))\n",
        "h__list2 = np.ones(shape=(601, 1, 1))\n",
        "\n",
        "x_list2 = np.ones(shape=(601, 350, 1))\n",
        "y_list2 = np.ones(shape=(601, 50, 1))\n",
        "z_list2 = np.ones(shape=(601, 8, 1))\n",
        "h_list2 = np.ones(shape=(601, 1, 1))\n",
        "l_list2 = np.zeros(shape=(601, 1, 1))\n",
        "\n",
        "x_list_tmp2 = np.ones(shape=(601, 350, 1))\n",
        "y_list_tmp2 = np.ones(shape=(601, 50, 1))\n",
        "z_list_tmp2 = np.ones(shape=(601, 8, 1))\n",
        "h_list_tmp2 = np.ones(shape=(601, 1, 1))\n",
        "\n",
        "x__list_tmp2 = np.ones(shape=(601, 350, 1))\n",
        "y__list_tmp2 = np.ones(shape=(601, 50, 1))\n",
        "z__list_tmp2 = np.ones(shape=(601, 8, 1))\n",
        "h__list_tmp2 = np.ones(shape=(601, 1, 1))\n",
        "\n",
        "\n",
        "#training set 1401개에 대해서\n",
        "for i in range(0, 601):  \n",
        "  bias2   = [1]       #bias 1\n",
        "  im_vector2 = np.concatenate((bias2, X_test[i]), axis = None)\n",
        "  j_list2[i] = im_vector2.reshape((vector_size, 1))\n",
        "\n",
        "  x_2 = np.dot(matrix_t, j_list2[i])  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "  x__list2[i] = x_2\n",
        "  \n",
        "  x_vector2 = np.ones((350,1)) #bias\n",
        "  for j in range (1, 350):\n",
        "    x_vector2[j][0] = 1 / (1 + math.exp(-x_2[j][0]))\n",
        "    # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  x2   = x_vector2.reshape((350, 1))\n",
        "  x_list2[i] = x2\n",
        "\n",
        "\n",
        "\n",
        "  y_2 = np.dot(matrix_u, x2)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "  y__list2[i] = y_2\n",
        "  \n",
        "  y_vector2 = np.ones((50,1)) #bias\n",
        "  for j in range (1, 50):\n",
        "    y_vector2[j][0] = 1 / (1 + math.exp(-y_2[j][0]))\n",
        "    # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  y2   = y_vector2.reshape((50, 1))\n",
        "  y_list2[i] = y2\n",
        "\n",
        "  z_2  = np.dot(matrix_v, y2)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "  z__list2[i] = z_2\n",
        "  z_vector2 = np.ones((8,1)) #bias\n",
        "  for j in range (1, 8):\n",
        "    z_vector2[j][0] = 1 / (1 + math.exp(-z_2[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "  z2   = z_vector2.reshape((8, 1))\n",
        "  z_list2[i] = z2\n",
        "  \n",
        "  # print(z)\n",
        "\n",
        "  h_2 = np.dot(matrix_w, z2)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "  h__list2[i] = h_2\n",
        "  \n",
        "  h_vector2 = np.ones((1,1)) #bias\n",
        "  for j in range (0, 1):\n",
        "    h_vector2[j][0] = 1 / (1 + math.exp(-h_2[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "  h2   = h_vector2.reshape((1, 1))\n",
        "  # tmp = h.ravel()\n",
        "  # max_value = max(tmp)\n",
        "  # print(max_value)\n",
        "  # max_index = tmp.index(max_value)\n",
        "  h_list2[i] = h2\n",
        "\n",
        "  tmp_label = 0\n",
        "\n",
        "  if h2 >= 0.5:\n",
        "    tmp_label = 1\n",
        "\n",
        "  l_list2[i][0] = tmp_label\n",
        "  # print(l_list[i])\n",
        "  # print(h2)\n",
        "\n",
        "  x_list_tmp2[i] = x_list2[i]\n",
        "  y_list_tmp2[i] = y_list2[i]\n",
        "  z_list_tmp2[i] = z_list2[i]\n",
        "  h_list_tmp2[i] = h_list2[i]\n",
        "  x__list_tmp2[i] = x__list2[i]\n",
        "  y__list_tmp2[i] = y__list2[i]\n",
        "  z__list_tmp2[i] = z__list2[i]\n",
        "  h__list_tmp2[i] = h__list2[i]\n",
        "\n",
        "# print(l_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVgg5PyfCGAn",
        "colab_type": "code",
        "outputId": "1f2081b0-ef88-4365-afbc-66d1576ae602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# (1401, 1500)\n",
        "# (601, 1500)\n",
        "# (1401,)\n",
        "# (601,)\n",
        "\n",
        "iteration = 2000\n",
        "l_r = 0.2\n",
        "lambda_ = 1\n",
        "\n",
        "cost_history = np.zeros(iteration)    #about train\n",
        "cost_history2 = np.zeros(iteration)   #about test\n",
        "\n",
        "label_group = np.zeros(shape = (1401, 1))\n",
        "label_group2 = np.zeros(shape = (601, 1))\n",
        "\n",
        "accuracy_history = np.zeros(iteration)\n",
        "accuracy_history2 = np.zeros(iteration)\n",
        "\n",
        "for i in range(0, 1401):\n",
        "  label_group[i][0] = l_list[i][0]\n",
        "\n",
        "for i in range(0, 601):\n",
        "  label_group2[i][0] = l_list2[i][0]\n",
        "\n",
        "for t in range(0, iteration):\n",
        "  number_of_correct = 0\n",
        "  number_of_correct2 = 0\n",
        "\n",
        "  print(t, end=' ')\n",
        "\n",
        "  parameter_sum = 0\n",
        "\n",
        "  for q in range(0, vector_size):\n",
        "    for w in range(1, 350):\n",
        "      parameter_sum = parameter_sum + matrix_t[w][q]**2\n",
        "\n",
        "  for q in range(0, 350):\n",
        "    for w in range(1, 50):\n",
        "      parameter_sum = parameter_sum + matrix_u[w][q]**2\n",
        "\n",
        "  for q in range(0, 50):\n",
        "    for w in range(1, 8):\n",
        "      parameter_sum = parameter_sum + matrix_v[w][q]**2\n",
        "\n",
        "  for q in range(0, 8):\n",
        "    for w in range(0, 1):\n",
        "      parameter_sum = parameter_sum + matrix_w[w][q]**2\n",
        "\n",
        "  result = 0.0\n",
        "  for i in range(0, 1401):\n",
        "    for k in range(0, 1):\n",
        "      result = result + ((-1)*label_group[i][k]*math.log(h_list[i][k][0]) - (1-label_group[i][k])*math.log(1-h_list[i][k][0])) + ((lambda_)/(2 * 328008)) * parameter_sum\n",
        "  result = result / 1401\n",
        "  cost_history[t] = result\n",
        "\n",
        "##gradient descent\n",
        "  result_w = 0\n",
        "  result_v = 0\n",
        "  result_u = 0\n",
        "  result_t = 0\n",
        "\n",
        "  for i in range(0, 1401):\n",
        "    small_delta_2 = np.zeros(shape = (1, 1))\n",
        "    for j in range(0, 1):\n",
        "      small_delta_2[j][0] = h_list[i][j] - label_group[i][j]\n",
        "    # print(h_list[i])\n",
        "    # print(label_group[i])\n",
        "    # small_delta_2 = h_list[i] - label_group[i] #여기 label_group 다시 봐야함.\n",
        "    small_delta_1 = np.dot(np.transpose(matrix_w), small_delta_2)\n",
        "    small_delta_0 = np.dot(np.transpose(matrix_v), small_delta_1)*(y_list_tmp[i]*(1 - y_list_tmp[i]))\n",
        "    small_delta_ = np.dot(np.transpose(matrix_u), small_delta_0)*(x_list_tmp[i]*(1 - x_list_tmp[i])) #이거..괜찮은가..?\n",
        "\n",
        "    # print(small_delta_2)\n",
        "    result_w = result_w + np.dot(small_delta_2, np.transpose(z_list[i]))\n",
        "    result_v = result_v + np.dot(small_delta_1, np.transpose(y_list[i]))\n",
        "    result_u = result_u + np.dot(small_delta_0, np.transpose(x_list[i]))\n",
        "    result_t = result_t + np.dot(small_delta_, np.transpose(j_list[i]))\n",
        "\n",
        "  matrix_w = matrix_w - (result_w / 1401 *l_r)\n",
        "  matrix_v = matrix_v - (result_v / 1401 *l_r)\n",
        "  matrix_u = matrix_u - (result_u / 1401 *l_r)\n",
        "  matrix_t = matrix_t - (result_t / 1401 *l_r)\n",
        "\n",
        "\n",
        "####################################################################\n",
        "\n",
        "  for k in range(0, 1401):\n",
        "    bias   = [1]       #bias 1\n",
        "    im_vector = np.concatenate((bias, X_train[k]), axis = None)\n",
        "    j_list[k] = im_vector.reshape((vector_size, 1))\n",
        "\n",
        "    x_ = np.dot(matrix_t, j_list[k])  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "    x__list[k] = x_\n",
        "    \n",
        "    x_vector = np.ones((350,1)) #bias\n",
        "    for j in range (1, 350):\n",
        "      x_vector[j][0] = 1 / (1 + math.exp(-x_[j][0]))\n",
        "      # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "    x   = x_vector.reshape((350, 1))\n",
        "    x_list[k] = x\n",
        "\n",
        "\n",
        "\n",
        "    y_ = np.dot(matrix_u, x)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "    y__list[k] = y_\n",
        "    \n",
        "    y_vector = np.ones((50,1)) #bias\n",
        "    for j in range (1, 50):\n",
        "      y_vector[j][0] = 1 / (1 + math.exp(-y_[j][0]))\n",
        "      # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "    y   = y_vector.reshape((50, 1))\n",
        "    y_list[k] = y\n",
        "\n",
        "    z_  = np.dot(matrix_v, y)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "    z__list[k] = z_\n",
        "    z_vector = np.ones((8,1)) #bias\n",
        "    for j in range (1, 8):\n",
        "      z_vector[j][0] = 1 / (1 + math.exp(-z_[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "    z   = z_vector.reshape((8, 1))\n",
        "    z_list[k] = z\n",
        "    \n",
        "    # print(z)\n",
        "\n",
        "    h_  = np.dot(matrix_w, z)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "    h__list[k] = h_\n",
        "    \n",
        "    h_vector = np.ones((1,1)) #bias\n",
        "    for j in range (0, 1):\n",
        "      h_vector[j][0] = 1 / (1 + math.exp(-h_[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "    h   = h_vector.reshape((1, 1))\n",
        "    # tmp = h.ravel()\n",
        "    # max_value = max(tmp)\n",
        "    # print(max_value)\n",
        "    # max_index = tmp.index(max_value)\n",
        "\n",
        "    h_list11_tmp = h_list.reshape((1401))\n",
        "    median_val = statistics.median(h_list11_tmp)\n",
        "\n",
        "    h_list[k] = h\n",
        "\n",
        "    tmp_label = 0\n",
        "\n",
        "    # if h >= median_val:\n",
        "    #     # print(h)\n",
        "    #     tmp_label = 1\n",
        "\n",
        "    if h >= 0.5:\n",
        "      # print(h)\n",
        "      tmp_label = 1\n",
        "\n",
        "    l_list[k][0] = tmp_label\n",
        "    # print(l_list[k])\n",
        "    # print(h)\n",
        "\n",
        "    x_list_tmp[k] = x_list[k]\n",
        "    y_list_tmp[k] = y_list[k]\n",
        "    z_list_tmp[k] = z_list[k]\n",
        "    h_list_tmp[k] = h_list[k]\n",
        "    x__list_tmp[k] = x__list[k]\n",
        "    y__list_tmp[k] = y__list[k]\n",
        "    z__list_tmp[k] = z__list[k]\n",
        "    h__list_tmp[k] = h__list[k]\n",
        "\n",
        "    if l_list[k][0] == y_train[k]:\n",
        "      number_of_correct = number_of_correct + 1\n",
        "\n",
        "  accuracy_history[t] = number_of_correct / 1401 * 100\n",
        "  print(accuracy_history[t], end=' ')\n",
        "  # accuracy_history[t] = number_of_correct_predictions/total_number_of_predictions*100\n",
        "  # print(accuracy_history[t])\n",
        "\n",
        "############################\n",
        "  #test data!\n",
        "\n",
        "  number_of_correct_predictions2 = 0\n",
        "  \n",
        "  # print(t, end=' ')\n",
        "\n",
        "  result2 = 0.0\n",
        "  for i in range(0, 601):\n",
        "    for k in range(0, 1):\n",
        "      # if h_list_tmp[i][k][0] <= 0:\n",
        "      # print(h_list_tmp[i][k][0])\n",
        "      result2 = result2 + ((-1)*label_group2[i][k]*math.log(h_list2[i][k][0]) - (1-label_group2[i][k])*math.log(1-h_list2[i][k][0])) + ((lambda_)/(2 * 328008)) * parameter_sum\n",
        "  result2 = result2 / 601\n",
        "  cost_history2[t] = result2\n",
        "\n",
        "\n",
        "  for k in range(0, 601):\n",
        "  # print(i)\n",
        "    bias2   = [1]       #bias 1\n",
        "    im_vector2 = np.concatenate((bias2, X_test[k]), axis = None)\n",
        "    j_list2[k] = im_vector2.reshape((vector_size, 1))\n",
        "\n",
        "    x_2 = np.dot(matrix_t, j_list2[k])  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "    x__list2[k] = x_2\n",
        "    \n",
        "    x_vector2 = np.ones((350,1)) #bias\n",
        "    for j in range (1, 350):\n",
        "      x_vector2[j][0] = 1 / (1 + math.exp(-x_2[j][0]))\n",
        "      # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "    x2   = x_vector2.reshape((350, 1))\n",
        "    x_list2[k] = x2\n",
        "\n",
        "\n",
        "\n",
        "    y_2 = np.dot(matrix_u, x2)  #x와 가중치 u를 곱해서 y_를 만듦\n",
        "    y__list2[k] = y_2\n",
        "    \n",
        "    y_vector2 = np.ones((50,1)) #bias\n",
        "    for j in range (1, 50):\n",
        "      y_vector2[j][0] = 1 / (1 + math.exp(-y_2[j][0]))\n",
        "      # sigmoid(y_[j][0])  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "    y2   = y_vector2.reshape((50, 1))\n",
        "    y_list2[k] = y2\n",
        "\n",
        "    z_2  = np.dot(matrix_v, y2)  #y와 가중치 v를 곱해서 z_를 만듦\n",
        "    z__list2[k] = z_2\n",
        "    z_vector2 = np.ones((8,1)) #bias\n",
        "    for j in range (1, 8):\n",
        "      z_vector2[j][0] = 1 / (1 + math.exp(-z_2[j][0]))  #sigmoid 함수를 적용해서 y_에서 y를 만듦\n",
        "\n",
        "    z2   = z_vector2.reshape((8, 1))\n",
        "    z_list2[k] = z2\n",
        "    \n",
        "    # print(z)\n",
        "\n",
        "    h_2 = np.dot(matrix_w, z2)  #z와 가중치 w를 곱해서 h_를 만듦\n",
        "    h__list2[k] = h_2\n",
        "    \n",
        "    h_vector2 = np.ones((1,1)) #bias\n",
        "    for j in range (0, 1):\n",
        "      h_vector2[j][0] = 1 / (1 + math.exp(-h_2[j][0]))  #sigmoid 함수를 적용해서 h_에서 h를 만듦\n",
        "\n",
        "    h2   = h_vector2.reshape((1, 1))\n",
        "    # tmp = h.ravel()\n",
        "    # max_value = max(tmp)\n",
        "    # print(max_value)\n",
        "    # max_index = tmp.index(max_value)\n",
        "\n",
        "    h_list22_tmp = h_list2.reshape((601))\n",
        "    median_val2 = statistics.median(h_list22_tmp)\n",
        "\n",
        "    h_list2[k] = h2\n",
        "\n",
        "    tmp_label = 0\n",
        "\n",
        "    # if h2 >= median_val2:\n",
        "    #   # print(h2)\n",
        "    #   tmp_label = 1\n",
        "\n",
        "    if h2 >= 0.5:\n",
        "      # print(h2)\n",
        "      tmp_label = 1\n",
        "\n",
        "    l_list2[k][0] = tmp_label\n",
        "    # print(l_list[k])\n",
        "    # print(h2)\n",
        "\n",
        "    x_list_tmp2[k] = x_list2[k]\n",
        "    y_list_tmp2[k] = y_list2[k]\n",
        "    z_list_tmp2[k] = z_list2[k]\n",
        "    h_list_tmp2[k] = h_list2[k]\n",
        "    x__list_tmp2[k] = x__list2[k]\n",
        "    y__list_tmp2[k] = y__list2[k]\n",
        "    z__list_tmp2[k] = z__list2[k]\n",
        "    h__list_tmp2[k] = h__list2[k]\n",
        "\n",
        "    if l_list2[k][0] == y_test[k]:\n",
        "      number_of_correct2 = number_of_correct2 + 1\n",
        "\n",
        "  accuracy_history2[t] = number_of_correct2 / 601 * 100\n",
        "  print(accuracy_history2[t])\n",
        "\n",
        "  # accuracy_history2[t] = number_of_correct_predictions2/total_number_of_predictions2*100\n",
        "  t = t + 1"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 49.892933618843685 50.249584026622294\n",
            "1 49.892933618843685 50.249584026622294\n",
            "2 49.892933618843685 50.249584026622294\n",
            "3 49.892933618843685 50.249584026622294\n",
            "4 49.892933618843685 50.249584026622294\n",
            "5 49.892933618843685 50.249584026622294\n",
            "6 49.892933618843685 50.249584026622294\n",
            "7 49.892933618843685 50.249584026622294\n",
            "8 49.892933618843685 50.249584026622294\n",
            "9 49.892933618843685 50.249584026622294\n",
            "10 49.892933618843685 50.249584026622294\n",
            "11 49.892933618843685 50.249584026622294\n",
            "12 49.892933618843685 50.249584026622294\n",
            "13 49.892933618843685 50.249584026622294\n",
            "14 49.892933618843685 50.249584026622294\n",
            "15 49.892933618843685 50.249584026622294\n",
            "16 49.892933618843685 50.249584026622294\n",
            "17 49.892933618843685 50.249584026622294\n",
            "18 49.892933618843685 50.249584026622294\n",
            "19 49.892933618843685 50.249584026622294\n",
            "20 49.892933618843685 50.249584026622294\n",
            "21 49.892933618843685 50.249584026622294\n",
            "22 49.892933618843685 50.249584026622294\n",
            "23 49.892933618843685 50.249584026622294\n",
            "24 49.892933618843685 50.249584026622294\n",
            "25 49.892933618843685 50.249584026622294\n",
            "26 49.892933618843685 50.249584026622294\n",
            "27 49.892933618843685 50.249584026622294\n",
            "28 49.892933618843685 50.249584026622294\n",
            "29 49.892933618843685 50.249584026622294\n",
            "30 49.892933618843685 50.249584026622294\n",
            "31 49.892933618843685 50.249584026622294\n",
            "32 49.892933618843685 50.249584026622294\n",
            "33 49.892933618843685 50.249584026622294\n",
            "34 49.892933618843685 50.249584026622294\n",
            "35 49.892933618843685 50.249584026622294\n",
            "36 49.892933618843685 50.249584026622294\n",
            "37 49.892933618843685 50.249584026622294\n",
            "38 49.892933618843685 50.249584026622294\n",
            "39 49.892933618843685 50.249584026622294\n",
            "40 49.892933618843685 50.249584026622294\n",
            "41 49.892933618843685 50.249584026622294\n",
            "42 49.892933618843685 50.249584026622294\n",
            "43 49.892933618843685 50.249584026622294\n",
            "44 49.892933618843685 50.249584026622294\n",
            "45 49.892933618843685 50.249584026622294\n",
            "46 49.892933618843685 50.249584026622294\n",
            "47 49.892933618843685 50.249584026622294\n",
            "48 49.892933618843685 50.249584026622294\n",
            "49 49.892933618843685 50.249584026622294\n",
            "50 49.892933618843685 50.249584026622294\n",
            "51 49.892933618843685 50.249584026622294\n",
            "52 49.892933618843685 50.249584026622294\n",
            "53 49.892933618843685 50.249584026622294\n",
            "54 49.892933618843685 50.249584026622294\n",
            "55 49.892933618843685 50.249584026622294\n",
            "56 49.892933618843685 50.249584026622294\n",
            "57 49.892933618843685 50.249584026622294\n",
            "58 49.892933618843685 50.249584026622294\n",
            "59 49.892933618843685 50.249584026622294\n",
            "60 49.892933618843685 50.249584026622294\n",
            "61 49.892933618843685 50.249584026622294\n",
            "62 49.892933618843685 50.249584026622294\n",
            "63 49.892933618843685 50.249584026622294\n",
            "64 49.892933618843685 50.249584026622294\n",
            "65 49.892933618843685 50.249584026622294\n",
            "66 49.892933618843685 50.249584026622294\n",
            "67 49.892933618843685 50.249584026622294\n",
            "68 49.892933618843685 50.249584026622294\n",
            "69 49.892933618843685 50.249584026622294\n",
            "70 49.892933618843685 50.249584026622294\n",
            "71 49.892933618843685 50.249584026622294\n",
            "72 49.892933618843685 50.249584026622294\n",
            "73 49.892933618843685 50.249584026622294\n",
            "74 49.892933618843685 50.249584026622294\n",
            "75 49.892933618843685 50.249584026622294\n",
            "76 49.892933618843685 50.249584026622294\n",
            "77 49.892933618843685 50.249584026622294\n",
            "78 49.892933618843685 50.249584026622294\n",
            "79 49.892933618843685 50.249584026622294\n",
            "80 49.892933618843685 50.249584026622294\n",
            "81 49.892933618843685 50.249584026622294\n",
            "82 49.892933618843685 50.249584026622294\n",
            "83 49.892933618843685 50.249584026622294\n",
            "84 49.892933618843685 50.249584026622294\n",
            "85 49.892933618843685 50.249584026622294\n",
            "86 49.892933618843685 50.249584026622294\n",
            "87 49.892933618843685 50.249584026622294\n",
            "88 49.892933618843685 50.249584026622294\n",
            "89 49.892933618843685 50.249584026622294\n",
            "90 49.892933618843685 50.249584026622294\n",
            "91 49.892933618843685 50.249584026622294\n",
            "92 49.892933618843685 50.249584026622294\n",
            "93 49.892933618843685 50.249584026622294\n",
            "94 49.892933618843685 50.249584026622294\n",
            "95 49.892933618843685 50.249584026622294\n",
            "96 49.892933618843685 50.249584026622294\n",
            "97 49.892933618843685 50.249584026622294\n",
            "98 49.892933618843685 50.249584026622294\n",
            "99 49.892933618843685 50.249584026622294\n",
            "100 49.892933618843685 50.249584026622294\n",
            "101 49.892933618843685 50.249584026622294\n",
            "102 49.892933618843685 50.249584026622294\n",
            "103 49.892933618843685 50.249584026622294\n",
            "104 49.892933618843685 50.249584026622294\n",
            "105 49.892933618843685 50.249584026622294\n",
            "106 49.892933618843685 50.249584026622294\n",
            "107 49.892933618843685 50.249584026622294\n",
            "108 49.892933618843685 50.249584026622294\n",
            "109 49.892933618843685 50.249584026622294\n",
            "110 49.892933618843685 50.249584026622294\n",
            "111 49.892933618843685 50.249584026622294\n",
            "112 49.892933618843685 50.249584026622294\n",
            "113 49.892933618843685 50.249584026622294\n",
            "114 49.892933618843685 50.249584026622294\n",
            "115 49.892933618843685 50.249584026622294\n",
            "116 49.892933618843685 50.249584026622294\n",
            "117 49.892933618843685 50.249584026622294\n",
            "118 49.892933618843685 50.249584026622294\n",
            "119 49.892933618843685 50.249584026622294\n",
            "120 49.892933618843685 50.249584026622294\n",
            "121 49.892933618843685 50.249584026622294\n",
            "122 49.892933618843685 50.249584026622294\n",
            "123 49.892933618843685 50.249584026622294\n",
            "124 49.892933618843685 50.249584026622294\n",
            "125 49.892933618843685 50.249584026622294\n",
            "126 49.892933618843685 50.249584026622294\n",
            "127 49.892933618843685 50.249584026622294\n",
            "128 49.892933618843685 50.249584026622294\n",
            "129 49.892933618843685 50.249584026622294\n",
            "130 49.892933618843685 50.249584026622294\n",
            "131 49.892933618843685 50.249584026622294\n",
            "132 49.892933618843685 50.249584026622294\n",
            "133 49.892933618843685 50.249584026622294\n",
            "134 49.892933618843685 50.249584026622294\n",
            "135 49.892933618843685 50.249584026622294\n",
            "136 49.892933618843685 50.249584026622294\n",
            "137 49.892933618843685 50.249584026622294\n",
            "138 49.892933618843685 50.249584026622294\n",
            "139 49.892933618843685 50.249584026622294\n",
            "140 49.892933618843685 50.249584026622294\n",
            "141 49.892933618843685 50.249584026622294\n",
            "142 49.892933618843685 50.249584026622294\n",
            "143 49.892933618843685 50.249584026622294\n",
            "144 49.892933618843685 50.249584026622294\n",
            "145 49.892933618843685 50.249584026622294\n",
            "146 49.892933618843685 50.249584026622294\n",
            "147 49.892933618843685 50.249584026622294\n",
            "148 49.892933618843685 50.249584026622294\n",
            "149 49.892933618843685 50.249584026622294\n",
            "150 49.892933618843685 50.249584026622294\n",
            "151 49.892933618843685 50.249584026622294\n",
            "152 49.892933618843685 50.249584026622294\n",
            "153 49.892933618843685 50.249584026622294\n",
            "154 49.892933618843685 50.249584026622294\n",
            "155 49.892933618843685 50.249584026622294\n",
            "156 49.892933618843685 50.249584026622294\n",
            "157 49.892933618843685 50.249584026622294\n",
            "158 49.892933618843685 50.249584026622294\n",
            "159 49.892933618843685 50.249584026622294\n",
            "160 49.892933618843685 50.249584026622294\n",
            "161 49.892933618843685 50.249584026622294\n",
            "162 49.892933618843685 50.249584026622294\n",
            "163 49.892933618843685 50.249584026622294\n",
            "164 49.892933618843685 50.249584026622294\n",
            "165 49.892933618843685 50.249584026622294\n",
            "166 49.892933618843685 50.249584026622294\n",
            "167 49.892933618843685 50.249584026622294\n",
            "168 49.892933618843685 50.249584026622294\n",
            "169 49.892933618843685 50.249584026622294\n",
            "170 49.892933618843685 50.249584026622294\n",
            "171 49.892933618843685 50.249584026622294\n",
            "172 49.892933618843685 50.249584026622294\n",
            "173 49.892933618843685 50.249584026622294\n",
            "174 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-aca6a4a690b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mj_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#x와 가중치 u를 곱해서 y_를 만듦\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mx__list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtFVXgejZDIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#그 코드를 바탕으로 training loss plotting 하기.\n",
        "real_iter = iteration\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "cost_tmp = np.zeros(real_iter)\n",
        "\n",
        "for i in range(0, real_iter):\n",
        "  cost_tmp[i] = cost_history[i]\n",
        "\n",
        "\n",
        "# print(cost_history[1])\n",
        "\n",
        "ax.set_ylabel('J(Theta)')\n",
        "ax.set_xlabel('Iterations')\n",
        "_=ax.plot(range(real_iter),cost_tmp,'b.', color = 'blue')\n",
        "_=ax.plot(range(real_iter),cost_history2,'b.', color = 'red')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJEhxJanbFCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#그 코드를 바탕으로 training loss plotting 하기.\n",
        "fig,ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "# print(cost_history[1])\n",
        "\n",
        "ax.set_ylabel('accuracy')\n",
        "ax.set_xlabel('Iterations')\n",
        "_=ax.plot(range(iteration),accuracy_history,'b.', color = 'blue')\n",
        "_=ax.plot(range(iteration),accuracy_history2,'b.', color = 'red')\n",
        "# print(ac_tmp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuTRLc611Ayn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Codes for Evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "l_list_tmp = l_list.reshape((1401))\n",
        "l_list2_tmp = l_list2.reshape((601))\n",
        "\n",
        "print(confusion_matrix(y_train,l_list_tmp))\n",
        "print(classification_report(y_train,l_list_tmp))\n",
        "print(accuracy_score(y_train, l_list_tmp))\n",
        "\n",
        "print(confusion_matrix(y_test,l_list2_tmp))\n",
        "print(classification_report(y_test,l_list2_tmp))\n",
        "print(accuracy_score(y_test, l_list2_tmp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDpSEICHtNlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_1 = 0\n",
        "\n",
        "for i in range(0, 601):\n",
        "  if l_list2_tmp[i] == 1:\n",
        "    class_1 = class_1 + 1\n",
        "\n",
        "print(class_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbPP4IIn3lMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(y_train)\n",
        "print(l_list_tmp)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}